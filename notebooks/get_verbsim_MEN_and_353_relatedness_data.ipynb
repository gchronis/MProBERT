{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the words we care about collecting?\n",
    "Ignore the ones we already have token files for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 3000 word pairs from MEN relatedness dataset\n",
      "processed 130 word pairs from VerbSim dataset\n",
      "processed 252 word pairs from WordSim relatedness dataset\n",
      "processed 26554 word pairs from BLESS dataset\n",
      "Total words between 353_rel, simverb, men, and bless: 6764\n",
      "Unique words between 353_rel, simverb, men, and bless: 1164\n",
      "New words that we don't have tokens collected yet for yada yada: 708\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import os\n",
    "\n",
    "words_to_collect = []\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "1) the words we want to collect data for\n",
    "\"\"\"\n",
    "men = datasets.get_men()\n",
    "verbsim = datasets.get_verbsim()\n",
    "ws353_rel = datasets.get_ws353_rel()\n",
    "bless = datasets.get_bless()\n",
    "\n",
    "# get all the words\n",
    "all_words = []\n",
    "for dataset in [men, verbsim, ws353_rel]:\n",
    "    for row in dataset:\n",
    "        w1 = row['word1']\n",
    "        w2 = row['word2']\n",
    "        all_words.append(w1)\n",
    "        all_words.append(w2)\n",
    "        \n",
    "unique_words = set(all_words)\n",
    "\n",
    "for word in unique_words:\n",
    "    pathname = os.path.join(\"./data/word_data/\", word)\n",
    "    # collect this new word unless its not new and we already have data for it\n",
    "    if not os.path.isdir(pathname):\n",
    "        words_to_collect.append(word)\n",
    "        \n",
    "\n",
    "print(\"Total words between 353_rel, simverb, and men: %s\" % len(all_words))\n",
    "print(\"Unique words between 353_rel, simverb, and men: %s\" % len(unique_words))\n",
    "print(\"New words that we don't have tokens collected yet for yada yada: %s\" % len(words_to_collect))\n",
    "\n",
    "\"\"\"\n",
    "2) the layers we want to analzye\n",
    "\"\"\"\n",
    "layers = [0,1,5,11]\n",
    "\n",
    "\"\"\"\n",
    "3) The cluster sizes we want to analyze\n",
    "\"\"\"\n",
    "cluster_sizes = [1,3,5,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect token data...again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Sentences in BNC corpus: 6026276\n",
      "shuffling indexes\n",
      "done shuffling\n",
      "[250087, 3235411, 5844650, 3531151, 5392874, 5323871, 1656272, 1046852, 406747, 1658543, 4051637, 3046040, 1870259, 5595167, 5202262, 4889351, 2494284, 1804113, 5927710, 5076530, 2626732, 3786115, 1931190, 235848, 2489126, 387482, 1298999, 1215573, 173734, 1522570, 1835438, 91688, 312228, 1572779, 230989, 2050391, 3979353, 151342, 5708478, 5882787, 4196364, 3761369, 4966045, 5885394, 4460127, 4337645, 1563753, 1787210, 3741152, 4736237]\n",
      "Processed 100000 sentences\n",
      "Processed 200000 sentences\n",
      "Processed 300000 sentences\n",
      "Processed 400000 sentences\n",
      "Processed 500000 sentences\n",
      "Processed 600000 sentences\n",
      "Processed 700000 sentences\n",
      "Processed 800000 sentences\n",
      "Processed 900000 sentences\n",
      "Processed 1000000 sentences\n",
      "Processed 1100000 sentences\n",
      "Processed 1200000 sentences\n",
      "Processed 1300000 sentences\n",
      "Processed 1400000 sentences\n",
      "Processed 1500000 sentences\n",
      "Processed 1600000 sentences\n",
      "Processed 1700000 sentences\n",
      "Processed 1800000 sentences\n",
      "Processed 1900000 sentences\n",
      "Processed 2000000 sentences\n",
      "Processed 2100000 sentences\n",
      "Processed 2200000 sentences\n",
      "Processed 2300000 sentences\n",
      "Processed 2400000 sentences\n",
      "Processed 2500000 sentences\n",
      "Processed 2600000 sentences\n",
      "Processed 2700000 sentences\n",
      "Processed 2800000 sentences\n",
      "Processed 2900000 sentences\n",
      "Processed 3000000 sentences\n",
      "Processed 3100000 sentences\n",
      "Processed 3200000 sentences\n",
      "Processed 3300000 sentences\n",
      "Processed 3400000 sentences\n",
      "Processed 3500000 sentences\n",
      "Processed 3600000 sentences\n",
      "Processed 3700000 sentences\n",
      "Processed 3800000 sentences\n",
      "Processed 3900000 sentences\n",
      "stork\n",
      "SUBST\n",
      "Microsoft Corp 's NT business manager , Carl Stork , say Microsoft is preparing to hand out ‘ many thousands ’ of NT developer kits beginning July 6 when the Windows 32 Developers Conference kicks off in San Francisco .\n",
      "2054524\n",
      "Processed 4000000 sentences\n",
      "Processed 4100000 sentences\n",
      "Processed 4200000 sentences\n",
      "Processed 4300000 sentences\n",
      "Processed 4400000 sentences\n",
      "Processed 4500000 sentences\n",
      "Processed 4600000 sentences\n",
      "Processed 4700000 sentences\n",
      "Processed 4800000 sentences\n",
      "Processed 4900000 sentences\n",
      "Processed 5000000 sentences\n",
      "Processed 5100000 sentences\n",
      "Processed 5200000 sentences\n",
      "Processed 5300000 sentences\n",
      "Processed 5400000 sentences\n",
      "Processed 5500000 sentences\n",
      "Processed 5600000 sentences\n",
      "Processed 5700000 sentences\n",
      "Processed 5800000 sentences\n",
      "Processed 5900000 sentences\n",
      "Processed 6000000 sentences\n"
     ]
    }
   ],
   "source": [
    "import grinders\n",
    "\n",
    "grinders.collect_bnc_tokens_for_words(words_to_collect, override=True, outfile='bnc_words_with_similarity_tokens.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort dataset of BNC tokens into files for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "ALLWORDS_DIR = './data/word_data'\n",
    "\n",
    "# you already have tokens collected for each word in simlex and wordsim\n",
    "# now these tokens ought to be sorted into their own files\n",
    "\n",
    "# ensure that there is a word_data directory to store in our words\n",
    "# you have to delete it first with rm -rf if we are reloading\n",
    "# os.mkdir(ALLWORDS_DIR)\n",
    "\n",
    "\n",
    "# create files for each word we care about\n",
    "for word in words_to_collect:\n",
    "    word_dir = os.path.join(ALLWORDS_DIR, word)\n",
    "    os.mkdir(word_dir)\n",
    "\n",
    "\n",
    "# read in the big long file\n",
    "with open('./data/bnc_words_with_similarity_tokens.csv', mode=\"r\") as infile:\n",
    "    fieldnames = [\"word\", \"sentence\", \"POS\", \"id\"]\n",
    "    reader = csv.DictReader(infile, delimiter=\"\\t\", quoting=csv.QUOTE_NONNUMERIC, fieldnames=fieldnames)\n",
    "    \n",
    "    # split the big long file into smaller, sorted files that are easier to process one at a time\n",
    "    for row in reader:\n",
    "        \n",
    "        word = row[\"word\"]\n",
    "        text = row[\"sentence\"]\n",
    "        pos = row[\"POS\"]\n",
    "        uid = \"BNC_\" + str(int(row[\"id\"]))\n",
    "\n",
    "        # open file for this word to spit tokens into\n",
    "        token_file = os.path.join(ALLWORDS_DIR, word, \"BNC_tokens.csv\")\n",
    "        with open(token_file, mode=\"a\") as outfile:\n",
    "            # finally, write all of the info with the vector to disk\n",
    "            writer = writer = csv.writer(outfile, delimiter='\\t', quoting=csv.QUOTE_NONNUMERIC)\n",
    "            writer.writerow([word, text, pos, uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we have files for the new words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anxiety', 'sculpture', 'surfer', 'pumpkin', 'downtown', 'yellow', 'cafe', 'reading', 'sphere', 'sunny', 'discovery', 'dude', 'jump', 'crew', 'zebra', 'trade', 'sport', 'crisis', 'concert', 'parking', 'row', 'clock', 'toy', 'lighthouse', 'forecast', 'arrangement', 'musician', 'cake', 'bead', 'keyboard', 'burger', 'windmill', 'handle', 'race', 'outfit', 'wave', 'network', 'auto', 'isolate', 'snowman', 'call', 'recovery', 'drive', 'archive', 'depression', 'arrival', 'town', 'war', 'mirror', 'reptile', 'strawberry', 'propose', 'gasoline', 'welcome', 'sand', 'credit', 'miniature', 'store', 'boardwalk', 'dragonfly', 'pug', 'military', 'research', 'lizard', 'dissipate', 'cheerleader', 'refine', 'tropical', 'temple', 'piano', 'racing', 'stone', 'interior', 'environment', 'traffic', 'candle', 'fly', 'painting', 'pattern', 'bruise', 'rainbow', 'seagull', 'hamster', 'whale', 'terminate', 'statue', 'wing', 'stripe', 'fingerprint', 'gold', 'pepper', 'blue', 'weapon', 'lily', 'noodle', 'black', 'boxer', 'truck', 'step', 'opec']\n"
     ]
    }
   ],
   "source": [
    "print(words_to_collect[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of each new word do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tokens collected for ipod\n",
      "No tokens collected for figure out\n",
      "number of unique words between sl999 and ws353 we've collected tokens for: 704\n",
      "average number of tokens per word: 97.20170454545455\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ALLWORDS_DIR = './data/word_data'\n",
    "\n",
    "\n",
    "token_counts = []\n",
    "n = 0\n",
    "\n",
    "collected_words = words_to_collect\n",
    "for word in words_to_collect:\n",
    "    wordfile = os.path.join(ALLWORDS_DIR, word, 'BNC_tokens.csv')   \n",
    "\n",
    "    try:\n",
    "        with open(wordfile, mode=\"r\") as infile:\n",
    "            fieldnames = [\"word\", \"sentence\", \"POS\", \"id\"]\n",
    "            reader = csv.DictReader(infile, delimiter=\"\\t\", quoting=csv.QUOTE_NONNUMERIC, fieldnames=fieldnames)\n",
    "\n",
    "            count = 0\n",
    "            for row in reader:\n",
    "                count +=1\n",
    "            token_counts.append(count)\n",
    "            n = n+1\n",
    "    except:\n",
    "        print(\"No tokens collected for %s\" % word)\n",
    "        collected_words.remove(word)\n",
    "average = np.sum(token_counts) / n\n",
    "print(\"number of unique words between sl999 and ws353 we've collected tokens for: %s\" % n)\n",
    "print(\"average number of tokens per word: %s\" % average)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and save clusters for these new words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/gabriellachronis/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /Users/gabriellachronis/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/9m/vzvx58rs51v_x5nm620fz4xr0000gn/T/tmpen4lrv07\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/gabriellachronis/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'i', 'suppose', 'this', 'is', 'er', '##m', ',', 'a', 'car', '##ica', '##ture', ',', 'a', 'self', 'portrait', 'this', 'little', ',', 'i', ',', 'in', 'fact', 'i', 'was', ',', 'really', 'actually', 'hilarious', 'as', 'i', 'felt', 'that', ',', 'i', ',', 'i', ',', 'do', 'n', \"'\", 't', 'actually', 'know', 'an', 'awful', 'lot', 'about', 'ga', '##ug', '##in', ',', 'but', 'if', ',', 'if', 'i', 'knew', 'nothing', 'about', 'him', 'at', 'all', ',', 'i', 'would', 'of', 'thought', 'he', 'was', 'having', 'a', 'bit', 'of', 'joke', 'of', 'himself', 'with', 'this', ',', 'but', 'er', ',', 'being', 'the', 'person', 'that', 'he', 'was', 'i', 'ca', 'n', \"'\", 't', 'image', 'that', 'he', 'had', 'that', 'quality', ',', 'that', ',', 'i', 'do', 'n', \"'\", 't', 'believe', 'he', 'would', 'be', 'laughing', 'at', 'himself', ',', 'er', '##m', ',', 'er', '##m', ',', 'the', 'symbolism', 'er', '##m', 'and', 'conflict', 'of', 'this', 'painting', 'its', 'da', '##zzle', '##s', 'me', 'more', 'than', ',', 'than', 'the', 'colour', 'or', 'at', 'least', 'as', 'much', 'as', 'the', 'colours', 'in', 'it', ',', 'but', 'there', \"'\", 's', 'a', ',', 'there', \"'\", 's', 'a', 'half', 'eaten', ',', 'well', 'it', 'is', 'n', \"'\", 't', 'half', 'eaten', ',', 'but', 'there', \"'\", 's', 'half', 'an', 'apple', 'at', 'the', 'top', 'and', ',', 'and', 'that', 'was', 'the', ',', 'the', 'way', 'into', 'me', 'finally', ',', 'for', ',', 'for', 'writing', 'about', 'this', ',', 'this', 'again', 'is', 'a', 'shopping', 'list', ',', 'i', 'call', 'it', 'a', 'shopping', 'list', ',', 'this', 'is', 'just', 'visual', 'images', 'that', ',', 'that', 'will', 'be', 'opened', 'out', 'at', 'some', 'point', 'and', 'turn', 'it', 'into', 'something', ',', 'and', 'my', 'images', 'were', 'er', '##m', 'shoulders', 'of', 'the', 'mata', '##dor', 'smoking', 'snakes', ',', 'dare', 'to', 'bit', 'an', 'apple', ',', 'see', 'one', 'half', 'gone', 'and', 'still', 'i', 'wear', 'a', 'halo', 'intact', ',', 'that', 'i', \"'\", 'm', 'sure', 'i', \"'\", 've', 'completely', 'wrong', 'about', 'him', 'as', 'a', ',', 'a', 'person', ',', 'but', 'as', 'the', 'painting', 'that', \"'\", 's', 'obviously', 'something', 'else', ',', 'er', '##m', ',', 'i', 'found', 'that', 'one', 'of', 'the', 'things', 'that', 'were', 'he', \"'\", 's', ',', 'i', ',', 'i', 'think', 'probably', 'that', 'everybody', 'who', 'writes', 'is', 'that', 'you', \"'\", 'll', 'come', 'to', 'a', 'point', 'when', 'you', 'ca', 'n', \"'\", 't', 'write', ',', 'you', 'stop', 'writing', ',', 'you', 'have', 'n', \"'\", 't', 'got', 'anything', 'you', 'want', 'to', 'write', 'about', ',', 'or', 'your', 'frightened', 'of', 'writing', ',', 'and', 'i', 'devi', '##se', 'exercises', 'so', 'that', ',', 'that', 'does', 'n', \"'\", 't', 'happened', 'to', 'me', ',', 'i', 'think', 'writing', 'is', 'like', 'any', 'skill', 'you', 'have', 'to', 'keep', 'doing', 'it', 'to', 'be', 'able', 'to', 'do', 'it', ',', 'its', ',', 'you', ',', 'some', 'of', 'it', 'is', 'a', 'game', 'and', 'the', 'rest', 'of', 'it', 'is', 'hard', 'work', ',', 'and', 'one', 'of', 'the', 'exercises', 'i', ',', 'i', 'delighted', 'using', 'er', '##m', 'a', 'portrait', 'of', 'a', 'woman', 'er', '##m', ',', 'its', 'about', 'er', '##m', 'the', 'er', 'still', 'life', ',', 'its', 'the', 'back', 'one', ',', 'yes', 'this', 'one', 'here', ',', 'i', 'have', ',', 'i', ',', 'i', 'hope', 'to', 'use', 'this', 'as', 'a', 'writing', 'exercise', 'i', 'found', 'the', ',', 'the', 'math', '##s', 'in', 'this', 'and', 'the', 'colour', 'of', 'the', 'piece', 'of', 'fruit', 'in', 'the', 'background', ',', 'very', 'interesting', 'because', 'most', 'of', 'the', 'colours', 'to', 'me', 'seem', 'a', ',', 'a', 'lot', 'less', 'vibrant', 'then', 'many', 'of', 'his', 'other', 'paintings', ',', 'and', 'so', 'they', ',', 'they', 'attracted', 'me', 'and', 'have', 'a', ',', 'a', 'strong', 'sense', 'of', 'er', '##m', ',', 'er', 'a', 'hidden', 'desire', 'in', 'that', 'and', 'so', 'it', ',', 'to', 'use', 'it', 'as', 'a', 'writing', 'exercise', 'which', 'i', 'intend', 'doing', ',', 'it', 'will', 'be', 'able', 'about', 'a', 'situation', 'of', 'character', \"'\", 's', 'with', 'er', '##m', 'something', 'that', \"'\", 's', 'hidden', ',', 'some', 'desire', ',', 'i', 'do', 'n', \"'\", 't', ',', 'the', ',', 'not', 'even', 'spoke', 'about', 'to', 'themselves', 'or', ',', 'or', 'generally', ',', 'er', '##m', 'i', 'like', 'to', 'sort', 'of', 'say', 'that', 'came', 'from', 'those', 'two', 'little', ',', 'just', 'this', 'amounts', 'of', 'colour', 'which', 'seemed', 'to', 'be', 'saying', 'such', 'a', 'lot', '[SEP]']\n",
      "processed 100 words\n",
      "calculating clusters for opec\n",
      "processed 200 words\n",
      "calculating clusters for santa\n",
      "not enough tokens to make 5 clusters for word: donut\n",
      "not enough tokens to make 7 clusters for word: donut\n",
      "not enough tokens to make 5 clusters for word: donut\n",
      "not enough tokens to make 7 clusters for word: donut\n",
      "not enough tokens to make 5 clusters for word: donut\n",
      "not enough tokens to make 7 clusters for word: donut\n",
      "not enough tokens to make 5 clusters for word: donut\n",
      "not enough tokens to make 7 clusters for word: donut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'i', 'suppose', 'this', 'is', 'er', '##m', ',', 'a', 'car', '##ica', '##ture', ',', 'a', 'self', 'portrait', 'this', 'little', ',', 'i', ',', 'in', 'fact', 'i', 'was', ',', 'really', 'actually', 'hilarious', 'as', 'i', 'felt', 'that', ',', 'i', ',', 'i', ',', 'do', 'n', \"'\", 't', 'actually', 'know', 'an', 'awful', 'lot', 'about', 'ga', '##ug', '##in', ',', 'but', 'if', ',', 'if', 'i', 'knew', 'nothing', 'about', 'him', 'at', 'all', ',', 'i', 'would', 'of', 'thought', 'he', 'was', 'having', 'a', 'bit', 'of', 'joke', 'of', 'himself', 'with', 'this', ',', 'but', 'er', ',', 'being', 'the', 'person', 'that', 'he', 'was', 'i', 'ca', 'n', \"'\", 't', 'image', 'that', 'he', 'had', 'that', 'quality', ',', 'that', ',', 'i', 'do', 'n', \"'\", 't', 'believe', 'he', 'would', 'be', 'laughing', 'at', 'himself', ',', 'er', '##m', ',', 'er', '##m', ',', 'the', 'symbolism', 'er', '##m', 'and', 'conflict', 'of', 'this', 'painting', 'its', 'da', '##zzle', '##s', 'me', 'more', 'than', ',', 'than', 'the', 'colour', 'or', 'at', 'least', 'as', 'much', 'as', 'the', 'colours', 'in', 'it', ',', 'but', 'there', \"'\", 's', 'a', ',', 'there', \"'\", 's', 'a', 'half', 'eaten', ',', 'well', 'it', 'is', 'n', \"'\", 't', 'half', 'eaten', ',', 'but', 'there', \"'\", 's', 'half', 'an', 'apple', 'at', 'the', 'top', 'and', ',', 'and', 'that', 'was', 'the', ',', 'the', 'way', 'into', 'me', 'finally', ',', 'for', ',', 'for', 'writing', 'about', 'this', ',', 'this', 'again', 'is', 'a', 'shopping', 'list', ',', 'i', 'call', 'it', 'a', 'shopping', 'list', ',', 'this', 'is', 'just', 'visual', 'images', 'that', ',', 'that', 'will', 'be', 'opened', 'out', 'at', 'some', 'point', 'and', 'turn', 'it', 'into', 'something', ',', 'and', 'my', 'images', 'were', 'er', '##m', 'shoulders', 'of', 'the', 'mata', '##dor', 'smoking', 'snakes', ',', 'dare', 'to', 'bit', 'an', 'apple', ',', 'see', 'one', 'half', 'gone', 'and', 'still', 'i', 'wear', 'a', 'halo', 'intact', ',', 'that', 'i', \"'\", 'm', 'sure', 'i', \"'\", 've', 'completely', 'wrong', 'about', 'him', 'as', 'a', ',', 'a', 'person', ',', 'but', 'as', 'the', 'painting', 'that', \"'\", 's', 'obviously', 'something', 'else', ',', 'er', '##m', ',', 'i', 'found', 'that', 'one', 'of', 'the', 'things', 'that', 'were', 'he', \"'\", 's', ',', 'i', ',', 'i', 'think', 'probably', 'that', 'everybody', 'who', 'writes', 'is', 'that', 'you', \"'\", 'll', 'come', 'to', 'a', 'point', 'when', 'you', 'ca', 'n', \"'\", 't', 'write', ',', 'you', 'stop', 'writing', ',', 'you', 'have', 'n', \"'\", 't', 'got', 'anything', 'you', 'want', 'to', 'write', 'about', ',', 'or', 'your', 'frightened', 'of', 'writing', ',', 'and', 'i', 'devi', '##se', 'exercises', 'so', 'that', ',', 'that', 'does', 'n', \"'\", 't', 'happened', 'to', 'me', ',', 'i', 'think', 'writing', 'is', 'like', 'any', 'skill', 'you', 'have', 'to', 'keep', 'doing', 'it', 'to', 'be', 'able', 'to', 'do', 'it', ',', 'its', ',', 'you', ',', 'some', 'of', 'it', 'is', 'a', 'game', 'and', 'the', 'rest', 'of', 'it', 'is', 'hard', 'work', ',', 'and', 'one', 'of', 'the', 'exercises', 'i', ',', 'i', 'delighted', 'using', 'er', '##m', 'a', 'portrait', 'of', 'a', 'woman', 'er', '##m', ',', 'its', 'about', 'er', '##m', 'the', 'er', 'still', 'life', ',', 'its', 'the', 'back', 'one', ',', 'yes', 'this', 'one', 'here', ',', 'i', 'have', ',', 'i', ',', 'i', 'hope', 'to', 'use', 'this', 'as', 'a', 'writing', 'exercise', 'i', 'found', 'the', ',', 'the', 'math', '##s', 'in', 'this', 'and', 'the', 'colour', 'of', 'the', 'piece', 'of', 'fruit', 'in', 'the', 'background', ',', 'very', 'interesting', 'because', 'most', 'of', 'the', 'colours', 'to', 'me', 'seem', 'a', ',', 'a', 'lot', 'less', 'vibrant', 'then', 'many', 'of', 'his', 'other', 'paintings', ',', 'and', 'so', 'they', ',', 'they', 'attracted', 'me', 'and', 'have', 'a', ',', 'a', 'strong', 'sense', 'of', 'er', '##m', ',', 'er', 'a', 'hidden', 'desire', 'in', 'that', 'and', 'so', 'it', ',', 'to', 'use', 'it', 'as', 'a', 'writing', 'exercise', 'which', 'i', 'intend', 'doing', ',', 'it', 'will', 'be', 'able', 'about', 'a', 'situation', 'of', 'character', \"'\", 's', 'with', 'er', '##m', 'something', 'that', \"'\", 's', 'hidden', ',', 'some', 'desire', ',', 'i', 'do', 'n', \"'\", 't', ',', 'the', ',', 'not', 'even', 'spoke', 'about', 'to', 'themselves', 'or', ',', 'or', 'generally', ',', 'er', '##m', 'i', 'like', 'to', 'sort', 'of', 'say', 'that', 'came', 'from', 'those', 'two', 'little', ',', 'just', 'this', 'amounts', 'of', 'colour', 'which', 'seemed', 'to', 'be', 'saying', 'such', 'a', 'lot', '[SEP]']\n",
      "not enough tokens to make 3 clusters for word: commercialize\n",
      "not enough tokens to make 5 clusters for word: commercialize\n",
      "not enough tokens to make 7 clusters for word: commercialize\n",
      "not enough tokens to make 3 clusters for word: commercialize\n",
      "not enough tokens to make 5 clusters for word: commercialize\n",
      "not enough tokens to make 7 clusters for word: commercialize\n",
      "not enough tokens to make 3 clusters for word: commercialize\n",
      "not enough tokens to make 5 clusters for word: commercialize\n",
      "not enough tokens to make 7 clusters for word: commercialize\n",
      "not enough tokens to make 3 clusters for word: commercialize\n",
      "not enough tokens to make 5 clusters for word: commercialize\n",
      "not enough tokens to make 7 clusters for word: commercialize\n",
      "processed 300 words\n",
      "calculating clusters for face\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'i', 'suppose', 'this', 'is', 'er', '##m', ',', 'a', 'car', '##ica', '##ture', ',', 'a', 'self', 'portrait', 'this', 'little', ',', 'i', ',', 'in', 'fact', 'i', 'was', ',', 'really', 'actually', 'hilarious', 'as', 'i', 'felt', 'that', ',', 'i', ',', 'i', ',', 'do', 'n', \"'\", 't', 'actually', 'know', 'an', 'awful', 'lot', 'about', 'ga', '##ug', '##in', ',', 'but', 'if', ',', 'if', 'i', 'knew', 'nothing', 'about', 'him', 'at', 'all', ',', 'i', 'would', 'of', 'thought', 'he', 'was', 'having', 'a', 'bit', 'of', 'joke', 'of', 'himself', 'with', 'this', ',', 'but', 'er', ',', 'being', 'the', 'person', 'that', 'he', 'was', 'i', 'ca', 'n', \"'\", 't', 'image', 'that', 'he', 'had', 'that', 'quality', ',', 'that', ',', 'i', 'do', 'n', \"'\", 't', 'believe', 'he', 'would', 'be', 'laughing', 'at', 'himself', ',', 'er', '##m', ',', 'er', '##m', ',', 'the', 'symbolism', 'er', '##m', 'and', 'conflict', 'of', 'this', 'painting', 'its', 'da', '##zzle', '##s', 'me', 'more', 'than', ',', 'than', 'the', 'colour', 'or', 'at', 'least', 'as', 'much', 'as', 'the', 'colours', 'in', 'it', ',', 'but', 'there', \"'\", 's', 'a', ',', 'there', \"'\", 's', 'a', 'half', 'eaten', ',', 'well', 'it', 'is', 'n', \"'\", 't', 'half', 'eaten', ',', 'but', 'there', \"'\", 's', 'half', 'an', 'apple', 'at', 'the', 'top', 'and', ',', 'and', 'that', 'was', 'the', ',', 'the', 'way', 'into', 'me', 'finally', ',', 'for', ',', 'for', 'writing', 'about', 'this', ',', 'this', 'again', 'is', 'a', 'shopping', 'list', ',', 'i', 'call', 'it', 'a', 'shopping', 'list', ',', 'this', 'is', 'just', 'visual', 'images', 'that', ',', 'that', 'will', 'be', 'opened', 'out', 'at', 'some', 'point', 'and', 'turn', 'it', 'into', 'something', ',', 'and', 'my', 'images', 'were', 'er', '##m', 'shoulders', 'of', 'the', 'mata', '##dor', 'smoking', 'snakes', ',', 'dare', 'to', 'bit', 'an', 'apple', ',', 'see', 'one', 'half', 'gone', 'and', 'still', 'i', 'wear', 'a', 'halo', 'intact', ',', 'that', 'i', \"'\", 'm', 'sure', 'i', \"'\", 've', 'completely', 'wrong', 'about', 'him', 'as', 'a', ',', 'a', 'person', ',', 'but', 'as', 'the', 'painting', 'that', \"'\", 's', 'obviously', 'something', 'else', ',', 'er', '##m', ',', 'i', 'found', 'that', 'one', 'of', 'the', 'things', 'that', 'were', 'he', \"'\", 's', ',', 'i', ',', 'i', 'think', 'probably', 'that', 'everybody', 'who', 'writes', 'is', 'that', 'you', \"'\", 'll', 'come', 'to', 'a', 'point', 'when', 'you', 'ca', 'n', \"'\", 't', 'write', ',', 'you', 'stop', 'writing', ',', 'you', 'have', 'n', \"'\", 't', 'got', 'anything', 'you', 'want', 'to', 'write', 'about', ',', 'or', 'your', 'frightened', 'of', 'writing', ',', 'and', 'i', 'devi', '##se', 'exercises', 'so', 'that', ',', 'that', 'does', 'n', \"'\", 't', 'happened', 'to', 'me', ',', 'i', 'think', 'writing', 'is', 'like', 'any', 'skill', 'you', 'have', 'to', 'keep', 'doing', 'it', 'to', 'be', 'able', 'to', 'do', 'it', ',', 'its', ',', 'you', ',', 'some', 'of', 'it', 'is', 'a', 'game', 'and', 'the', 'rest', 'of', 'it', 'is', 'hard', 'work', ',', 'and', 'one', 'of', 'the', 'exercises', 'i', ',', 'i', 'delighted', 'using', 'er', '##m', 'a', 'portrait', 'of', 'a', 'woman', 'er', '##m', ',', 'its', 'about', 'er', '##m', 'the', 'er', 'still', 'life', ',', 'its', 'the', 'back', 'one', ',', 'yes', 'this', 'one', 'here', ',', 'i', 'have', ',', 'i', ',', 'i', 'hope', 'to', 'use', 'this', 'as', 'a', 'writing', 'exercise', 'i', 'found', 'the', ',', 'the', 'math', '##s', 'in', 'this', 'and', 'the', 'colour', 'of', 'the', 'piece', 'of', 'fruit', 'in', 'the', 'background', ',', 'very', 'interesting', 'because', 'most', 'of', 'the', 'colours', 'to', 'me', 'seem', 'a', ',', 'a', 'lot', 'less', 'vibrant', 'then', 'many', 'of', 'his', 'other', 'paintings', ',', 'and', 'so', 'they', ',', 'they', 'attracted', 'me', 'and', 'have', 'a', ',', 'a', 'strong', 'sense', 'of', 'er', '##m', ',', 'er', 'a', 'hidden', 'desire', 'in', 'that', 'and', 'so', 'it', ',', 'to', 'use', 'it', 'as', 'a', 'writing', 'exercise', 'which', 'i', 'intend', 'doing', ',', 'it', 'will', 'be', 'able', 'about', 'a', 'situation', 'of', 'character', \"'\", 's', 'with', 'er', '##m', 'something', 'that', \"'\", 's', 'hidden', ',', 'some', 'desire', ',', 'i', 'do', 'n', \"'\", 't', ',', 'the', ',', 'not', 'even', 'spoke', 'about', 'to', 'themselves', 'or', ',', 'or', 'generally', ',', 'er', '##m', 'i', 'like', 'to', 'sort', 'of', 'say', 'that', 'came', 'from', 'those', 'two', 'little', ',', 'just', 'this', 'amounts', 'of', 'colour', 'which', 'seemed', 'to', 'be', 'saying', 'such', 'a', 'lot', '[SEP]']\n",
      "processed 400 words\n",
      "calculating clusters for sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (661 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'oh', ':', 'car', 'of', 'menace', ',', 'car', 'of', 'b', '##light', 'cars', 'the', 'atmosphere', 'ign', '##ite', 'greenhouse', 'warming', ',', 'havoc', 'forming', 'parkinson', 'must', 'see', 'us', 'right', 'we', \"'\", 're', 'au', 'fai', '##t', 'with', 'entropy', 'ga', '##ia', ',', 'eco', '##sphere', ',', 'syn', '##ergy', 'words', 'for', 'green', '##ing', 'but', 'their', 'meaning', \"'\", 's', 'a', 'linguistic', 'mystery', 'oh', ':', 'politicians', 'must', 'in', '##vent', 'worship', 'of', 'environment', 'gen', '##uf', '##le', '##cting', 'by', 'reflecting', 'words', 'of', 've', '##rdan', '##t', 'ba', '##ffle', '##ment', 'now', 'our', 'water', \"'\", 's', 'unfit', 'to', 'drink', 'too', 'much', 'aluminium', 'and', 'zinc', 'no', 'solution', 'to', 'pollution', 'no', '-', 'one', 'can', 'stand', 'the', 'stink', 'oh', ':', 'public', 'fi', '##lth', 'and', 'pest', '##ile', '##nce', 'highlights', 'private', 'op', '##ule', '##nce', 'does', 'the', 'glitter', ',', 'clear', 'the', 'litter', 'all', 'it', 'needs', 'is', 'pounds', 'and', 'pen', '##ce', 'god', 'rest', 'ye', 'merry', 'god', 'rest', 'ye', 'merry', ',', 'gentle', 'greens', 'let', 'nothing', 'you', 'dismay', 'the', 'much', 'fore', '##to', '##ld', 'apocalypse', 'is', 'now', 'well', 'under', 'way', 'not', 'even', 'mr', 'go', '##rba', '##chev', 'can', 'stop', 'the', 'world', \"'\", 's', 'decay', 'oh', ',', 'ti', '##ding', '##s', 'of', 'comfort', 'and', 'joy', ',', 'oh', ',', 'ti', '##ding', '##s', 'of', 'comfort', 'and', 'joy', 'we', 'ca', 'n', \"'\", 't', 'eat', 'meat', 'or', 'hen', '##s', 'or', 'fish', 'the', 'farming', 'is', 'too', 'cruel', 'the', 'only', 'food', 'we', 'now', 'permit', 'is', 'foul', 'organic', 'gr', '##uel', 'ir', '##rad', '##iated', 'food', 'and', 've', '##g', 'now', 'double', 'up', 'as', 'fuel', 'oh', 'ti', '##ding', '##s', 'of', 'comfort', 'and', 'joy', ',', 'oh', 'ti', '##ding', '##s', 'of', 'comfort', 'and', 'joy', 'now', 'everybody', 'wants', 'a', 'car', 'though', 'noise', 'and', 'fu', '##mes', 'are', 'vile', 'the', 'iron', 'curtain', 'fractures', 'and', 'the', 'jam', '##s', 'stretch', 'back', 'for', 'miles', 'mobility', 'and', 'liberty', 'can', 'not', 'be', 'reconciled', 'oh', 'ti', '##ding', '##s', 'of', 'comfort', 'and', 'joy', ',', 'oh', 'ti', '##ding', '##s', 'of', 'comfort', 'and', 'joy', 'we', 'think', 'that', 'greenhouse', 'warming', 'will', 'bring', 'on', 'the', 'world', \"'\", 's', 'demise', 'if', 'forests', 'burn', 'it', \"'\", 's', 'not', 'just', 'trees', 'but', 'mankind', 'too', 'that', 'fries', 'but', 'all', 'this', 'may', 'be', 'garbage', 'because', 'scientists', 'tell', 'lies', 'oh', 'ti', '##ding', '##s', 'of', 'comfort', 'and', 'joy', ',', 'oh', 'ti', '##ding', '##s', 'of', 'comfort', 'and', 'joy', 'consumers', 'must', 'buy', 'less', 'to', 'reach', 'sustainability', 'for', 'gallo', '##ping', 'consumption', 'is', 'more', 'lethal', 'than', 'tb', 'so', 'much', 'for', 'third', 'world', 'dreams', 'of', 'fleeing', 'grinding', 'poverty', 'oh', 'ti', '##ding', '##s', 'of', 'comfort', 'and', 'joy', ',', 'oh', 'ti', '##ding', '##s', 'of', 'comfort', 'and', 'joy', 'unless', 'we', 'stop', 'producing', 'kids', 'the', 'planet', 'will', 'not', 'cope', 'no', 'hope', 'for', 'birth', 'controllers', 'short', 'of', 'kidnapping', 'the', 'pope', 'but', 'since', 'the', 'greens', 'rec', '##y', '##cle', 'people', 'they', 'may', 'turn', 'us', 'into', 'soap', 'oh', 'ti', '##ding', '##s', 'of', 'comfort', 'and', 'joy', ',', 'oh', 'ti', '##ding', '##s', 'of', 'comfort', 'and', 'joy', 'in', 'searching', 'for', 'clean', 'energy', 'the', 'choice', 'is', 'pretty', 'stark', 'the', 'floods', 'that', 'come', 'when', 'coal', 'is', 'burnt', 'will', 'keep', 'us', 'in', 'the', 'ark', 'but', 'had', 'lord', 'marshall', 'got', 'his', 'way', 'we', \"'\", 'd', 'all', 'glow', 'in', 'the', 'dark', 'oh', 'ti', '##ding', '##s', 'of', 'comfort', 'and', 'joy', ',', 'oh', 'ti', '##ding', '##s', 'of', 'comfort', 'and', 'joy', 'dumb', 'animals', 'are', 'much', 'prefer', '##rred', 'to', 'flawed', 'humanity', 'ill', '-', 'treatment', 'of', 'old', 'people', 'may', 'provoke', 'insanity', 'but', 'cu', '##lling', 'seals', 'and', 'whales', 'is', 'judged', 'the', 'worst', 'prof', '##ani', '##ty', 'oh', 'ti', '##ding', '##s', 'of', 'comfort', 'and', 'joy', ',', 'oh', 'ti', '##ding', '##s', 'of', 'comfort', 'and', 'joy', 'now', 'mrs', 'thatcher', 'goes', 'bright', 'green', 'a', 'highly', 'suspect', 'hue', 'her', 'policies', 'have', 'after', 'all', 'kept', 'fi', '##lth', 'and', 'sq', '##ual', '##or', 'blue', 'it', \"'\", 's', 'just', 'another', 'way', 'she', \"'\", 's', 'found', 'to', 'tell', 'us', 'what', 'to', 'do', 'oh', ',', 'ti', '##ding', '##s', 'of', 'comfort', 'and', 'joy', ',', 'oh', 'ti', '##ding', '##s', 'of', 'comfort', 'and', 'joy', 'with', 'greenhouse', 'gases', ',', 'pc', '##bs', 'sul', '##ph', '##uri', '##c', 'acid', 'rain', 'this', 'fragile', 'globe', \"'\", 's', 'environment', 'is', 'going', 'down', 'the', 'drain', 'it', 'is', 'a', 'cosmic', 'punishment', 'that', 'we', 'ca', 'n', \"'\", 't', 'start', 'again', 'oh', ',', 'ti', '##ding', '##s', 'of', 'comfort', 'and', 'joy', ',', 'oh', 'ti', '##ding', '##s', 'of', 'comfort', 'and', 'joy', 'melanie', 'phillips', 'overnight', 'file', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (781 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'bel', '##grave', '##s', 'league', 'division', 'one', ';', 'aston', 'villa', 'three', ',', 'tottenham', 'hotspur', 'two', ';', 'crystal', 'palace', 'two', ',', 'derby', 'county', 'one', ';', 'liverpool', 'two', ',', 'sunderland', 'one', ';', 'luton', 'town', 'ni', '##l', ',', 'norwich', 'city', 'one', ';', 'manchester', 'city', 'one', ',', 'wimbledon', 'one', ';', 'nottingham', 'forest', 'one', ',', 'manchester', 'united', 'one', ';', 'queens', 'park', 'rangers', 'one', ',', 'coventry', 'city', 'ni', '##l', ';', 'sheffield', 'united', 'one', ',', 'chelsea', 'ni', '##l', ';', 'southampton', 'three', ',', 'everton', 'four', ';', 'division', 'two', ';', 'barnsley', 'one', ',', 'charlton', 'athletic', 'one', ';', 'brighton', 'and', 'hove', 'albion', 'one', ',', 'blackburn', 'rovers', 'ni', '##l', ';', 'bristol', 'rovers', 'one', ',', 'not', '##ts', 'county', 'one', ';', 'leicester', 'city', 'four', ',', 'middlesbrough', 'three', ';', 'mil', '##wall', 'one', ',', 'swindon', 'town', 'ni', '##l', ';', 'newcastle', 'united', 'ni', '##l', ',', 'bristol', 'city', 'ni', '##l', ';', 'oldham', 'athletic', 'two', ',', 'west', 'bromwich', 'albion', 'one', ';', 'plymouth', 'argyll', 'one', ',', 'portsmouth', 'one', ';', 'port', 'vale', 'ni', '##l', ',', 'hull', 'city', 'ni', '##l', ';', 'watford', 'one', ',', 'ipswich', 'town', 'one', ';', 'west', 'ham', 'united', 'one', ',', 'sheffield', 'wednesday', 'three', ';', 'wolverhampton', 'wanderers', 'three', ',', 'oxford', 'united', 'three', ';', 'division', 'three', ';', 'bolton', 'wanderers', 'two', ',', 'wigan', 'athletic', 'one', ';', 'bradford', 'city', 'four', ',', 'ley', '##ton', 'orient', 'ni', '##l', ';', 'cambridge', 'united', 'one', ',', 'exeter', 'city', 'ni', '##l', ';', 'crewe', 'alexandra', 'two', ',', 'bury', 'two', ';', 'fulham', 'one', ',', 'bournemouth', 'one', ';', 'grimsby', 'town', 'two', ',', 'brentford', 'ni', '##l', ';', 'huddersfield', 'town', 'one', ',', 'chester', 'city', 'one', ';', 'preston', 'north', '##end', 'two', ',', 'birmingham', 'ni', '##l', ';', 'reading', 'two', ',', 'rotherham', 'united', 'ni', '##l', ';', 'shrewsbury', 'town', 'two', ',', 'stoke', 'city', 'ni', '##l', ';', 'division', 'four', ';', 'burnley', 'two', ',', 'carlisle', 'united', 'one', ';', 'cardiff', 'city', 'one', ',', 'sc', '##unt', '##horpe', 'united', 'ni', '##l', ';', 'chesterfield', 'two', ',', 'wrexham', 'one', ';', 'darlington', 'one', ',', 'doncaster', 'rovers', 'one', ';', 'gill', '##ingham', 'ni', '##l', ',', 'york', 'city', 'ni', '##l', ';', 'hart', '##le', '##pool', 'united', 'one', ',', 'blackpool', 'two', ';', 'lincoln', 'city', 'two', ',', 'maids', '##tone', 'united', 'one', ';', 'scarborough', 'two', ',', 'al', '##ders', '##hot', 'ni', '##l', ';', 'stockport', 'county', 'four', ',', 'hereford', 'united', 'two', ';', 'walsall', 'ni', '##l', ',', 'rochdale', 'one', ';', 'in', 'the', 'f', 'a', 'trophy', 'fourth', 'round', ';', 'alt', '##ring', '##ham', 'five', ',', 'ho', '##r', '##wich', 'ni', '##l', ';', 'colchester', 'united', 'two', ',', 'wit', '##ton', 'albion', 'ni', '##l', ';', 'kidd', '##er', '##minster', 'three', ',', 'em', '##ley', 'ni', '##l', ';', 'north', '##wich', 'victoria', 'two', ',', 'w', '##y', '##combe', 'wanderers', 'three', ';', 'in', 'the', 'h', 'f', 's', 'loans', 'league', ',', 'premier', 'division', ';', 'cho', '##rley', 'five', ',', 'gains', '##borough', 'three', ';', 'fleetwood', 'four', ',', 'hyde', 'one', ';', 'moss', '##ley', 'two', ',', 'goo', '##le', 'ni', '##l', ';', 'she', '##ps', '##hed', 'ni', '##l', ',', 'marine', 'two', ';', 'south', 'liverpool', 'ni', '##l', ',', 'lee', '##k', 'one', ';', 'stale', '##y', '##bridge', 'three', ',', 'bangor', 'one', ';', 'the', 'bee', '##zer', 'homes', 'league', ',', 'premier', 'division', ';', 'at', '##hers', '##ton', 'one', ',', 'cambridge', 'united', 'one', ',', 'i', \"'\", 'm', 'sorry', ',', 'cambridge', 'city', 'one', ';', 'moor', 'green', 'one', ',', 'we', '##ym', '##outh', 'two', ';', 'poole', 'one', ',', 'worcester', 'two', ';', 'rush', '##ton', 'four', ',', 'hale', '##s', 'owen', 'three', ';', 'the', 'waterloo', '##ville', 'versus', 'che', '##lm', '##sford', 'result', 'is', 'a', 'late', 'kick', 'off', ';', 'the', 'va', '##ux', '##hall', 'league', ',', 'premier', 'division', ';', 'basin', '##gs', '##tok', '##e', 'ni', '##l', ',', 'st', 'albans', 'four', ';', 'bishops', 'st', '##ort', '##ford', 'two', ',', 'wo', '##king', 'ni', '##l', ';', 'bog', '##nor', 'regis', 'two', ',', 'da', '##gen', '##ham', 'ni', '##l', ';', 'kingston', '##ian', 'two', ',', 'harrow', 'ni', '##l', ';', 'ley', '##ton', 'wing', '##ate', 'one', ',', 'wo', '##king', '##ham', 'four', ';', 'mar', '##low', 'one', ',', 'grey', '##s', 'athletic', 'ni', '##l', ';', 'in', 'the', 'ten', '##nant', '##s', 'scottish', 'cup', 'quarter', 'finals', ';', 'mother', '##well', 'ni', '##l', ',', 'morton', 'ni', '##l', ';', 'st', 'johnston', 'five', ',', 'a', '##yr', 'united', 'two', ';', 'in', 'the', 'scottish', 'league', ',', 'premier', 'division', ';', 'dun', '##fer', '##ml', '##ine', 'one', ',', 'hi', '##ber', '##nian', 'one', ';', 'division', 'one', ';', 'aid', '##rie', 'two', ',', 'ki', '##lma', '##rno', '##ck', 'ni', '##l', ';', 'clyde', '##bank', 'one', ',', 'br', '##eck', '##in', 'city', 'ni', '##l', ';', 'dundee', 'one', ',', 'for', '##far', 'ni', '##l', ';', 'fa', '##lkirk', 'three', ',', 'hamilton', 'ni', '##l', ';', 'part', '##ick', 'two', ',', 'meadow', '##bank', 'four', ';', 'wr', '##aith', 'rovers', 'one', ',', 'clyde', 'ni', '##l', ';', 'division', 'two', ';', 'albion', 'ni', '##l', ',', 'ste', '##nh', '##ouse', 'muir', 'ni', '##l', ';', 'all', '##oa', 'two', ',', 'dunbar', '##ton', 'one', ';', 'ar', '##bro', '##ath', 'ni', '##l', ',', 'east', 'stirling', 'one', ';', 'bury', 'two', ',', 'queens', 'park', 'one', ';', 'cow', '##den', 'beat', '##h', 'two', ',', 'st', '##ran', '##rae', '##r', 'ni', '##l', ';', 'east', 'fife', 'two', ',', 'montrose', 'two', ';', 'and', 'finally', ',', 'queen', 'of', 'the', 'south', 'ni', '##l', ',', 'stirling', 'albion', 'ni', '##l', '.', '[SEP]']\n",
      "not enough tokens to make 7 clusters for word: colorful\n",
      "not enough tokens to make 7 clusters for word: colorful\n",
      "not enough tokens to make 7 clusters for word: colorful\n",
      "not enough tokens to make 7 clusters for word: colorful\n",
      "processed 500 words\n",
      "calculating clusters for design\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'i', 'suppose', 'this', 'is', 'er', '##m', ',', 'a', 'car', '##ica', '##ture', ',', 'a', 'self', 'portrait', 'this', 'little', ',', 'i', ',', 'in', 'fact', 'i', 'was', ',', 'really', 'actually', 'hilarious', 'as', 'i', 'felt', 'that', ',', 'i', ',', 'i', ',', 'do', 'n', \"'\", 't', 'actually', 'know', 'an', 'awful', 'lot', 'about', 'ga', '##ug', '##in', ',', 'but', 'if', ',', 'if', 'i', 'knew', 'nothing', 'about', 'him', 'at', 'all', ',', 'i', 'would', 'of', 'thought', 'he', 'was', 'having', 'a', 'bit', 'of', 'joke', 'of', 'himself', 'with', 'this', ',', 'but', 'er', ',', 'being', 'the', 'person', 'that', 'he', 'was', 'i', 'ca', 'n', \"'\", 't', 'image', 'that', 'he', 'had', 'that', 'quality', ',', 'that', ',', 'i', 'do', 'n', \"'\", 't', 'believe', 'he', 'would', 'be', 'laughing', 'at', 'himself', ',', 'er', '##m', ',', 'er', '##m', ',', 'the', 'symbolism', 'er', '##m', 'and', 'conflict', 'of', 'this', 'painting', 'its', 'da', '##zzle', '##s', 'me', 'more', 'than', ',', 'than', 'the', 'colour', 'or', 'at', 'least', 'as', 'much', 'as', 'the', 'colours', 'in', 'it', ',', 'but', 'there', \"'\", 's', 'a', ',', 'there', \"'\", 's', 'a', 'half', 'eaten', ',', 'well', 'it', 'is', 'n', \"'\", 't', 'half', 'eaten', ',', 'but', 'there', \"'\", 's', 'half', 'an', 'apple', 'at', 'the', 'top', 'and', ',', 'and', 'that', 'was', 'the', ',', 'the', 'way', 'into', 'me', 'finally', ',', 'for', ',', 'for', 'writing', 'about', 'this', ',', 'this', 'again', 'is', 'a', 'shopping', 'list', ',', 'i', 'call', 'it', 'a', 'shopping', 'list', ',', 'this', 'is', 'just', 'visual', 'images', 'that', ',', 'that', 'will', 'be', 'opened', 'out', 'at', 'some', 'point', 'and', 'turn', 'it', 'into', 'something', ',', 'and', 'my', 'images', 'were', 'er', '##m', 'shoulders', 'of', 'the', 'mata', '##dor', 'smoking', 'snakes', ',', 'dare', 'to', 'bit', 'an', 'apple', ',', 'see', 'one', 'half', 'gone', 'and', 'still', 'i', 'wear', 'a', 'halo', 'intact', ',', 'that', 'i', \"'\", 'm', 'sure', 'i', \"'\", 've', 'completely', 'wrong', 'about', 'him', 'as', 'a', ',', 'a', 'person', ',', 'but', 'as', 'the', 'painting', 'that', \"'\", 's', 'obviously', 'something', 'else', ',', 'er', '##m', ',', 'i', 'found', 'that', 'one', 'of', 'the', 'things', 'that', 'were', 'he', \"'\", 's', ',', 'i', ',', 'i', 'think', 'probably', 'that', 'everybody', 'who', 'writes', 'is', 'that', 'you', \"'\", 'll', 'come', 'to', 'a', 'point', 'when', 'you', 'ca', 'n', \"'\", 't', 'write', ',', 'you', 'stop', 'writing', ',', 'you', 'have', 'n', \"'\", 't', 'got', 'anything', 'you', 'want', 'to', 'write', 'about', ',', 'or', 'your', 'frightened', 'of', 'writing', ',', 'and', 'i', 'devi', '##se', 'exercises', 'so', 'that', ',', 'that', 'does', 'n', \"'\", 't', 'happened', 'to', 'me', ',', 'i', 'think', 'writing', 'is', 'like', 'any', 'skill', 'you', 'have', 'to', 'keep', 'doing', 'it', 'to', 'be', 'able', 'to', 'do', 'it', ',', 'its', ',', 'you', ',', 'some', 'of', 'it', 'is', 'a', 'game', 'and', 'the', 'rest', 'of', 'it', 'is', 'hard', 'work', ',', 'and', 'one', 'of', 'the', 'exercises', 'i', ',', 'i', 'delighted', 'using', 'er', '##m', 'a', 'portrait', 'of', 'a', 'woman', 'er', '##m', ',', 'its', 'about', 'er', '##m', 'the', 'er', 'still', 'life', ',', 'its', 'the', 'back', 'one', ',', 'yes', 'this', 'one', 'here', ',', 'i', 'have', ',', 'i', ',', 'i', 'hope', 'to', 'use', 'this', 'as', 'a', 'writing', 'exercise', 'i', 'found', 'the', ',', 'the', 'math', '##s', 'in', 'this', 'and', 'the', 'colour', 'of', 'the', 'piece', 'of', 'fruit', 'in', 'the', 'background', ',', 'very', 'interesting', 'because', 'most', 'of', 'the', 'colours', 'to', 'me', 'seem', 'a', ',', 'a', 'lot', 'less', 'vibrant', 'then', 'many', 'of', 'his', 'other', 'paintings', ',', 'and', 'so', 'they', ',', 'they', 'attracted', 'me', 'and', 'have', 'a', ',', 'a', 'strong', 'sense', 'of', 'er', '##m', ',', 'er', 'a', 'hidden', 'desire', 'in', 'that', 'and', 'so', 'it', ',', 'to', 'use', 'it', 'as', 'a', 'writing', 'exercise', 'which', 'i', 'intend', 'doing', ',', 'it', 'will', 'be', 'able', 'about', 'a', 'situation', 'of', 'character', \"'\", 's', 'with', 'er', '##m', 'something', 'that', \"'\", 's', 'hidden', ',', 'some', 'desire', ',', 'i', 'do', 'n', \"'\", 't', ',', 'the', ',', 'not', 'even', 'spoke', 'about', 'to', 'themselves', 'or', ',', 'or', 'generally', ',', 'er', '##m', 'i', 'like', 'to', 'sort', 'of', 'say', 'that', 'came', 'from', 'those', 'two', 'little', ',', 'just', 'this', 'amounts', 'of', 'colour', 'which', 'seemed', 'to', 'be', 'saying', 'such', 'a', 'lot', '[SEP]']\n",
      "processed 600 words\n",
      "calculating clusters for pencil\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'i', 'suppose', 'this', 'is', 'er', '##m', ',', 'a', 'car', '##ica', '##ture', ',', 'a', 'self', 'portrait', 'this', 'little', ',', 'i', ',', 'in', 'fact', 'i', 'was', ',', 'really', 'actually', 'hilarious', 'as', 'i', 'felt', 'that', ',', 'i', ',', 'i', ',', 'do', 'n', \"'\", 't', 'actually', 'know', 'an', 'awful', 'lot', 'about', 'ga', '##ug', '##in', ',', 'but', 'if', ',', 'if', 'i', 'knew', 'nothing', 'about', 'him', 'at', 'all', ',', 'i', 'would', 'of', 'thought', 'he', 'was', 'having', 'a', 'bit', 'of', 'joke', 'of', 'himself', 'with', 'this', ',', 'but', 'er', ',', 'being', 'the', 'person', 'that', 'he', 'was', 'i', 'ca', 'n', \"'\", 't', 'image', 'that', 'he', 'had', 'that', 'quality', ',', 'that', ',', 'i', 'do', 'n', \"'\", 't', 'believe', 'he', 'would', 'be', 'laughing', 'at', 'himself', ',', 'er', '##m', ',', 'er', '##m', ',', 'the', 'symbolism', 'er', '##m', 'and', 'conflict', 'of', 'this', 'painting', 'its', 'da', '##zzle', '##s', 'me', 'more', 'than', ',', 'than', 'the', 'colour', 'or', 'at', 'least', 'as', 'much', 'as', 'the', 'colours', 'in', 'it', ',', 'but', 'there', \"'\", 's', 'a', ',', 'there', \"'\", 's', 'a', 'half', 'eaten', ',', 'well', 'it', 'is', 'n', \"'\", 't', 'half', 'eaten', ',', 'but', 'there', \"'\", 's', 'half', 'an', 'apple', 'at', 'the', 'top', 'and', ',', 'and', 'that', 'was', 'the', ',', 'the', 'way', 'into', 'me', 'finally', ',', 'for', ',', 'for', 'writing', 'about', 'this', ',', 'this', 'again', 'is', 'a', 'shopping', 'list', ',', 'i', 'call', 'it', 'a', 'shopping', 'list', ',', 'this', 'is', 'just', 'visual', 'images', 'that', ',', 'that', 'will', 'be', 'opened', 'out', 'at', 'some', 'point', 'and', 'turn', 'it', 'into', 'something', ',', 'and', 'my', 'images', 'were', 'er', '##m', 'shoulders', 'of', 'the', 'mata', '##dor', 'smoking', 'snakes', ',', 'dare', 'to', 'bit', 'an', 'apple', ',', 'see', 'one', 'half', 'gone', 'and', 'still', 'i', 'wear', 'a', 'halo', 'intact', ',', 'that', 'i', \"'\", 'm', 'sure', 'i', \"'\", 've', 'completely', 'wrong', 'about', 'him', 'as', 'a', ',', 'a', 'person', ',', 'but', 'as', 'the', 'painting', 'that', \"'\", 's', 'obviously', 'something', 'else', ',', 'er', '##m', ',', 'i', 'found', 'that', 'one', 'of', 'the', 'things', 'that', 'were', 'he', \"'\", 's', ',', 'i', ',', 'i', 'think', 'probably', 'that', 'everybody', 'who', 'writes', 'is', 'that', 'you', \"'\", 'll', 'come', 'to', 'a', 'point', 'when', 'you', 'ca', 'n', \"'\", 't', 'write', ',', 'you', 'stop', 'writing', ',', 'you', 'have', 'n', \"'\", 't', 'got', 'anything', 'you', 'want', 'to', 'write', 'about', ',', 'or', 'your', 'frightened', 'of', 'writing', ',', 'and', 'i', 'devi', '##se', 'exercises', 'so', 'that', ',', 'that', 'does', 'n', \"'\", 't', 'happened', 'to', 'me', ',', 'i', 'think', 'writing', 'is', 'like', 'any', 'skill', 'you', 'have', 'to', 'keep', 'doing', 'it', 'to', 'be', 'able', 'to', 'do', 'it', ',', 'its', ',', 'you', ',', 'some', 'of', 'it', 'is', 'a', 'game', 'and', 'the', 'rest', 'of', 'it', 'is', 'hard', 'work', ',', 'and', 'one', 'of', 'the', 'exercises', 'i', ',', 'i', 'delighted', 'using', 'er', '##m', 'a', 'portrait', 'of', 'a', 'woman', 'er', '##m', ',', 'its', 'about', 'er', '##m', 'the', 'er', 'still', 'life', ',', 'its', 'the', 'back', 'one', ',', 'yes', 'this', 'one', 'here', ',', 'i', 'have', ',', 'i', ',', 'i', 'hope', 'to', 'use', 'this', 'as', 'a', 'writing', 'exercise', 'i', 'found', 'the', ',', 'the', 'math', '##s', 'in', 'this', 'and', 'the', 'colour', 'of', 'the', 'piece', 'of', 'fruit', 'in', 'the', 'background', ',', 'very', 'interesting', 'because', 'most', 'of', 'the', 'colours', 'to', 'me', 'seem', 'a', ',', 'a', 'lot', 'less', 'vibrant', 'then', 'many', 'of', 'his', 'other', 'paintings', ',', 'and', 'so', 'they', ',', 'they', 'attracted', 'me', 'and', 'have', 'a', ',', 'a', 'strong', 'sense', 'of', 'er', '##m', ',', 'er', 'a', 'hidden', 'desire', 'in', 'that', 'and', 'so', 'it', ',', 'to', 'use', 'it', 'as', 'a', 'writing', 'exercise', 'which', 'i', 'intend', 'doing', ',', 'it', 'will', 'be', 'able', 'about', 'a', 'situation', 'of', 'character', \"'\", 's', 'with', 'er', '##m', 'something', 'that', \"'\", 's', 'hidden', ',', 'some', 'desire', ',', 'i', 'do', 'n', \"'\", 't', ',', 'the', ',', 'not', 'even', 'spoke', 'about', 'to', 'themselves', 'or', ',', 'or', 'generally', ',', 'er', '##m', 'i', 'like', 'to', 'sort', 'of', 'say', 'that', 'came', 'from', 'those', 'two', 'little', ',', 'just', 'this', 'amounts', 'of', 'colour', 'which', 'seemed', 'to', 'be', 'saying', 'such', 'a', 'lot', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (781 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'bel', '##grave', '##s', 'league', 'division', 'one', ';', 'aston', 'villa', 'three', ',', 'tottenham', 'hotspur', 'two', ';', 'crystal', 'palace', 'two', ',', 'derby', 'county', 'one', ';', 'liverpool', 'two', ',', 'sunderland', 'one', ';', 'luton', 'town', 'ni', '##l', ',', 'norwich', 'city', 'one', ';', 'manchester', 'city', 'one', ',', 'wimbledon', 'one', ';', 'nottingham', 'forest', 'one', ',', 'manchester', 'united', 'one', ';', 'queens', 'park', 'rangers', 'one', ',', 'coventry', 'city', 'ni', '##l', ';', 'sheffield', 'united', 'one', ',', 'chelsea', 'ni', '##l', ';', 'southampton', 'three', ',', 'everton', 'four', ';', 'division', 'two', ';', 'barnsley', 'one', ',', 'charlton', 'athletic', 'one', ';', 'brighton', 'and', 'hove', 'albion', 'one', ',', 'blackburn', 'rovers', 'ni', '##l', ';', 'bristol', 'rovers', 'one', ',', 'not', '##ts', 'county', 'one', ';', 'leicester', 'city', 'four', ',', 'middlesbrough', 'three', ';', 'mil', '##wall', 'one', ',', 'swindon', 'town', 'ni', '##l', ';', 'newcastle', 'united', 'ni', '##l', ',', 'bristol', 'city', 'ni', '##l', ';', 'oldham', 'athletic', 'two', ',', 'west', 'bromwich', 'albion', 'one', ';', 'plymouth', 'argyll', 'one', ',', 'portsmouth', 'one', ';', 'port', 'vale', 'ni', '##l', ',', 'hull', 'city', 'ni', '##l', ';', 'watford', 'one', ',', 'ipswich', 'town', 'one', ';', 'west', 'ham', 'united', 'one', ',', 'sheffield', 'wednesday', 'three', ';', 'wolverhampton', 'wanderers', 'three', ',', 'oxford', 'united', 'three', ';', 'division', 'three', ';', 'bolton', 'wanderers', 'two', ',', 'wigan', 'athletic', 'one', ';', 'bradford', 'city', 'four', ',', 'ley', '##ton', 'orient', 'ni', '##l', ';', 'cambridge', 'united', 'one', ',', 'exeter', 'city', 'ni', '##l', ';', 'crewe', 'alexandra', 'two', ',', 'bury', 'two', ';', 'fulham', 'one', ',', 'bournemouth', 'one', ';', 'grimsby', 'town', 'two', ',', 'brentford', 'ni', '##l', ';', 'huddersfield', 'town', 'one', ',', 'chester', 'city', 'one', ';', 'preston', 'north', '##end', 'two', ',', 'birmingham', 'ni', '##l', ';', 'reading', 'two', ',', 'rotherham', 'united', 'ni', '##l', ';', 'shrewsbury', 'town', 'two', ',', 'stoke', 'city', 'ni', '##l', ';', 'division', 'four', ';', 'burnley', 'two', ',', 'carlisle', 'united', 'one', ';', 'cardiff', 'city', 'one', ',', 'sc', '##unt', '##horpe', 'united', 'ni', '##l', ';', 'chesterfield', 'two', ',', 'wrexham', 'one', ';', 'darlington', 'one', ',', 'doncaster', 'rovers', 'one', ';', 'gill', '##ingham', 'ni', '##l', ',', 'york', 'city', 'ni', '##l', ';', 'hart', '##le', '##pool', 'united', 'one', ',', 'blackpool', 'two', ';', 'lincoln', 'city', 'two', ',', 'maids', '##tone', 'united', 'one', ';', 'scarborough', 'two', ',', 'al', '##ders', '##hot', 'ni', '##l', ';', 'stockport', 'county', 'four', ',', 'hereford', 'united', 'two', ';', 'walsall', 'ni', '##l', ',', 'rochdale', 'one', ';', 'in', 'the', 'f', 'a', 'trophy', 'fourth', 'round', ';', 'alt', '##ring', '##ham', 'five', ',', 'ho', '##r', '##wich', 'ni', '##l', ';', 'colchester', 'united', 'two', ',', 'wit', '##ton', 'albion', 'ni', '##l', ';', 'kidd', '##er', '##minster', 'three', ',', 'em', '##ley', 'ni', '##l', ';', 'north', '##wich', 'victoria', 'two', ',', 'w', '##y', '##combe', 'wanderers', 'three', ';', 'in', 'the', 'h', 'f', 's', 'loans', 'league', ',', 'premier', 'division', ';', 'cho', '##rley', 'five', ',', 'gains', '##borough', 'three', ';', 'fleetwood', 'four', ',', 'hyde', 'one', ';', 'moss', '##ley', 'two', ',', 'goo', '##le', 'ni', '##l', ';', 'she', '##ps', '##hed', 'ni', '##l', ',', 'marine', 'two', ';', 'south', 'liverpool', 'ni', '##l', ',', 'lee', '##k', 'one', ';', 'stale', '##y', '##bridge', 'three', ',', 'bangor', 'one', ';', 'the', 'bee', '##zer', 'homes', 'league', ',', 'premier', 'division', ';', 'at', '##hers', '##ton', 'one', ',', 'cambridge', 'united', 'one', ',', 'i', \"'\", 'm', 'sorry', ',', 'cambridge', 'city', 'one', ';', 'moor', 'green', 'one', ',', 'we', '##ym', '##outh', 'two', ';', 'poole', 'one', ',', 'worcester', 'two', ';', 'rush', '##ton', 'four', ',', 'hale', '##s', 'owen', 'three', ';', 'the', 'waterloo', '##ville', 'versus', 'che', '##lm', '##sford', 'result', 'is', 'a', 'late', 'kick', 'off', ';', 'the', 'va', '##ux', '##hall', 'league', ',', 'premier', 'division', ';', 'basin', '##gs', '##tok', '##e', 'ni', '##l', ',', 'st', 'albans', 'four', ';', 'bishops', 'st', '##ort', '##ford', 'two', ',', 'wo', '##king', 'ni', '##l', ';', 'bog', '##nor', 'regis', 'two', ',', 'da', '##gen', '##ham', 'ni', '##l', ';', 'kingston', '##ian', 'two', ',', 'harrow', 'ni', '##l', ';', 'ley', '##ton', 'wing', '##ate', 'one', ',', 'wo', '##king', '##ham', 'four', ';', 'mar', '##low', 'one', ',', 'grey', '##s', 'athletic', 'ni', '##l', ';', 'in', 'the', 'ten', '##nant', '##s', 'scottish', 'cup', 'quarter', 'finals', ';', 'mother', '##well', 'ni', '##l', ',', 'morton', 'ni', '##l', ';', 'st', 'johnston', 'five', ',', 'a', '##yr', 'united', 'two', ';', 'in', 'the', 'scottish', 'league', ',', 'premier', 'division', ';', 'dun', '##fer', '##ml', '##ine', 'one', ',', 'hi', '##ber', '##nian', 'one', ';', 'division', 'one', ';', 'aid', '##rie', 'two', ',', 'ki', '##lma', '##rno', '##ck', 'ni', '##l', ';', 'clyde', '##bank', 'one', ',', 'br', '##eck', '##in', 'city', 'ni', '##l', ';', 'dundee', 'one', ',', 'for', '##far', 'ni', '##l', ';', 'fa', '##lkirk', 'three', ',', 'hamilton', 'ni', '##l', ';', 'part', '##ick', 'two', ',', 'meadow', '##bank', 'four', ';', 'wr', '##aith', 'rovers', 'one', ',', 'clyde', 'ni', '##l', ';', 'division', 'two', ';', 'albion', 'ni', '##l', ',', 'ste', '##nh', '##ouse', 'muir', 'ni', '##l', ';', 'all', '##oa', 'two', ',', 'dunbar', '##ton', 'one', ';', 'ar', '##bro', '##ath', 'ni', '##l', ',', 'east', 'stirling', 'one', ';', 'bury', 'two', ',', 'queens', 'park', 'one', ';', 'cow', '##den', 'beat', '##h', 'two', ',', 'st', '##ran', '##rae', '##r', 'ni', '##l', ';', 'east', 'fife', 'two', ',', 'montrose', 'two', ';', 'and', 'finally', ',', 'queen', 'of', 'the', 'south', 'ni', '##l', ',', 'stirling', 'albion', 'ni', '##l', '.', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (623 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'right', 'so', ',', 'you', 'know', ',', 'there', 'are', 'those', 'who', 'would', 'teach', 'that', 'jesus', 'he', 'would', 'die', 'for', 'our', 'sins', 'and', 'he', \"'\", 's', 'forgiven', 'us', 'sins', ',', 'but', 'only', 'those', 'who', 'come', 'to', 'him', ',', 'jesus', 'died', 'for', 'the', 'sin', 'of', 'the', 'whole', 'world', ',', 'for', 'every', 'man', ',', 'woman', ',', 'boy', 'and', 'girl', 'that', 'has', 'ever', 'lived', 'or', 'ever', 'will', 'live', ',', 'he', 'died', 'for', 'the', 'sin', 'of', 'the', 'whole', 'world', ',', 'not', 'just', 'for', 'those', 'even', 'who', 'lived', 'after', 'his', 'death', ',', 'that', \"'\", 's', 'why', 'it', 'talks', 'about', 'in', 'the', 'old', 'testament', 'people', 'like', 'abraham', 'looking', 'for', 'that', 'day', ',', 'and', 'so', 'jesus', 'who', 'in', ',', 'when', 'he', 'died', ',', 'because', 'he', \"'\", 's', 'eternal', ',', 'so', 'we', \"'\", 've', 'got', 'the', 'problems', 'with', 'time', ',', 'god', 'has', 'n', \"'\", 't', 'got', 'problems', 'with', 'time', ',', 'he', \"'\", 's', 'eternal', 'and', 'so', 'his', 'sacrifice', ',', 'the', 'sacrifice', 'of', 'him', 'on', 'the', 'cross', 'was', 'effective', 'for', 'abraham', 'as', 'it', 'is', 'for', 'you', ',', 'it', 'was', 'as', 'effective', 'for', 'david', 'as', 'it', 'was', 'for', 'paul', 'otherwise', 'abraham', 'would', 'never', 'of', 'had', 'his', 'sins', 'forgiven', 'because', 'what', 'happened', 'with', 'all', 'the', 'sacrifice', 'with', 'all', 'the', 'little', 'lamb', '##s', 'that', 'were', 'killed', 'and', 'all', 'the', 'goats', 'and', 'all', 'the', 'rest', 'they', 'only', 'acted', 'as', 'a', 'covering', 'for', 'sin', ',', 'did', 'n', \"'\", 't', 'take', 'them', 'away', ',', 'it', 'covered', 'them', ',', 'what', 'for', ',', 'until', 'the', 'moment', 'when', 'jesus', 'would', 'come', 'and', 'would', 'take', 'those', 'sins', 'away', 'and', 'so', 'when', 'you', 'think', 'of', 'david', \"'\", 's', 'sin', ',', 'his', 'adultery', 'and', 'his', 'murder', ',', 'how', 'does', 'he', 'get', 'forgiven', 'for', 'that', 'because', 'jesus', 'died', 'from', 'the', 'cross', 'and', 'he', 'takes', 'upon', 'himself', 'david', \"'\", 's', 'sin', 'and', 'he', 'takes', 'upon', 'him', 'abraham', \"'\", 's', 'sin', 'and', 'noah', \"'\", 's', 'sin', 'and', 'adam', \"'\", 's', 'sin', ',', 'just', 'as', 'much', 'as', 'your', 'sin', 'and', 'the', 'person', 'who', 'will', 'be', 'born', 'in', 'ten', 'years', 'time', 'their', 'sin', 'also', ',', 'all', 'our', 'sins', 'er', 'as', 'gloria', 'just', 'read', 'there', 'from', ',', 'from', 'one', 'john', 'to', 'two', 'they', 'were', 'all', 'of', 'him', 'he', 'has', 'died', 'for', 'every', 'one', ',', 'well', 'that', \"'\", 's', 'his', 'humiliation', ',', 'hurry', 'along', 'quickly', 'now', 'his', 'ex', '##hort', '##ation', ',', 'the', 'period', 'from', 'jesus', \"'\", 's', 'resurrection', 'onward', 'is', 'referred', 'to', 'as', 'to', 'the', ',', 'as', 'the', 'state', 'of', 'ex', '##hort', '##ation', ',', 'now', 'what', 'does', 'that', 'term', 'mean', ',', 'well', 'as', 'jesus', 'according', 'to', 'his', 'divine', 'nature', 'has', 'always', 'been', ',', 'he', 'was', 'always', 'every', 'where', ',', 'now', 'in', 'his', 'human', 'nature', ',', 'before', ',', 'be', ',', 'sorry', 'it', \"'\", 's', 'not', ',', 'it', \"'\", 's', 'not', 'on', 'that', 'one', ',', 'but', 'before', 'he', ',', 'he', 'came', 'to', 'earth', ',', 'he', 'was', 'every', 'where', ',', 'he', 'was', 'god', ',', 'he', 'was', ',', 'he', 'was', 'om', '##nia', 'present', 'that', 'means', 'he', 'was', 'every', 'where', 'at', 'the', 'same', 'time', ',', 'but', 'he', 'takes', 'upon', 'himself', 'he', \"'\", 's', 'su', ',', 'he', \"'\", 's', ',', 'he', \"'\", 's', 'human', 'nature', 'and', 'he', 'takes', 'upon', 'himself', 'the', 'limitations', 'and', 'when', 'jesus', 'is', 'walking', 'down', 'second', 'avenue', 'in', ',', 'in', 'jerusalem', 'he', \"'\", 's', 'not', 'in', 'nazareth', 'that', \"'\", 's', 'why', 'there', 'were', 'times', 'when', 'people', 'came', 'to', 'er', ',', 'to', ',', 'to', ',', 'came', 'rushing', 'out', 'because', 'they', 'heard', 'that', 'jesus', 'was', 'passing', 'by', ',', 'see', 'he', 'was', 'n', \"'\", 't', 'there', 'resident', 'with', 'them', ',', 'he', 'passed', 'by', ',', 'now', 'he', \"'\", 's', 'gone', 'back', 'to', 'heaven', 'and', 'where', 'is', 'he', ',', 'he', \"'\", 's', 'in', 'heaven', ',', 'he', ',', 'er', 'whereabouts', ',', 'where', 'do', 'you', 'think', 'jesus', 'is', 'now', ',', 'that', 'resurrected', 'body', 'that', 'was', 'g', '##lor', '##ified', 'that', 'has', 'gone', 'back', 'to', 'heaven', ',', 'where', 'do', 'you', 'think', 'it', 'is', '[SEP]']\n",
      "processed 700 words\n",
      "calculating clusters for coin\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "import numpy as np\n",
    "import bert_helper\n",
    "import csv\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for each word file we have, do the following:\n",
    "    for each layer we care about, calculate the token embedding at that layer for each token\n",
    "        for each number of clusters we care about, calculate the centroids of those clusters\n",
    "        \n",
    "store results in a file, one for each word+layer+cluster_number combo, resulting in a file structure like the following:\n",
    "\n",
    "word_data/\n",
    "  |-airplane/\n",
    "  | |- bnc_tokens.csv\n",
    "  | |- layer_0_k_1_clusters.csv\n",
    "  | |   ...\n",
    "  | |- layer_0_k_7_clusters.csv\n",
    "  | |   ...\n",
    "  | |- layer_11_k_7_clusters.csv\n",
    "  \n",
    "each cluster file is a csv with the following fields:\n",
    "    word\n",
    "    layer\n",
    "    cluster_size_k\n",
    "    cluster_number\n",
    "    centroid\n",
    "    token_ids\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "(model, tokenizer) = bert_helper.initialize()\n",
    "\n",
    "i = 0\n",
    "for word in collected_words:\n",
    "    i+=1\n",
    "    if i % 100 == 0:\n",
    "        print(\"processed %s words\" % i)\n",
    "        print(\"calculating clusters for %s\" % word)\n",
    "\n",
    "    # it's more efficient to collect all the vectors for all the layers at once,\n",
    "    # since we calculate the whole activation network at once for each token\n",
    "    vectors = []\n",
    "\n",
    "    \n",
    "    # create a directory to store all our clustering results in\n",
    "    data_dir = './data/word_data'\n",
    "    results_dir = os.path.join(data_dir, word, 'analysis_results')    \n",
    "    if os.path.exists(results_dir):\n",
    "        shutil.rmtree(results_dir)\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "    # read in the tokens for this word\n",
    "    pathname = os.path.join(data_dir, word, 'BNC_tokens.csv')\n",
    "    with open(pathname, mode='r') as csv_file:\n",
    "        reader = csv.DictReader(csv_file, delimiter='\\t', fieldnames=[\"word\", \"sentence\", \"tag\", \"uid\"])\n",
    "        \n",
    "        data = [row for row in reader]\n",
    "\n",
    "        # generate embeddings for each token\n",
    "        for row in data:\n",
    "            sentence = row[\"sentence\"]\n",
    "            vector = bert_helper.get_bert_vectors_for(word, sentence, model, tokenizer)\n",
    "            # if the token was too long we may not have succeeded in generating embeddings for it, in which case we will throw it out\n",
    "            if vector != None:\n",
    "                row[\"embedding\"] = vector\n",
    "            else:\n",
    "                row[\"embedding\"] = None\n",
    "        data = list(filter(lambda row: row[\"embedding\"] != None, data))\n",
    "\n",
    "        for layer in layers:\n",
    "            layer_vectors = [row[\"embedding\"][layer] for row in data]\n",
    "        \n",
    "            for k in cluster_sizes:\n",
    "                if len(data) >= k:\n",
    "                    # calculate clusters\n",
    "                    kmeans_obj = KMeans(n_clusters=k)\n",
    "                    kmeans_obj.fit(layer_vectors)\n",
    "                    label_list = kmeans_obj.labels_\n",
    "                    cluster_centroids = kmeans_obj.cluster_centers_\n",
    "\n",
    "\n",
    "                    # store clusternumber with data\n",
    "                    for index,datapoint in enumerate(data):\n",
    "                        datapoint['cluster_number'] = label_list[index]\n",
    "\n",
    "                    # generate outfile name\n",
    "                    filename = \"layer_\" + str(layer) + \"_clusters_k_equals_\" + str(k) + \".csv\"\n",
    "                    outpath = os.path.join(results_dir, filename)\n",
    "\n",
    "\n",
    "                    with open(outpath, mode='w') as disk:\n",
    "                        writer = csv.DictWriter(disk, delimiter='\\t', fieldnames=['word', 'clusternumber', 'centroid', 'sentence_uids'])\n",
    "\n",
    "\n",
    "                        # retrieve centroid for each cluster and uids of sentences in cluster:\n",
    "                        for clusternumber in range(k):\n",
    "                            sentence_uids = []\n",
    "                            for index, datapoint in enumerate(data):\n",
    "                                if datapoint['cluster_number'] == clusternumber:\n",
    "                                    sentence_uids.append(datapoint['uid'])\n",
    "                            out_data = {'word': word,\n",
    "                                        'clusternumber': clusternumber,\n",
    "                                        'centroid': cluster_centroids[clusternumber],\n",
    "                                        'sentence_uids': sentence_uids}\n",
    "\n",
    "                            # store in file\n",
    "                            # write dta for this cluster\n",
    "                            writer.writerow(out_data)\n",
    "\n",
    "                else:\n",
    "                    print(\"not enough tokens to make %s clusters for word: %s\" % (k, word))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
