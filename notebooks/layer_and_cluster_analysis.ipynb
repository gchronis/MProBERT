{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the things we care about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 203 word pairs from WordSim similarity dataset\n",
      "processed 999 word pairs from simlex999 dataset\n",
      "Total words between 353 and simlex: 2404\n",
      "Unique words between 353 and simlex: 1224\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\"\"\"\n",
    "1) the words we want to collect data for\n",
    "\"\"\"\n",
    "ws353 = datasets.get_ws353()\n",
    "simlex999 = datasets.get_simlex999()\n",
    "\n",
    "# get a list of all the words in ws353\n",
    "first_word = [row['word1'] for row in ws353]\n",
    "second_word = [row['word2'] for row in ws353]\n",
    "ws353_wordlist = first_word + second_word\n",
    "\n",
    "# get a list of all the words in simlex999\n",
    "first_word = [row['word1'] for row in simlex999]\n",
    "second_word = [row['word2'] for row in simlex999]\n",
    "simlex999_wordlist = first_word + second_word\n",
    "\n",
    "\n",
    "all_words = ws353_wordlist + simlex999_wordlist\n",
    "print(\"Total words between 353 and simlex: %s\" % len(all_words))\n",
    "unique_words = set(all_words) \n",
    "print(\"Unique words between 353 and simlex: %s\" % len(unique_words))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "2) the layers we want to analzye\n",
    "\"\"\"\n",
    "layers = [0,1,5,11]\n",
    "\n",
    "\"\"\"\n",
    "3) The cluster sizes we want to analyze\n",
    "\"\"\"\n",
    "cluster_sizes = [1,3,5,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort dataset of BNC tokens into files for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "ALLWORDS_DIR = './data/word_data'\n",
    "\n",
    "# you already have tokens collected for each word in simlex and wordsim\n",
    "# now these tokens ought to be sorted into their own files\n",
    "\n",
    "# ensure that there is a word_data directory to store in our words\n",
    "# you have to delete it first with rm -rf if we are reloading\n",
    "os.mkdir(ALLWORDS_DIR)\n",
    "\n",
    "\n",
    "# create files for each word we care about\n",
    "for word in unique_words:\n",
    "    word_dir = os.path.join(ALLWORDS_DIR, word)\n",
    "    os.mkdir(word_dir)\n",
    "\n",
    "\n",
    "# read in the big long file\n",
    "with open('./data/bnc_tokens_353_and_simlex.csv', mode=\"r\") as infile:\n",
    "    fieldnames = [\"word\", \"sentence\", \"POS\", \"id\"]\n",
    "    reader = csv.DictReader(infile, delimiter=\"\\t\", quoting=csv.QUOTE_NONNUMERIC, fieldnames=fieldnames)\n",
    "    \n",
    "    # split the big long file into smaller, sorted files that are easier to process one at a time\n",
    "    for row in reader:\n",
    "        \n",
    "        word = row[\"word\"]\n",
    "        text = row[\"sentence\"]\n",
    "        pos = row[\"POS\"]\n",
    "        uid = \"BNC_\" + str(int(row[\"id\"]))\n",
    "\n",
    "        # open file for this word to spit tokens into\n",
    "        token_file = os.path.join(ALLWORDS_DIR, word, \"BNC_tokens.csv\")\n",
    "        with open(token_file, mode=\"a\") as outfile:\n",
    "            # finally, write all of the info with the vector to disk\n",
    "            writer = writer = csv.writer(outfile, delimiter='\\t', quoting=csv.QUOTE_NONNUMERIC)\n",
    "            writer.writerow([word, text, pos, uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How many instances per word do you have?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words between sl999 and ws353 we've collected tokens for: 1224\n",
      "average number of tokens per word: 100.25490196078431\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ALLWORDS_DIR = './data/word_data'\n",
    "\n",
    "\n",
    "token_counts = []\n",
    "n = 0\n",
    "\n",
    "for word in unique_words:\n",
    "    wordfile = os.path.join(ALLWORDS_DIR, word, 'BNC_tokens.csv')   \n",
    "\n",
    "    with open(wordfile, mode=\"r\") as infile:\n",
    "        fieldnames = [\"word\", \"sentence\", \"POS\", \"id\"]\n",
    "        reader = csv.DictReader(infile, delimiter=\"\\t\", quoting=csv.QUOTE_NONNUMERIC, fieldnames=fieldnames)\n",
    "        \n",
    "        count = 0\n",
    "        for row in reader:\n",
    "            count +=1\n",
    "        token_counts.append(count)\n",
    "        n = n+1\n",
    "\n",
    "average = np.sum(token_counts) / n\n",
    "print(\"number of unique words between sl999 and ws353 we've collected tokens for: %s\" % n)\n",
    "print(\"average number of tokens per word: %s\" % average)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the cluster centroids for each word, layer, and k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/gabriellachronis/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /Users/gabriellachronis/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/9m/vzvx58rs51v_x5nm620fz4xr0000gn/T/tmpv164mjyy\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/gabriellachronis/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'example', '3', ':', '1', 'limitation', 'on', 'liability', 'of', 'original', 'tenant', '(', '1', ')', 'in', 'this', 'clause', '\"', 'the', 'original', 'tenant', '\"', 'means', 'the', 'said', '…', 'only', 'and', 'this', 'clause', 'applies', 'to', 'any', 'period', 'after', 'the', 'term', 'here', '##by', 'granted', 'cease', '##s', 'to', 'be', 'vested', 'in', 'the', 'original', 'tenant', '(', '2', ')', 'if', 'and', 'so', 'often', 'as', 'the', 'tenant', 'fails', 'to', 'pay', 'the', 'rent', 'or', 'any', 'other', 'sum', 'properly', 'due', 'under', 'this', 'lease', 'or', 'commits', 'any', 'breach', 'of', 'covenant', 'known', 'to', 'the', 'landlord', 'then', 'the', 'landlord', 'shall', 'forth', '##with', 'not', '##ify', 'the', 'original', 'tenant', 'of', 'that', 'fact', '(', '3', ')', 'the', 'landlord', 'shall', 'not', 'be', 'entitled', 'to', 'recover', 'from', 'the', 'original', 'tenant', 'any', 'ar', '##rea', '##rs', 'of', 'rent', 'or', 'other', 'sums', 'pay', '##able', 'under', 'this', 'lease', 'where', 'the', 'rent', 'or', 'other', 'sums', 'claimed', 'became', 'due', 'earlier', 'than', 'three', 'months', 'before', 'the', 'original', 'tenant', 'was', 'notified', 'under', 'sub', '-', 'clause', '(', '2', ')', 'above', '(', '4', ')', 'the', 'original', 'tenant', 'shall', 'not', 'be', 'liable', 'for', 'any', 'ar', '##rea', '##rs', 'of', 'rent', 'or', 'other', 'sum', 'falling', 'due', 'after', 'the', 'date', 'upon', 'which', 'this', 'lease', 'is', 'expressed', 'to', 'ex', '##pire', 'or', 'any', 'breach', 'of', 'covenant', 'committed', 'after', 'that', 'date', 'example', '3', ':', '2', 'limitation', 'on', 'liability', 'of', 'tenant', '(', '1', ')', 'in', 'this', 'clause', '(', 'a', ')', '\"', 'the', 'original', 'tenant', '\"', 'means', 'only', '(', 'b', ')', '\"', 'the', 'original', 'assign', '##ee', '\"', 'means', 'a', 'person', 'to', 'whom', 'the', 'original', 'tenant', 'lawful', '##ly', 'assigns', 'this', 'lease', '(', '2', ')', 'upon', 'a', 'lawful', 'assignment', 'of', 'this', 'lease', 'by', 'the', 'original', 'tenant', 'the', 'original', 'tenant', '(', 'a', ')', 'shall', 'be', 'released', 'from', 'further', 'personal', 'liability', 'for', 'any', 'breach', 'of', 'any', 'of', 'the', 'tenant', \"'\", 's', 'obligations', 'under', 'this', 'lease', 'occurring', 'after', 'the', 'date', 'of', 'the', 'assignment', 'but', '(', 'b', ')', 'shall', 'guarantee', 'performance', 'by', 'the', 'original', 'assign', '##ee', 'of', 'those', 'obligations', 'until', 'the', 'ex', '##pi', '##ry', 'or', 'other', 'determination', 'of', 'the', 'term', 'or', '(', 'if', 'sooner', ')', 'a', 'lawful', 'assignment', 'of', 'this', 'lease', 'by', 'the', 'original', 'assign', '##ee', 'example', '3', ':', '3', 'restriction', 'on', 'landlord', \"'\", 's', 'ability', 'to', 'sue', 'original', 'tenant', 'at', 'any', 'time', 'after', 'the', 'lawful', 'assignment', 'of', 'this', 'lease', 'by', '[', 'name', 'of', 'original', 'tenant', ']', 'the', 'landlord', 'shall', 'not', 'be', 'entitled', 'to', 'enforce', 'against', 'him', 'the', 'tenant', \"'\", 's', 'obligations', 'under', 'this', 'lease', 'unless', 'the', 'landlord', 'shall', 'have', 'first', '(', '1', ')', 'recovered', 'judgment', 'against', 'all', 'other', 'persons', 'against', 'whom', 'the', 'landlord', 'is', 'or', 'has', 'become', 'entitled', 'to', 'enforce', 'those', 'obligations', 'either', 'as', 'principal', 'or', 'sure', '##ty', 'and', '(', '2', ')', 'attempted', 'to', 'levy', 'ex', '##cut', '##ion', 'upon', 'such', 'judgment', 'and', 'upon', 'payment', 'by', '[', 'name', 'of', 'original', 'tenant', ']', 'of', 'any', 'sum', 'due', 'under', 'such', 'judgment', 'the', 'landlord', 'shall', 'assign', 'to', 'him', 'the', 'benefit', 'of', 'it', 'example', '3', ':', '4', 'definition', 'clause', 'making', 'tenant', 'liable', 'for', 'rent', 'during', 'holding', 'over', 'period', '\"', 'the', 'term', '\"', 'includes', 'not', 'only', 'the', 'term', 'expressed', 'to', 'be', 'granted', 'by', 'this', 'lease', 'but', 'also', 'any', 'period', 'after', 'the', 'date', 'on', 'which', 'the', 'term', 'is', 'expressed', 'to', 'ex', '##pire', 'during', 'which', 'the', 'ten', '##ancy', 'continues', 'under', 'the', 'landlord', 'and', 'tenant', 'act', '1954', 'example', '3', ':', '5', 'clause', 'making', 'the', 'tenant', 'liable', 'to', 'pay', 'rent', 'and', 'interim', 'rent', 'promptly', 'to', 'pay', 'the', 'rent', 'reserved', 'by', 'this', 'lease', 'without', 'any', 'de', '##duction', 'or', 'set', '-', 'off', 'and', 'any', 'rent', 'substituted', 'for', 'it', 'either', 'as', 'a', 'result', 'of', 'a', 'rent', 'review', 'under', 'this', 'lease', 'or', 'the', 'agreement', 'or', 'determination', 'of', 'a', 'rent', 'pay', '##able', 'by', 'virtue', 'of', 'the', 'landlord', 'and', 'tenant', 'act', '1954', ',', 's', '##24', '##a', '[SEP]']\n",
      "processed 100 words\n",
      "calculating clusters for self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'i', 'suppose', 'this', 'is', 'er', '##m', ',', 'a', 'car', '##ica', '##ture', ',', 'a', 'self', 'portrait', 'this', 'little', ',', 'i', ',', 'in', 'fact', 'i', 'was', ',', 'really', 'actually', 'hilarious', 'as', 'i', 'felt', 'that', ',', 'i', ',', 'i', ',', 'do', 'n', \"'\", 't', 'actually', 'know', 'an', 'awful', 'lot', 'about', 'ga', '##ug', '##in', ',', 'but', 'if', ',', 'if', 'i', 'knew', 'nothing', 'about', 'him', 'at', 'all', ',', 'i', 'would', 'of', 'thought', 'he', 'was', 'having', 'a', 'bit', 'of', 'joke', 'of', 'himself', 'with', 'this', ',', 'but', 'er', ',', 'being', 'the', 'person', 'that', 'he', 'was', 'i', 'ca', 'n', \"'\", 't', 'image', 'that', 'he', 'had', 'that', 'quality', ',', 'that', ',', 'i', 'do', 'n', \"'\", 't', 'believe', 'he', 'would', 'be', 'laughing', 'at', 'himself', ',', 'er', '##m', ',', 'er', '##m', ',', 'the', 'symbolism', 'er', '##m', 'and', 'conflict', 'of', 'this', 'painting', 'its', 'da', '##zzle', '##s', 'me', 'more', 'than', ',', 'than', 'the', 'colour', 'or', 'at', 'least', 'as', 'much', 'as', 'the', 'colours', 'in', 'it', ',', 'but', 'there', \"'\", 's', 'a', ',', 'there', \"'\", 's', 'a', 'half', 'eaten', ',', 'well', 'it', 'is', 'n', \"'\", 't', 'half', 'eaten', ',', 'but', 'there', \"'\", 's', 'half', 'an', 'apple', 'at', 'the', 'top', 'and', ',', 'and', 'that', 'was', 'the', ',', 'the', 'way', 'into', 'me', 'finally', ',', 'for', ',', 'for', 'writing', 'about', 'this', ',', 'this', 'again', 'is', 'a', 'shopping', 'list', ',', 'i', 'call', 'it', 'a', 'shopping', 'list', ',', 'this', 'is', 'just', 'visual', 'images', 'that', ',', 'that', 'will', 'be', 'opened', 'out', 'at', 'some', 'point', 'and', 'turn', 'it', 'into', 'something', ',', 'and', 'my', 'images', 'were', 'er', '##m', 'shoulders', 'of', 'the', 'mata', '##dor', 'smoking', 'snakes', ',', 'dare', 'to', 'bit', 'an', 'apple', ',', 'see', 'one', 'half', 'gone', 'and', 'still', 'i', 'wear', 'a', 'halo', 'intact', ',', 'that', 'i', \"'\", 'm', 'sure', 'i', \"'\", 've', 'completely', 'wrong', 'about', 'him', 'as', 'a', ',', 'a', 'person', ',', 'but', 'as', 'the', 'painting', 'that', \"'\", 's', 'obviously', 'something', 'else', ',', 'er', '##m', ',', 'i', 'found', 'that', 'one', 'of', 'the', 'things', 'that', 'were', 'he', \"'\", 's', ',', 'i', ',', 'i', 'think', 'probably', 'that', 'everybody', 'who', 'writes', 'is', 'that', 'you', \"'\", 'll', 'come', 'to', 'a', 'point', 'when', 'you', 'ca', 'n', \"'\", 't', 'write', ',', 'you', 'stop', 'writing', ',', 'you', 'have', 'n', \"'\", 't', 'got', 'anything', 'you', 'want', 'to', 'write', 'about', ',', 'or', 'your', 'frightened', 'of', 'writing', ',', 'and', 'i', 'devi', '##se', 'exercises', 'so', 'that', ',', 'that', 'does', 'n', \"'\", 't', 'happened', 'to', 'me', ',', 'i', 'think', 'writing', 'is', 'like', 'any', 'skill', 'you', 'have', 'to', 'keep', 'doing', 'it', 'to', 'be', 'able', 'to', 'do', 'it', ',', 'its', ',', 'you', ',', 'some', 'of', 'it', 'is', 'a', 'game', 'and', 'the', 'rest', 'of', 'it', 'is', 'hard', 'work', ',', 'and', 'one', 'of', 'the', 'exercises', 'i', ',', 'i', 'delighted', 'using', 'er', '##m', 'a', 'portrait', 'of', 'a', 'woman', 'er', '##m', ',', 'its', 'about', 'er', '##m', 'the', 'er', 'still', 'life', ',', 'its', 'the', 'back', 'one', ',', 'yes', 'this', 'one', 'here', ',', 'i', 'have', ',', 'i', ',', 'i', 'hope', 'to', 'use', 'this', 'as', 'a', 'writing', 'exercise', 'i', 'found', 'the', ',', 'the', 'math', '##s', 'in', 'this', 'and', 'the', 'colour', 'of', 'the', 'piece', 'of', 'fruit', 'in', 'the', 'background', ',', 'very', 'interesting', 'because', 'most', 'of', 'the', 'colours', 'to', 'me', 'seem', 'a', ',', 'a', 'lot', 'less', 'vibrant', 'then', 'many', 'of', 'his', 'other', 'paintings', ',', 'and', 'so', 'they', ',', 'they', 'attracted', 'me', 'and', 'have', 'a', ',', 'a', 'strong', 'sense', 'of', 'er', '##m', ',', 'er', 'a', 'hidden', 'desire', 'in', 'that', 'and', 'so', 'it', ',', 'to', 'use', 'it', 'as', 'a', 'writing', 'exercise', 'which', 'i', 'intend', 'doing', ',', 'it', 'will', 'be', 'able', 'about', 'a', 'situation', 'of', 'character', \"'\", 's', 'with', 'er', '##m', 'something', 'that', \"'\", 's', 'hidden', ',', 'some', 'desire', ',', 'i', 'do', 'n', \"'\", 't', ',', 'the', ',', 'not', 'even', 'spoke', 'about', 'to', 'themselves', 'or', ',', 'or', 'generally', ',', 'er', '##m', 'i', 'like', 'to', 'sort', 'of', 'say', 'that', 'came', 'from', 'those', 'two', 'little', ',', 'just', 'this', 'amounts', 'of', 'colour', 'which', 'seemed', 'to', 'be', 'saying', 'such', 'a', 'lot', '[SEP]']\n",
      "processed 200 words\n",
      "calculating clusters for machine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'christ', 'god', 'dealt', 'with', 'the', 'problem', 'which', 'spoiled', 'his', 'image', 'in', 'us', 'and', 'he', 'has', 'to', 'do', 'it', 'because', 'of', 'fundamental', 'thing', ',', 'he', \"'\", 's', 'got', 'ta', 'do', 'it', 'from', 'the', 'centre', ',', 'you', 'know', 'you', 'can', 'get', 'an', 'apple', ',', 'an', 'ordinary', 'apple', 'and', 'you', 'can', 'polish', 'it', 'up', 'and', 'you', 'can', 'have', 'it', 'so', 'that', 'it', \"'\", 's', 'bright', 'and', 'glistening', 'and', 'the', 'red', 'is', 'almost', 'you', 'know', 'it', ',', 'it', ',', 'it', ',', 'it', 'almost', 'da', '##zzle', '##s', 'you', 'the', 'shining', 'on', 'it', ',', 'it', \"'\", 's', 'got', 'a', 'real', 'good', 'polish', 'on', 'the', 'skin', ',', 'but', 'inside', ',', 'there', \"'\", 's', 'a', 'gr', '##ub', ',', 'and', 'all', 'the', 'polish', '##ing', 'in', 'the', 'world', 'does', 'n', \"'\", 't', 'get', 'rid', 'of', 'the', 'gr', '##ub', ',', 'and', 'you', 'see', 'that', \"'\", 's', 'so', 'often', 'what', 'we', 'do', ',', 'we', 'polish', 'and', 'polish', 'away', 'on', 'the', 'outside', ',', 'that', \"'\", 's', 'go', '##n', 'na', 'make', 'us', 'better', 'but', 'it', \"'\", 's', 'only', 'skin', 'deep', 'because', 'inside', 'the', 'gr', '##ub', 'is', 'having', 'a', 'field', 'day', ',', 'he', \"'\", 's', 'having', 'a', 'party', 'of', 'all', 'party', \"'\", 's', ',', 'he', \"'\", 's', 'got', 'an', 'whole', 'apple', 'to', 'himself', 'and', 'the', 'gr', '##ub', 'of', 'sin', 'in', 'your', 'life', 'and', 'in', 'my', 'life', 'is', 'having', ',', 'has', 'a', 'field', 'day', 'and', 'we', 'polish', 'the', 'outside', 'and', 'we', 'try', 'and', 'make', 'it', 'look', 'good', 'and', 'we', 'be', 'we', 'become', 'present', '##able', 'and', 'there', 'like', 'the', 'apple', 'on', 'the', 'market', 'stall', 'it', 'looks', 'good', ',', 'it', 'looks', 'tremendous', 'until', 'you', 'take', 'a', 'bite', 'out', 'of', 'it', 'and', 'you', 'see', 'in', 'the', 'bit', 'that', 'you', \"'\", 've', 'bitten', 'there', \"'\", 's', 'a', ',', 'there', \"'\", 's', 'a', 'hole', 'going', 'through', 'and', 'you', 'wonder', 'where', 'the', 'gr', '##ub', 'is', ',', 'is', 'it', 'in', 'the', 'bit', 'that', \"'\", 's', 'left', 'or', 'in', 'the', 'bit', 'that', 'you', \"'\", 've', 'eaten', 'and', 'this', 'is', 'just', 'like', 'sin', 'you', 'see', 'in', 'our', 'lives', 'and', 'so', 'god', 'in', 'christ', 'he', 'did', 'n', \"'\", 't', 'deal', 'with', 'the', 'outside', 'bit', ',', 'he', 'did', 'n', \"'\", 't', 'bother', 'trying', 'to', 'make', 'our', 'conditions', 'better', ',', 'he', 'did', 'n', \"'\", 't', 'bother', 'trying', 'to', 'work', 'on', 'the', 'outside', ',', 'that', \"'\", 's', 'the', 'difference', 'between', 'the', 'gospel', 'and', 'social', 'work', 'and', 'there', \"'\", 's', 'nothing', 'wrong', 'with', 'social', 'work', ',', 'it', \"'\", 's', 'just', 'that', 'it', \"'\", 's', 'going', ',', 'it', \"'\", 's', 'coming', 'from', 'the', 'wrong', 'end', ',', 'it', 'starts', 'on', 'the', 'outside', ',', 'it', 'will', 'educate', 'people', 'if', 'we', 'give', 'them', 'better', 'housing', ',', 'if', 'we', 'give', 'them', 'better', 'circumstances', ',', 'if', 'we', 'give', 'them', 'better', 'wages', ',', 'now', 'all', 'these', 'things', 'are', 'right', 'and', 'that', 'we', 'should', 'have', 'them', ',', 'but', 'that', 'does', 'n', \"'\", 't', 'make', 'any', 'difference', ',', 'you', 'see', ',', 'the', 'person', 'is', 'a', 'sinn', '##er', ',', 'all', 'he', 'becomes', 'if', 'you', 'educate', 'him', 'is', 'an', 'educated', 'sinn', '##er', ',', 'if', 'you', 'give', 'him', 'a', 'huge', 'pay', 'rise', 'all', 'he', 'becomes', 'is', 'a', 'rich', 'sinn', '##er', ',', 'if', 'you', 'put', 'him', 'in', 'a', 'palace', 'all', 'he', 'becomes', 'is', 'er', 'a', 'sinn', '##er', 'living', 'in', 'a', 'palace', ',', 'it', 'does', 'n', \"'\", 't', 'make', 'any', 'basic', 'difference', 'to', 'the', 'person', '.', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2292 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'if', 'we', \"'\", 've', 'come', 'to', 'guard', 'and', 'received', 'forgiveness', 'of', 'sins', ',', 'if', 'we', 'have', 'become', 'good', 'followers', 'of', 'jesus', 'christ', 'and', 'we', 'are', 'not', 'amazed', 'then', 'there', \"'\", 's', 'something', 'wrong', 'with', 'what', 'we', \"'\", 've', 'received', 'that', 'god', 'should', 'so', 'love', ',', 'not', 'just', 'the', 'world', ',', 'but', 'should', 'so', 'love', 'me', ',', 'that', 'he', 'gave', 'his', 'son', 'to', 'die', 'for', 'me', 'and', 'that', 'was', 'the', 'sort', 'of', 'er', 'discovery', 'that', 'these', 'four', 'le', '##pers', 'made', 'they', \"'\", 've', 'come', 'down', 'there', ',', 'they', \"'\", 've', 'found', 'that', 'the', 'sight', 'before', 'them', 'was', 'amazing', ',', 'there', 'was', 'no', 'enemy', 'there', ',', 'the', 'enemy', 'had', 'disappeared', 'and', 'the', 'tents', 'with', 'all', 'their', 'contents', 'were', 'there', 'before', 'them', ',', 'they', 'were', 'amazed', 'with', 'what', 'they', 'found', 'and', 'you', 'and', 'i', 'when', 'we', 'come', 'to', 'god', 'through', 'jesus', 'christ', ',', 'we', 'are', 'amazed', 'at', 'what', 'we', 'find', ',', 'we', 'find', 'forgiveness', ',', 'we', 'find', 'the', 'restoration', 'of', 'a', 'relationship', 'between', 'ourselves', 'and', 'god', ',', 'we', 'find', 'an', 'access', 'to', 'receive', 'god', \"'\", 's', 'blessing', 'to', 'receive', 'his', 'favour', ',', 'to', 'receive', 'his', 'gifts', 'that', 'he', 'has', 'for', 'us', ',', 'no', 'wonder', 'the', 'apostle', 'paul', 'cries', 'out', 'thanks', 'beyond', 'to', 'god', 'for', 'his', 'un', '##sp', '##eak', '##able', 'gift', ',', 'but', 'then', 'again', 'these', 'four', 'men', 'they', 'were', 'not', 'just', 'amazed', 'that', 'what', 'they', 'found', ',', 'they', 'were', ',', 'they', 'got', 'absorbed', 'in', 'what', 'they', 'got', ',', 'because', 'they', 'got', 'a', 'lot', 'more', 'than', 'they', 'bargain', '##ed', 'for', ',', 'they', 'possibly', 'in', 'their', 'wilde', '##st', 'dreams', 'thought', 'they', 'might', 'at', 'least', 'get', ',', 'get', 'what', 'the', 'cook', 'was', 'throwing', 'out', ',', 'they', 'might', 'get', 'to', ',', 'to', 'the', 'dust', '##bin', '##s', ',', 'they', 'might', 'get', 'what', 'was', 'left', 'over', ',', 'that', 'would', 'of', 'been', 'great', ',', 'they', 'were', 'dying', 'of', 'starvation', ',', 'the', 'dr', '##iest', 'mo', '##uld', '##iest', 'crust', 'would', 'of', 'been', 'like', ',', 'like', 'a', 'banquet', 'to', 'them', ',', 'but', 'they', 'got', 'so', 'much', 'more', 'than', 'they', 'anticipated', 'and', 'they', 'got', 'absorbed', 'in', 'it', ',', 'every', 'thing', 'was', 'there', \"'\", 's', 'for', 'the', 'taking', 'as', 'they', 'pulled', 'back', 'the', ',', 'the', 'flap', 'of', 'the', 'tent', 'as', 'they', 'go', 'in', 'and', 'they', 'see', 'the', 'tables', 'laid', 'out', 'there', ',', 'they', 'see', 'the', 'food', 'and', 'the', 'drink', ',', 'they', 'see', 'the', 'plenty', ',', 'these', 'men', 'who', 'for', 'weeks', 'have', 'known', 'terrible', 'poverty', ',', 'there', 'might', 'of', 'been', 'a', 'time', 'earlier', 'on', 'in', 'the', 'siege', 'when', 'a', 'few', 'scrap', '##s', 'got', 'thrown', 'over', 'the', 'city', 'wall', ',', 'when', 'the', 'bin', '##s', 'were', 'put', 'out', 'the', 'side', 'of', 'the', 'city', 'of', 'an', 'evening', ',', 'er', 'they', 'would', 'go', 'there', 'and', 'for', '##age', 'amongst', 'them', ',', 'but', 'all', 'that', 'had', 'stopped', 'long', 'since', 'and', 'it', 'was', 'only', 'the', 'bits', 'and', 'pieces', 'that', 'they', 'managed', 'to', 'for', '##age', 'for', 'themselves', 'and', 'get', 'for', 'themselves', 'that', 'they', \"'\", 'd', 'been', 'eating', 'of', 'late', ',', 'but', 'here', 'every', 'thing', 'is', 'there', 'for', 'the', 'taking', ',', 'they', 'rubbed', 'their', 'eyes', ',', 'they', 'pinched', 'one', 'another', 'to', 'make', 'sure', 'their', 'not', 'dreaming', ',', 'it', 'really', 'is', 'food', 'and', 'drink', 'in', 'a', ',', 'in', 'an', 'abundance', 'they', 'could', 'n', \"'\", 't', 'of', 'thought', 'of', 'a', 'few', 'mo', 'hours', 'earlier', 'one', 'moment', 'they', 'had', 'nothing', ',', 'the', 'next', 'they', \"'\", 've', 'got', 'every', 'thing', ',', 'what', 'was', 'it', 'they', 'needed', ',', 'food', ',', 'the', 'tables', 'would', 'of', 'been', 'laden', 'with', 'it', ',', 'it', 'was', 'the', 'food', ',', 'enough', 'food', 'for', 'an', 'army', 'and', 'there', \"'\", 's', 'only', 'four', 'of', 'them', ',', 'did', 'they', ',', 'were', 'they', 'thirsty', ',', 'here', 'was', 'drink', ',', 'here', 'was', 'wine', 'and', ',', 'and', 'drink', 'in', 'abundance', 'the', 'rags', ',', 'the', 'ta', '##tters', 'they', 'were', 'dressed', 'in', ',', 'there', 'were', 'garments', 'and', 'wardrobe', 'full', 'of', 'clothes', 'here', 'for', 'them', ',', 'did', 'they', 'need', 'money', ',', 'well', 'the', 'tents', 'were', 'full', 'of', 'the', 'gold', 'and', 'the', 'silver', 'and', ',', 'and', ',', 'and', 'valuable', '##s', ',', 'there', 'were', 'a', 'su', '##ffi', '##ciency', ',', 'every', 'thing', 'was', 'there', 'you', 'know', 'the', 'idea', 'that', 'the', 'christian', 'life', 'is', 'dr', '##ab', 'and', 'poor', 'is', 'such', 'a', 'terrible', 'false', 'hood', ',', 'its', 'an', 'in', '##iq', '##uit', '##ous', 'lie', 'of', 'the', 'devil', ',', 'the', 'tragedy', 'is', 'that', 'we', 'have', 'actually', 'often', 'made', 'it', 'that', 'way', ',', 'we', 'have', 'made', 'the', 'christian', 'faith', 'something', 'dr', '##ab', ',', 'something', 'boring', ',', 'something', 'for', 'old', 'folk', 'er', 'and', 'er', 'you', 'know', ',', 'people', 'who', 'are', ',', 'who', 'are', ',', 'just', 'wanting', 'a', 'cr', '##ut', '##ch', 'because', 'their', 'coming', 'to', 'the', 'end', 'of', 'their', 'natural', 'life', 'and', 'we', \"'\", 've', 'made', 'it', 'something', 'dr', '##ab', 'and', 'dull', 'listen', 'to', 'what', 'the', 'apostle', 'paul', 'says', 'when', 'he', \"'\", 's', 'writing', 'to', 'car', '##int', '##hian', \"'\", 's', 'in', 'his', 'second', 'letter', 'in', 'chapter', 'eight', ',', 'he', 'says', 'you', 'know', 'the', 'grace', 'of', 'our', 'lord', 'jesus', 'christ', ',', 'that', 'though', 'he', 'was', 'rich', 'for', 'your', 'sake', '##s', 'he', 'became', 'poor', ',', 'so', 'that', 'you', 'through', 'his', 'poverty', 'might', 'be', 'rich', ',', 'god', ',', 'he', \"'\", 's', 'purpose', 'follows', 'his', 'people', ',', 'he', \"'\", 's', 'not', 'that', 'we', \"'\", 've', 'a', 'dr', '##ab', ',', 'grey', ',', 'dull', 'un', '##int', '##eres', '##ting', 'life', ',', 'jesus', 'said', 'i', \"'\", 've', 'come', 'that', 'you', 'might', 'of', 'life', ',', 'and', 'that', 'more', 'abundant', ',', 'that', 'in', 'all', 'its', 'full', '##ness', 'and', 'god', 'has', 'purpose', 'for', 'us', ',', 'and', 'when', 'paul', 'is', 'talking', 'about', 'riches', 'there', ',', 'he', \"'\", 's', 'not', 'talking', 'about', 'pounds', 'and', 'pen', '##ce', ',', 'he', \"'\", 's', 'talking', 'about', 'the', 'rich', '##ness', 'of', 'the', 'life', 'that', 'we', 'enjoy', 'its', 'not', 'a', 'case', 'of', 'not', 'doing', 'this', 'and', 'having', 'to', 'do', 'that', 'the', 'other', 'thing', ',', 'its', 'a', 'case', 'of', 'enjoying', 'life', 'as', 'god', 'purposes', 'it', ',', 'as', 'god', 'intends', 'it', 'you', 'know', 'if', 'you', 'do', 'n', \"'\", 't', 'enjoy', 'your', 'christian', 'life', 'now', ',', 'let', 'me', 'tell', 'you', 'your', 'in', 'for', 'a', 'rude', 'awakening', 'when', 'you', 'get', 'to', 'heaven', ',', 'because', 'the', 'quality', 'of', 'life', 'is', 'not', 'go', '##n', 'na', 'change', 'the', 'only', 'things', 'that', \"'\", 'll', 'change', 'is', 'its', 'la', 'it', ',', 'it', 'will', ',', 'it', 'will', 'be', 'in', 'his', 'presence', ',', 'the', 'quality', 'of', 'life', 'will', 'not', 'change', 'because', 'already', 'now', 'we', 'have', 'received', 'eternal', 'life', ',', 'he', 'has', 'given', 'his', 'life', 'to', 'us', 'and', 'he', 'has', 'n', \"'\", 't', 'got', 'some', 'other', 'special', ',', 'you', 'know', ',', 'super', 'du', '##per', 'life', 'laid', 'up', ',', 'there', \"'\", 's', 'nothing', ',', 'there', \"'\", 's', 'nothing', 'greater', 'ahead', ',', 'god', 'has', 'n', \"'\", 't', 'got', 'any', 'thing', 'greater', 'for', 'us', 'than', 'what', 'he', \"'\", 's', 'already', 'given', 'to', 'us', 'in', 'embryo', '##nic', 'form', 'here', 'and', 'now', 'why', 'if', 'we', 'take', 'on', 'er', 'a', ',', 'a', ',', 'a', 'dazzling', 'sci', '##nti', '##lla', '##ting', 'new', 'ze', '##st', 'and', 'za', '##p', 'when', 'you', 'get', 'to', 'heaven', ',', 'that', 'life', 'is', 'already', 'given', 'to', 'you', 'and', 'to', 'me', 'know', 'go', 'back', 'to', 'these', 'four', 'men', 'at', 'the', 'moment', ',', 'they', 'had', 'never', 'known', 'any', 'thing', 'like', 'this', 'before', 'this', 'was', 'better', 'than', 'all', 'their', 'birthday', '##s', 'rolled', 'into', 'one', ',', 'this', 'was', 'the', 'greatest', 'day', 'in', 'their', 'experience', 'and', 'if', 'they', 'would', 'live', 'to', 'be', 'a', 'hundred', 'they', 'would', 'never', 'know', 'another', 'day', 'like', 'this', ',', 'they', 'were', 'having', 'a', 'tremendous', 'time', ',', 'it', 'said', 'they', ',', 'they', ',', 'they', ',', 'they', 'went', 'into', 'one', 'tent', ',', 'listen', 'to', 'what', 'they', 'did', ',', 'they', 'went', 'into', 'one', 'tent', 'and', 'they', ',', 'they', 'ate', ',', 'they', 'drank', ',', 'they', 'had', 'a', 'party', 'and', 'they', 'carried', 'from', 'there', 'the', 'silver', ',', 'the', 'gold', 'and', 'the', 'clothes', 'and', 'they', 'went', 'and', 'hid', 'they', 'returned', 'and', 'entered', 'another', 'tent', 'and', 'then', 'they', 'did', 'the', 'same', 'there', ',', 'they', 'were', 'having', 'a', 'tremendous', 'time', ',', 'this', 'was', 'a', 'bean', '##o', 'to', 'end', 'all', 'bean', '##o', \"'\", 's', ',', 'this', 'was', 'the', 'greatest', 'day', 'in', 'their', 'life', ',', 'they', 'were', 'having', 'a', 'wonderful', 'time', 'and', 'why', 'should', 'n', \"'\", 't', 'they', ',', 'why', 'not', 'you', 'know', 'there', 'are', 'folk', 'who', 'would', ',', 'who', \"'\", 'd', 'want', 'to', 'make', 'us', 'as', 'christians', 'er', 'and', 'er', ',', 'ee', ',', 'put', 'us', 'into', 'a', 'straight', 'jacket', 'the', 'bible', 'tells', 'me', 'even', 'the', 'sunset', '##s', 'free', ',', 'is', 'free', 'indeed', 'and', 'i', 'do', 'n', \"'\", 't', 'see', 'any', 'suggestions', 'as', 'i', 'read', 'the', 'new', 'testament', ',', 'that', 'first', 'of', 'all', 'the', 'life', 'of', 'jesus', 'was', 'dr', '##ab', 'and', 'un', '##int', '##eres', '##ting', ',', 'or', 'that', 'he', 'expects', 'me', 'as', 'his', 'follow', 'to', 'lead', 'a', 'dr', '##ab', ',', 'a', 'grey', 'life', ',', 'oh', 'its', 'not', 'always', 'go', '##n', 'na', 'be', 'a', 'pl', '##e', 'an', 'easy', 'life', 'but', 'that', 'does', 'n', \"'\", 't', 'reduce', 'the', ',', 'the', 'ze', '##st', 'and', 'the', 'excitement', 'in', 'it', 'but', 'you', 'see', 'the', 'danger', 'is', 'when', 'having', 'a', 'good', 'time', 'is', 'the', 'reason', 'for', 'living', 'and', 'the', 'only', 'reason', 'for', 'it', ',', 'you', 'see', ',', 'if', 'god', 'has', 'intervened', 'in', 'our', 'life', ',', 'if', 'the', 'message', 'of', 'the', 'gospel', 'is', 'true', ',', 'if', 'god', 'in', 'christ', 'has', 'taken', 'away', 'your', 'sin', 'and', 'made', 'you', 'in', 'christ', 'a', 'new', 'creation', 'then', 'you', 'have', 'every', 'reason', 'to', 'enjoy', 'life', ',', 'in', 'a', 'sense', 'your', 'only', 'able', 'to', 'start', 'enjoying', 'life', 'now', ',', 'you', 'may', 'have', 'enjoyed', 'some', 'of', 'the', 'things', 'that', ',', 'that', 'folks', 'suggest', 'that', 'make', 'up', 'life', ',', 'but', 'they', \"'\", 've', 'finished', ',', 'there', 'gone', ',', 'what', 'happens', 'when', 'the', ',', 'when', ',', 'when', 'the', 'wine', 'has', 'run', 'out', ',', 'what', 'happens', 'when', 'the', 'parties', 'over', ',', 'you', 'know', 'all', 'about', 'it', 'the', 'next', 'day', ',', 'do', 'n', \"'\", 't', 'you', ',', 'what', 'happens', 'then', ',', 'its', 'such', 'short', 'lived', ',', 'its', 'only', 'worth', 'having', 'whilst', 'its', 'coming', 'to', 'you', 'all', 'the', 'time', ',', 'but', 'that', \"'\", 's', 'not', 'so', 'with', 'a', 'christian', 'life', ',', 'because', 'it', 'doe', ',', 'depend', 'on', 'just', 'the', 'things', 'that', 'we', 'have', 'or', 'the', 'experiences', 'that', 'we', 'go', 'through', ',', 'because', 'it', 'is', 'something', 'that', ',', 'that', 'we', 'have', 'within', ',', 'it', 'is', ',', 'it', 'is', 'a', 'quality', 'of', 'life', 'that', 'we', 'possess', ',', 'because', 'we', 'possess', 'the', 'one', 'who', 'is', 'life', 'himself', ',', 'listen', 'to', 'what', 'paul', 'says', 'when', 'he', \"'\", 's', 'writing', 'to', 'timothy', 'in', 'his', 'first', 'letter', 'in', 'chapter', 'six', 'it', 'is', 'command', 'those', 'who', 'are', 'rich', 'in', 'this', 'present', 'world', 'not', 'to', 'be', 'arrogant', 'or', 'to', 'put', 'their', 'hope', 'in', 'wealth', ',', 'those', 'who', 'think', 'that', ',', 'that', 'er', 'in', 'having', 'possessions', 'that', 'is', 'the', 'secret', 'of', 'life', 'or', ',', 'or', 'in', 'having', 'a', 'good', 'time', 'and', ',', 'and', ',', 'and', 'the', 'rest', 'of', 'it', 'that', 'is', 'what', 'life', 'is', 'all', 'about', ',', 'he', 'said', 'warn', 'them', 'not', 'to', 'do', 'that', ',', 'because', 'that', 'is', 'so', 'uncertain', ',', 'he', 'says', 'but', 'to', 'put', 'their', 'hope', 'in', 'god', 'who', 'richly', 'provides', 'us', 'with', 'every', 'thing', 'what', 'for', ',', 'for', 'our', 'enjoyment', ',', 'why', 'has', 'god', 'given', 'us', 'these', 'things', ',', 'why', 'is', ',', 'why', 'is', 'god', 'even', ',', 'he', 'is', 'natural', 'creation', 'there', 'for', 'us', ',', 'it', 'is', 'for', 'our', 'enjoyment', ',', 'its', 'not', 'to', 'make', 'us', 'miserable', 'or', 'to', 'make', 'us', 'grey', 'and', 'dr', '##ab', 'and', 'burden', 'by', 'it', ',', 'it', 'is', 'for', 'us', 'to', 'enjoy', ',', 'when', 'god', 'created', 'ada', 'adam', 'and', 'eve', 'and', 'put', 'them', 'in', 'the', 'garden', ',', 'the', ',', 'they', 'were', 'told', 'to', 'enjoy', 'it', ',', 'even', 'the', 'fruit', 'enjoy', 'it', ',', 'its', 'there', 'for', 'your', 'benefit', 'and', 'then', 'the', 'new', 'creation', ',', 'every', 'thing', 'that', 'god', 'has', 'provided', 'is', 'there', 'for', 'our', 'enjoyment', ',', 'but', 'the', 'dangers', 'is', 'when', 'that', 'enjoyment', ',', 'is', 'the', 'reason', 'for', 'living', 'and', 'that', \"'\", 's', 'all', 'we', 'do', 'it', 'for', 'and', 'were', 'so', 'taking', 'up', 'with', 'ourselves', ',', 'i', 'am', 'go', '##n', 'na', 'have', 'my', 'good', 'time', ',', 'i', \"'\", 'm', 'go', '##n', 'na', 'enjoy', 'myself', 'as', 'a', 'christian', 'and', 'i', 'can', 'do', 'it', 'and', 'you', 'can', 'do', 'it', ',', 'you', 'become', 'ins', '##ular', 'and', 'we', 'become', 'intro', '##verted', 'and', 'the', 'only', 'thing', 'that', 'matters', 'is', 'me', 'having', 'a', 'good', 'time', ',', 'my', 'world', 'centres', 'around', 'me', 'and', 'me', 'enjoying', 'myself', 'and', 'me', 'having', 'this', 'and', 'me', 'having', 'that', ',', 'this', 'blessing', 'and', 'that', 'gift', 'and', 'that', 'other', 'blessing', ',', 'we', 'become', 'self', 'centred', 'and', 'taken', 'up', 'with', 'our', 'own', 'good', 'times', ',', 'as', 'long', 'as', 'i', 'can', 'be', 'there', 'in', 'the', 'centre', ',', 'as', 'long', 'as', 'i', 'can', 'go', 'from', ',', 'from', ',', 'from', 'this', 'celebration', 'to', 'that', 'celebration', ',', 'as', 'long', 'as', 'i', 'can', 'go', 'from', 'this', 'er', 'festival', 'to', 'that', 'festival', 'to', 'this', 'special', 'meeting', 'to', 'that', 'one', ',', 'i', \"'\", 'm', 'go', '##n', 'na', 'have', 'my', 'good', 'time', 'well', 'that', 'was', 'what', 'these', 'fell', '##a', \"'\", 's', 'were', 'doing', ',', 'they', 'were', 'going', 'from', 'tent', 'to', 'tent', ',', 'from', 'celebration', 'to', 'celebration', 'having', 'a', 'great', 'time', 'and', 'then', 'the', 'truth', 'hit', 'them', 'they', 'were', 'ashamed', 'with', 'what', 'they', 'had', 'done', ',', 'they', 'said', 'to', 'one', 'another', 'we', 'are', 'not', 'doing', 'right', ',', 'this', 'days', 'a', 'day', 'of', 'good', 'news', ',', 'but', 'we', 'are', 'keeping', 'silent', ',', 'if', 'we', 'wait', 'until', 'morning', 'light', 'punishment', 'will', 'over', '##take', 'us', ',', 'now', 'therefore', ',', 'come', ',', 'let', 'us', 'go', 'and', 'tell', 'the', 'kings', 'household', 'how', 'guilty', 'are', 'we', ',', 'how', 'guilty', 'are', 'you', ',', 'how', 'guilty', 'am', 'i', 'of', 'the', 'sinn', '##er', 'silence', ',', 'remember', 'how', 'we', 'started', ',', 'its', 'not', 'always', 'the', 'things', 'that', 'we', 'do', 'its', 'often', 'the', 'things', 'that', 'we', 'do', 'n', \"'\", 't', 'do', ',', 'how', 'guilty', 'are', 'we', 'of', 'the', 'sin', 'of', 'silence', 'these', 'men', 'had', 'known', 'nothing', ',', 'known', 'poverty', 'and', ',', 'and', ',', 'and', ',', 'and', 'starvation', ',', 'they', 'were', 'amazed', 'at', 'what', 'they', \"'\", 'd', 'found', ',', 'they', \"'\", 'd', 'became', 'absorbed', 'in', 'what', 'they', 'had', 'got', 'and', 'now', 'they', \"'\", 'd', 'became', 'ashamed', 'of', 'what', 'they', 'had', 'done', 'with', 'it', 'what', 'was', 'the', 'sin', 'that', 'troubled', 'these', 'men', 'they', 'said', 'we', 'are', 'keeping', 'silent', '.', '[SEP]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough tokens to make 5 clusters for word: disorganize\n",
      "not enough tokens to make 7 clusters for word: disorganize\n",
      "not enough tokens to make 5 clusters for word: disorganize\n",
      "not enough tokens to make 7 clusters for word: disorganize\n",
      "not enough tokens to make 5 clusters for word: disorganize\n",
      "not enough tokens to make 7 clusters for word: disorganize\n",
      "not enough tokens to make 5 clusters for word: disorganize\n",
      "not enough tokens to make 7 clusters for word: disorganize\n",
      "processed 300 words\n",
      "calculating clusters for cabbage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (801 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'yeah', ',', 'well', 'let', 'me', 'just', 'read', 'you', 'two', 'or', 'three', 'verses', 'from', 'exodus', ',', 'chapter', 'forty', ',', 'this', 'is', 'what', 'it', 'says', 'then', 'the', 'cloud', 'covered', 'the', 'tent', 'of', 'meeting', 'and', 'the', 'glory', 'of', 'the', 'lord', 'filled', 'the', 'tab', '##ern', '##acle', 'and', 'moses', 'was', 'not', 'able', 'to', 'enter', 'the', 'tent', 'of', 'meeting', 'because', 'the', 'cloud', 'had', 'settled', 'on', 'it', 'and', 'the', 'glory', 'of', 'the', 'lord', 'filled', 'the', 'tab', '##ern', '##acle', 'and', 'throughout', 'all', 'their', 'journeys', 'whenever', 'the', 'cloud', 'was', 'taken', 'up', 'from', 'over', 'the', 'tab', '##ern', '##acle', 'the', 'sons', 'of', 'israel', 'would', 'set', 'out', ',', 'but', 'if', 'the', 'cloud', 'was', 'not', 'taken', 'up', 'then', 'they', 'did', 'not', 'set', 'out', 'until', 'the', 'day', 'that', 'it', 'was', 'taken', 'up', ',', 'for', 'throughout', 'all', 'their', 'journeys', 'the', 'cloud', 'of', 'the', 'lord', 'was', 'on', 'the', 'tab', '##ern', '##acle', 'by', 'day', 'and', 'there', 'was', 'fire', 'in', 'it', 'by', 'night', ',', 'in', 'the', 'sight', 'of', 'all', 'the', 'house', 'in', 'israel', 'and', 'if', 'you', 'were', 'to', 'turn', 'over', 'to', 'kings', 'you', \"'\", 've', 'got', 'a', ',', 'you', \"'\", 've', 'got', 'a', 'similar', 'thing', 'there', 'with', 'the', 'dedication', 'of', 'the', 'temple', 'and', 'as', 'be', 'ben', 'was', 'saying', 'the', 'power', 'really', 'it', \"'\", 's', 'the', 'it', \"'\", 's', 'the', 'presence', 'of', 'god', ',', 'the', 'shine', ',', 'the', 'glory', ',', 'that', 'cloud', 'of', ',', 'and', 'so', 'what', ',', 'what', ',', 'what', 'catches', 'the', 'lord', 'jesus', 'up', 'is', 'really', 'the', 'glory', 'of', 'god', 'here', 'he', 'is', ',', 'the', ',', 'the', 'risen', '##ess', ',', 'the', 'g', '##lor', '##ified', 'christ', 'being', 'called', 'up', 'into', 'heaven', 'in', 'the', ',', 'in', 'the', 'glory', ',', 'what', 'he', \"'\", 's', 'been', 'g', '##lor', '##ified', ',', 'so', 'he', 'withdraw', '##s', 'his', 'physical', ',', 'physical', 'presence', 'from', 'one', 'place', 'here', 'on', 'earth', 'to', 'present', 'there', 'on', 'the', 'throne', 'and', 'yet', 'by', 'the', 'holy', 'spirit', 'to', 'be', 'every', 'where', 'now', 'jesus', 'then', ',', 'he', 'did', 'n', \"'\", 't', 'cease', 'to', 'be', 'truly', 'man', 'at', 'either', 'his', 'resurrection', 'or', 'at', 'his', 'ascension', ',', 'he', 'stays', 'man', ',', 'god', ',', 'the', 'god', 'man', 'all', 'the', 'way', 'through', 'and', 'it', \"'\", 's', 'still', 'true', 'today', 'he', 'is', 'the', 'god', 'man', 'today', 'and', 'that', \"'\", 's', 'important', 'for', 'you', 'and', 'me', ',', 'think', 'of', 'the', 'very', 'worse', 'experience', 'that', 'you', 'have', 'ever', 'had', 'in', 'your', 'life', ',', 'think', 'of', 'the', 'very', 'worse', 'experience', 'that', 'could', 'happen', 'to', 'you', ',', 'with', 'the', 'exception', 'of', 'you', 'know', 'that', 'of', ',', 'of', 'say', 'total', 'failure', 'of', 'some', 'awful', 'sin', ',', 'the', 'worse', 'thing', ',', 'maybe', 'a', 'loss', 'of', 'someone', 'dear', 'to', 'you', ',', 'someone', 'very', 'close', 'to', 'you', ',', 'er', ',', 'er', ',', 'a', 'be', '##rea', '##ve', '##ment', ',', 'the', 'most', 'awful', 'experience', 'you', 'have', 'had', 'well', 'he', 'has', 'gone', 'through', ',', 'he', 'has', 'known', 'that', 'experience', ',', 'he', 'has', ',', 'has', 'tempted', 'in', 'all', 'points', 'like', 'as', 'we', 'are', 'he', 'knows', 'our', 'frame', ',', 'he', 'remembers', 'were', 'dust', 'and', 'he', 'has', 'been', 'there', 'and', 'it', 'is', 'a', 'man', 'who', 'has', 'experienced', 'those', 'same', 'experiences', 'that', 'you', 'and', 'i', 'experience', 'day', 'by', 'day', ',', 'year', 'after', 'year', ',', 'it', 'is', 'a', 'man', 'who', 'has', 'gone', 'that', ',', 'who', 'has', 'walked', 'that', 'path', ',', 'who', 'is', 'in', 'heaven', 'inter', '##ced', '##ing', 'and', 'praying', 'for', 'us', ',', 'we', \"'\", 'll', 'stop', 'there', 'co', '##s', 'time', 'has', 'gone', 'er', '##m', 'we', \"'\", 'll', 'stop', 'there', ',', 'we', 'wo', 'n', \"'\", 't', 'go', 'on', 'otherwise', 'i', \"'\", 'll', 'get', 'into', 'trouble', 'during', 'this', 'past', 'month', 'some', 'of', 'the', 'questions', 'in', 'the', 'new', 'testament', ',', 'the', 'first', 'one', 'we', 'looked', 'at', 'you', 'remember', 'was', 'that', 'question', 'that', 'jesus', 'asked', 'of', 'his', 'disciples', ',', 'do', 'you', 'believe', 'that', 'i', 'am', 'able', 'to', 'do', 'this', ',', 'then', 'we', 'looked', 'at', 'a', 'question', 'which', 'the', 'disciples', 'asked', 'of', 'jesus', ',', 'why', 'could', 'we', 'not', 'cast', 'it', 'out', 'last', 'week', 'we', 'looked', 'at', 'another', 'question', ',', 'are', 'only', 'a', 'few', 'people', 'going', 'to', 'be', 'saved', 'and', 'this', 'morning', 'i', \"'\", 'd', 'like', 'us', 'it', \"'\", 's', 'the', 'final', 'one', 'of', 'these', 'questions', 'not', 'that', 'there', 'are', 'n', \"'\", 't', 'other', 'questions', 'in', 'the', 'new', 'testament', 'and', 'scores', ',', 'scores', 'of', 'others', 'but', 'were', 'just', 'looking', 'at', 'four', 'er', 'throughout', 'this', 'month', ',', 'i', \"'\", 'd', 'like', 'us', 'to', 'look', 'this', 'morning', 'for', 'one', 'at', ',', 'for', 'a', 'few', 'minutes', ',', 'at', 'one', 'that', 'jesus', 'asked', 'of', 'a', 'man', 'who', 'confronted', 'him', ',', 'i', \"'\", 'd', 'like', 'to', 'read', 'a', 'few', 'verses', 'from', 'luke', 'chapter', 'eighteen', ',', 'luke', 'chapter', 'eighteen', 'i', \"'\", 'm', 'go', '##n', 'na', 'read', 'from', 'verse', 'thirty', 'five', ',', 'it', \"'\", 's', 'the', 'well', 'known', 'account', 'of', 'blind', 'bart', '##ima', '##eus', ',', 'luke', 'chapter', 'eighteen', 'and', 'verse', 'thirty', 'five', 'and', 'he', 'came', 'about', 'that', 'as', 'jesus', 'was', 'approaching', 'jericho', 'a', 'certain', 'blind', 'man', 'was', 'sitting', 'by', 'the', 'road', 'begging', ',', 'now', 'hearing', 'a', 'multitude', 'going', 'by', 'he', 'began', 'to', 'in', '##qui', '##re', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'example', '3', ':', '1', 'limitation', 'on', 'liability', 'of', 'original', 'tenant', '(', '1', ')', 'in', 'this', 'clause', '\"', 'the', 'original', 'tenant', '\"', 'means', 'the', 'said', '…', 'only', 'and', 'this', 'clause', 'applies', 'to', 'any', 'period', 'after', 'the', 'term', 'here', '##by', 'granted', 'cease', '##s', 'to', 'be', 'vested', 'in', 'the', 'original', 'tenant', '(', '2', ')', 'if', 'and', 'so', 'often', 'as', 'the', 'tenant', 'fails', 'to', 'pay', 'the', 'rent', 'or', 'any', 'other', 'sum', 'properly', 'due', 'under', 'this', 'lease', 'or', 'commits', 'any', 'breach', 'of', 'covenant', 'known', 'to', 'the', 'landlord', 'then', 'the', 'landlord', 'shall', 'forth', '##with', 'not', '##ify', 'the', 'original', 'tenant', 'of', 'that', 'fact', '(', '3', ')', 'the', 'landlord', 'shall', 'not', 'be', 'entitled', 'to', 'recover', 'from', 'the', 'original', 'tenant', 'any', 'ar', '##rea', '##rs', 'of', 'rent', 'or', 'other', 'sums', 'pay', '##able', 'under', 'this', 'lease', 'where', 'the', 'rent', 'or', 'other', 'sums', 'claimed', 'became', 'due', 'earlier', 'than', 'three', 'months', 'before', 'the', 'original', 'tenant', 'was', 'notified', 'under', 'sub', '-', 'clause', '(', '2', ')', 'above', '(', '4', ')', 'the', 'original', 'tenant', 'shall', 'not', 'be', 'liable', 'for', 'any', 'ar', '##rea', '##rs', 'of', 'rent', 'or', 'other', 'sum', 'falling', 'due', 'after', 'the', 'date', 'upon', 'which', 'this', 'lease', 'is', 'expressed', 'to', 'ex', '##pire', 'or', 'any', 'breach', 'of', 'covenant', 'committed', 'after', 'that', 'date', 'example', '3', ':', '2', 'limitation', 'on', 'liability', 'of', 'tenant', '(', '1', ')', 'in', 'this', 'clause', '(', 'a', ')', '\"', 'the', 'original', 'tenant', '\"', 'means', 'only', '(', 'b', ')', '\"', 'the', 'original', 'assign', '##ee', '\"', 'means', 'a', 'person', 'to', 'whom', 'the', 'original', 'tenant', 'lawful', '##ly', 'assigns', 'this', 'lease', '(', '2', ')', 'upon', 'a', 'lawful', 'assignment', 'of', 'this', 'lease', 'by', 'the', 'original', 'tenant', 'the', 'original', 'tenant', '(', 'a', ')', 'shall', 'be', 'released', 'from', 'further', 'personal', 'liability', 'for', 'any', 'breach', 'of', 'any', 'of', 'the', 'tenant', \"'\", 's', 'obligations', 'under', 'this', 'lease', 'occurring', 'after', 'the', 'date', 'of', 'the', 'assignment', 'but', '(', 'b', ')', 'shall', 'guarantee', 'performance', 'by', 'the', 'original', 'assign', '##ee', 'of', 'those', 'obligations', 'until', 'the', 'ex', '##pi', '##ry', 'or', 'other', 'determination', 'of', 'the', 'term', 'or', '(', 'if', 'sooner', ')', 'a', 'lawful', 'assignment', 'of', 'this', 'lease', 'by', 'the', 'original', 'assign', '##ee', 'example', '3', ':', '3', 'restriction', 'on', 'landlord', \"'\", 's', 'ability', 'to', 'sue', 'original', 'tenant', 'at', 'any', 'time', 'after', 'the', 'lawful', 'assignment', 'of', 'this', 'lease', 'by', '[', 'name', 'of', 'original', 'tenant', ']', 'the', 'landlord', 'shall', 'not', 'be', 'entitled', 'to', 'enforce', 'against', 'him', 'the', 'tenant', \"'\", 's', 'obligations', 'under', 'this', 'lease', 'unless', 'the', 'landlord', 'shall', 'have', 'first', '(', '1', ')', 'recovered', 'judgment', 'against', 'all', 'other', 'persons', 'against', 'whom', 'the', 'landlord', 'is', 'or', 'has', 'become', 'entitled', 'to', 'enforce', 'those', 'obligations', 'either', 'as', 'principal', 'or', 'sure', '##ty', 'and', '(', '2', ')', 'attempted', 'to', 'levy', 'ex', '##cut', '##ion', 'upon', 'such', 'judgment', 'and', 'upon', 'payment', 'by', '[', 'name', 'of', 'original', 'tenant', ']', 'of', 'any', 'sum', 'due', 'under', 'such', 'judgment', 'the', 'landlord', 'shall', 'assign', 'to', 'him', 'the', 'benefit', 'of', 'it', 'example', '3', ':', '4', 'definition', 'clause', 'making', 'tenant', 'liable', 'for', 'rent', 'during', 'holding', 'over', 'period', '\"', 'the', 'term', '\"', 'includes', 'not', 'only', 'the', 'term', 'expressed', 'to', 'be', 'granted', 'by', 'this', 'lease', 'but', 'also', 'any', 'period', 'after', 'the', 'date', 'on', 'which', 'the', 'term', 'is', 'expressed', 'to', 'ex', '##pire', 'during', 'which', 'the', 'ten', '##ancy', 'continues', 'under', 'the', 'landlord', 'and', 'tenant', 'act', '1954', 'example', '3', ':', '5', 'clause', 'making', 'the', 'tenant', 'liable', 'to', 'pay', 'rent', 'and', 'interim', 'rent', 'promptly', 'to', 'pay', 'the', 'rent', 'reserved', 'by', 'this', 'lease', 'without', 'any', 'de', '##duction', 'or', 'set', '-', 'off', 'and', 'any', 'rent', 'substituted', 'for', 'it', 'either', 'as', 'a', 'result', 'of', 'a', 'rent', 'review', 'under', 'this', 'lease', 'or', 'the', 'agreement', 'or', 'determination', 'of', 'a', 'rent', 'pay', '##able', 'by', 'virtue', 'of', 'the', 'landlord', 'and', 'tenant', 'act', '1954', ',', 's', '##24', '##a', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'i', 'suppose', 'this', 'is', 'er', '##m', ',', 'a', 'car', '##ica', '##ture', ',', 'a', 'self', 'portrait', 'this', 'little', ',', 'i', ',', 'in', 'fact', 'i', 'was', ',', 'really', 'actually', 'hilarious', 'as', 'i', 'felt', 'that', ',', 'i', ',', 'i', ',', 'do', 'n', \"'\", 't', 'actually', 'know', 'an', 'awful', 'lot', 'about', 'ga', '##ug', '##in', ',', 'but', 'if', ',', 'if', 'i', 'knew', 'nothing', 'about', 'him', 'at', 'all', ',', 'i', 'would', 'of', 'thought', 'he', 'was', 'having', 'a', 'bit', 'of', 'joke', 'of', 'himself', 'with', 'this', ',', 'but', 'er', ',', 'being', 'the', 'person', 'that', 'he', 'was', 'i', 'ca', 'n', \"'\", 't', 'image', 'that', 'he', 'had', 'that', 'quality', ',', 'that', ',', 'i', 'do', 'n', \"'\", 't', 'believe', 'he', 'would', 'be', 'laughing', 'at', 'himself', ',', 'er', '##m', ',', 'er', '##m', ',', 'the', 'symbolism', 'er', '##m', 'and', 'conflict', 'of', 'this', 'painting', 'its', 'da', '##zzle', '##s', 'me', 'more', 'than', ',', 'than', 'the', 'colour', 'or', 'at', 'least', 'as', 'much', 'as', 'the', 'colours', 'in', 'it', ',', 'but', 'there', \"'\", 's', 'a', ',', 'there', \"'\", 's', 'a', 'half', 'eaten', ',', 'well', 'it', 'is', 'n', \"'\", 't', 'half', 'eaten', ',', 'but', 'there', \"'\", 's', 'half', 'an', 'apple', 'at', 'the', 'top', 'and', ',', 'and', 'that', 'was', 'the', ',', 'the', 'way', 'into', 'me', 'finally', ',', 'for', ',', 'for', 'writing', 'about', 'this', ',', 'this', 'again', 'is', 'a', 'shopping', 'list', ',', 'i', 'call', 'it', 'a', 'shopping', 'list', ',', 'this', 'is', 'just', 'visual', 'images', 'that', ',', 'that', 'will', 'be', 'opened', 'out', 'at', 'some', 'point', 'and', 'turn', 'it', 'into', 'something', ',', 'and', 'my', 'images', 'were', 'er', '##m', 'shoulders', 'of', 'the', 'mata', '##dor', 'smoking', 'snakes', ',', 'dare', 'to', 'bit', 'an', 'apple', ',', 'see', 'one', 'half', 'gone', 'and', 'still', 'i', 'wear', 'a', 'halo', 'intact', ',', 'that', 'i', \"'\", 'm', 'sure', 'i', \"'\", 've', 'completely', 'wrong', 'about', 'him', 'as', 'a', ',', 'a', 'person', ',', 'but', 'as', 'the', 'painting', 'that', \"'\", 's', 'obviously', 'something', 'else', ',', 'er', '##m', ',', 'i', 'found', 'that', 'one', 'of', 'the', 'things', 'that', 'were', 'he', \"'\", 's', ',', 'i', ',', 'i', 'think', 'probably', 'that', 'everybody', 'who', 'writes', 'is', 'that', 'you', \"'\", 'll', 'come', 'to', 'a', 'point', 'when', 'you', 'ca', 'n', \"'\", 't', 'write', ',', 'you', 'stop', 'writing', ',', 'you', 'have', 'n', \"'\", 't', 'got', 'anything', 'you', 'want', 'to', 'write', 'about', ',', 'or', 'your', 'frightened', 'of', 'writing', ',', 'and', 'i', 'devi', '##se', 'exercises', 'so', 'that', ',', 'that', 'does', 'n', \"'\", 't', 'happened', 'to', 'me', ',', 'i', 'think', 'writing', 'is', 'like', 'any', 'skill', 'you', 'have', 'to', 'keep', 'doing', 'it', 'to', 'be', 'able', 'to', 'do', 'it', ',', 'its', ',', 'you', ',', 'some', 'of', 'it', 'is', 'a', 'game', 'and', 'the', 'rest', 'of', 'it', 'is', 'hard', 'work', ',', 'and', 'one', 'of', 'the', 'exercises', 'i', ',', 'i', 'delighted', 'using', 'er', '##m', 'a', 'portrait', 'of', 'a', 'woman', 'er', '##m', ',', 'its', 'about', 'er', '##m', 'the', 'er', 'still', 'life', ',', 'its', 'the', 'back', 'one', ',', 'yes', 'this', 'one', 'here', ',', 'i', 'have', ',', 'i', ',', 'i', 'hope', 'to', 'use', 'this', 'as', 'a', 'writing', 'exercise', 'i', 'found', 'the', ',', 'the', 'math', '##s', 'in', 'this', 'and', 'the', 'colour', 'of', 'the', 'piece', 'of', 'fruit', 'in', 'the', 'background', ',', 'very', 'interesting', 'because', 'most', 'of', 'the', 'colours', 'to', 'me', 'seem', 'a', ',', 'a', 'lot', 'less', 'vibrant', 'then', 'many', 'of', 'his', 'other', 'paintings', ',', 'and', 'so', 'they', ',', 'they', 'attracted', 'me', 'and', 'have', 'a', ',', 'a', 'strong', 'sense', 'of', 'er', '##m', ',', 'er', 'a', 'hidden', 'desire', 'in', 'that', 'and', 'so', 'it', ',', 'to', 'use', 'it', 'as', 'a', 'writing', 'exercise', 'which', 'i', 'intend', 'doing', ',', 'it', 'will', 'be', 'able', 'about', 'a', 'situation', 'of', 'character', \"'\", 's', 'with', 'er', '##m', 'something', 'that', \"'\", 's', 'hidden', ',', 'some', 'desire', ',', 'i', 'do', 'n', \"'\", 't', ',', 'the', ',', 'not', 'even', 'spoke', 'about', 'to', 'themselves', 'or', ',', 'or', 'generally', ',', 'er', '##m', 'i', 'like', 'to', 'sort', 'of', 'say', 'that', 'came', 'from', 'those', 'two', 'little', ',', 'just', 'this', 'amounts', 'of', 'colour', 'which', 'seemed', 'to', 'be', 'saying', 'such', 'a', 'lot', '[SEP]']\n",
      "processed 400 words\n",
      "calculating clusters for knife\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (774 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'jesus', 'had', 'had', 'many', 'interviews', 'with', 'people', ',', 'we', \"'\", 've', 'looked', 'at', 'some', 'of', 'them', 'over', 'these', 'past', 'few', 'weeks', ',', 'the', 'time', 'when', 'he', 'met', 'with', 'nic', '##ade', '##mus', ',', 'the', 'religious', 'leader', ',', 'the', 'time', 'he', 'went', 'out', 'of', 'his', 'way', 'to', 'meet', 'with', 'a', 'woman', 'of', 'se', '##mar', '##ia', 'in', 'her', 'dyer', 'need', ',', 'the', 'other', 'occasion', 'that', 'we', 'looked', 'at', 'er', 'a', 'week', 'or', 'so', 'back', 'when', 'he', 'called', 'an', '##zaki', '##as', 'from', 'that', 'tree', 'of', 'which', 'he', 'was', 'hiding', ',', 'last', 'week', 'his', 'judge', ',', 'pilot', ',', 'but', 'of', 'all', 'those', 'interviews', 'and', 'as', 'many', 'others', 'that', 'we', 'have', 'n', \"'\", 't', 'looked', 'at', 'this', 'surely', 'must', 'be', 'one', 'of', 'the', 'strange', '##st', 'as', 'jesus', 'himself', 'is', 'in', 'the', 'process', 'of', 'dying', 'and', 'as', 'he', 'is', 'dying', 'he', 'is', 'confronted', 'with', 'another', 'person', 'who', 'has', 'a', 'need', ',', 'but', 'jesus', 'your', 'need', 'is', 'as', 'greatest', 'as', 'any', 'body', 'else', '##s', ',', 'your', 'pain', ',', 'your', 'suffering', ',', 'your', 'physical', 'suffering', 'was', 'every', 'bit', 'of', 'great', 'as', 'those', 'around', 'you', ',', 'why', 'be', 'bothered', 'with', 'others', 'is', 'n', \"'\", 't', 'that', 'so', 'often', 'our', 'story', ',', 'when', 'we', 'are', 'in', 'need', 'we', 'can', 'forget', 'all', 'about', 'other', 'people', ',', 'it', 'does', 'n', \"'\", 't', 'matter', 'there', 'need', ',', 'its', 'poor', 'me', ',', 'what', 'about', 'me', ',', 'what', 'about', 'my', 'need', ',', 'what', 'about', 'my', 'requirements', ',', 'what', 'about', 'my', 'suffering', ',', 'but', 'we', 'see', 'here', 'how', 'jesus', 'apart', 'from', 'any', 'thing', 'else', 'deals', 'with', 'his', 'own', 'suffering', ',', 'he', 'deals', 'with', 'it', 'by', 'minister', '##ing', 'to', 'the', 'needs', 'of', 'other', 'people', ',', 'and', 'this', 'surely', 'then', 'must', 'be', 'one', 'of', 'the', 'most', 'strange', 'and', 'one', 'of', 'the', 'most', 'interviews', 'that', 'our', 'lord', 'ever', 'had', 'when', 'he', 'was', 'here', 'on', 'earth', ',', 'with', 'this', 'dying', 'thief', ',', 'but', 'he', 'was', 'more', 'than', 'a', 'thief', 'he', 'was', 'a', 'er', ',', 'he', 'was', 'a', 're', 'a', 'rebel', ',', 'he', 'was', 'a', 'terrorist', 'or', 'a', 'freedom', 'fighter', 'depending', 'on', 'which', 'way', 'you', 'wanted', 'to', 'look', 'at', 'it', 'and', 'he', 'was', 'dying', 'for', 'his', 'crimes', 'and', 'he', 'was', 'n', \"'\", 't', 'alone', 'because', 'there', 'there', 'was', 'this', 'man', 'we', \"'\", 've', 'been', 'talking', 'about', ',', 'there', 'was', 'jesus', 'and', 'there', 'was', 'another', 'one', ',', 'another', 'criminal', 'on', 'the', 'other', 'side', 'and', 'we', 'find', 'that', 'this', 'is', 'all', 'in', 'keeping', 'with', 'what', 'god', 'had', 'promised', ',', 'all', 'there', 'in', ',', 'in', 'line', 'with', 'his', 'prophecy', 'way', 'back', 'in', 'i', '##zia', '##h', 'chapter', 'fifty', 'three', ',', 'it', 'tells', 'us', 'that', 'he', 'was', 'numbered', 'with', 'the', 'trans', '##gre', '##sso', '##rs', ',', 'that', 'he', 'died', 'with', 'sin', '##ful', 'men', 'with', ',', 'with', 'law', 'breakers', 'and', 'here', 'it', 'is', 'its', 'happening', 'right', 'in', 'front', 'of', 'the', ',', 'the', 'very', 'eyes', 'of', 'the', 'jewish', 'leaders', 'and', 'the', 'jewish', 'authorities', 'our', 'lords', 'intention', 'in', 'coming', 'into', 'the', 'world', 'was', 'to', 'save', 'men', 'and', 'women', ',', 'to', 'seek', 'out', 'and', 'to', 'save', 'sinn', '##ers', ',', 'remember', 'thirty', 'odd', 'years', 'previous', 'to', 'this', 'event', 'the', 'word', 'had', 'come', ',', 'for', 'mary', 'his', 'mother', ',', 'to', 'joseph', ',', 'we', 'will', 'call', 'his', 'name', 'jesus', 'because', 'he', 'will', 'save', 'his', 'people', 'from', 'their', 'sins', 'and', 'later', 'on', 'writing', 'to', 'timothy', 'the', 'apostle', 'paul', 'in', 'the', 'first', 'chapter', 'of', 'the', 'first', 'book', 'in', 'verse', 'fifteen', 'he', 'says', 'it', 'is', 'a', 'trust', 'worthy', 'statement', 'des', '##erving', 'full', 'acceptance', 'that', 'christ', 'jesus', 'came', 'into', 'the', 'world', 'to', 'save', 'sinn', '##ers', ',', 'this', 'was', 'his', 'purpose', ',', 'this', 'was', 'his', 'reason', 'for', 'coming', 'into', 'the', 'world', ',', 'not', 'to', 'be', 'a', 'good', 'man', ',', 'not', 'to', 'be', 'a', ',', 'a', 'great', 'leader', ',', 'not', 'to', 'give', 'us', 'some', 'model', 'that', 'we', 'can', ',', 'you', 'know', ',', 'that', 'we', 'can', 'plan', 'our', 'life', 'out', 'and', 'try', 'and', 'live', 'up', 'to', 'his', 'standards', ',', 'he', 'says', 'i', \"'\", 've', 'come', 'to', 'give', 'my', 'life', 'as', 'a', 'ransom', ',', 'i', 'have', 'come', 'to', 'save', 'and', 'to', 'seek', 'that', 'which', 'was', 'lost', 'and', 'here', 'in', 'this', 'incident', 'as', 'he', 'himself', 'is', 'dying', 'and', 'is', 'in', 'physical', 'pain', 'and', 'torment', 'he', 'is', 'carrying', 'out', 'this', 'very', 'work', ',', 'of', 'seeking', 'out', 'and', 'saving', 'of', 'those', 'who', 'will', 'turn', 'to', 'him', ',', 'those', 'who', 'will', 'put', 'their', 'trust', 'in', 'him', ',', 'he', 'is', 'saving', 'the', 'lost', ',', 'and', 'we', 'see', 'in', 'a', 'wonderful', 'how', 'great', 'the', 'compassion', 'of', 'jesus', 'was', 'and', 'is', ',', 'in', 'reaching', 'out', 'and', 'rescuing', 'those', 'who', 'are', 'lost', ',', 'here', 'we', 'see', 'our', 'lord', 'suffering', 'the', 'most', 'terrible', 'agony', 'and', 'yet', 'in', 'the', 'midst', 'of', 'his', 'own', 'sorrow', 'and', 'pain', 'and', ',', 'and', 'torment', 'he', 'thinks', 'of', 'this', 'dying', 'thief', 'and', 'extends', 'his', 'grace', 'and', 'mercy', 'to', 'him', '.', '[SEP]']\n",
      "processed 500 words\n",
      "calculating clusters for attempt\n",
      "processed 600 words\n",
      "calculating clusters for demon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'over', 'these', 'er', '##m', 'past', 'couple', 'of', 'weeks', 'we', \"'\", 've', 'been', 'looking', 'at', 'er', 'some', 'of', 'the', 'questions', 'in', 'the', 'new', 'testament', ',', 'we', 'thought', 'a', 'couple', 'of', 'weeks', 'back', 'of', 'the', 'question', 'that', 'jesus', 'asked', 'his', 'disciples', ',', 'do', 'you', 'think', 'i', \"'\", 'm', 'able', 'to', 'do', 'this', 'and', 'then', 'last', 'week', 'we', 'looked', 'at', 'a', 'question', 'that', 'the', 'disciples', 'put', 'to', 'jesus', ',', 'that', 'time', 'when', 'they', 'came', 'down', 'from', 'the', 'mountain', 'and', 'they', 'found', 'the', 're', ',', 'three', 'of', 'them', 'came', 'down', 'with', 'jesus', 'from', 'the', 'mountain', 'of', 'trans', '##fi', '##gur', '##ation', 'and', 'they', 'found', 'the', 'other', 'disciples', 'with', 'a', 'man', 'who', 'and', 'a', ',', 'whose', 'son', 'was', 'demon', 'possessed', 'and', 'er', 'they', 'had', 'been', 'unable', 'to', 'help', 'him', 'and', 'the', 'man', 'or', 'brings', 'his', 'son', 'to', 'jesus', 'and', 'jesus', 'delivers', 'him', 'and', 'afterwards', 'the', 'disciples', 'who', 'had', 'been', 'so', 'helpless', 'put', 'the', 'question', 'to', 'jesus', ',', 'why', 'could', 'we', 'not', 'cast', 'out', 'this', 'demon', 'and', 'this', 'morning', 'i', \"'\", 'd', 'like', 'us', 'to', 'look', 'at', 'another', 'question', ',', 'we', \"'\", 've', 'got', 'another', 'one', 'today', 'and', 'one', 'god', 'willing', 'next', 'week', ',', 'er', 'and', 'the', 'question', 'is', ',', 'is', 'found', 'in', 'luke', 'chapter', 'thirteen', ',', 'let', 'me', 'just', 'read', 'a', 'few', 'verses', ',', 'because', 'of', 'course', 'it', \"'\", 's', ',', 'it', \"'\", 's', 'not', 'just', 'the', 'questions', ',', 'it', \"'\", 's', 'the', 'answers', 'that', 'are', 'important', 'as', 'well', 'in', 'luke', 'chapter', 'thirteen', ',', 'go', '##n', 'na', 'read', 'from', 'verse', 'twenty', 'two', 'it', 'says', 'in', 'jesus', 'was', 'passing', 'through', 'from', 'one', 'city', 'and', 'village', 'to', 'another', ',', 'teaching', 'and', 'proceeding', 'on', 'his', 'way', 'to', 'jerusalem', 'now', 'that', 'gives', 'us', 'a', 'clue', 'in', 'that', ',', 'because', 'jesus', 'only', 'ever', 'went', 'to', 'jerusalem', 'apart', 'from', 'when', 'he', 'was', 'a', 'boy', ',', 'he', 'only', 'ever', 'went', 'to', 'jerusalem', 'once', 'and', 'that', ',', 'after', 'since', 'that', 'time', ',', 'and', 'that', 'was', 'when', 'he', 'was', 'cr', '##uc', '##ified', ',', 'so', 'jesus', 'was', 'now', 'on', 'his', 'way', 'to', 'jerusalem', ',', 'it', 'was', 'the', 'latter', 'days', ',', 'the', 'latter', 'weeks', 'of', 'the', 'life', 'of', 'jesus', ',', 'he', 'was', 'making', 'his', 'way', 'now', 'to', 'jerusalem', 'and', 'someone', 'said', 'to', 'him', 'lord', 'are', 'there', 'just', 'a', 'few', 'who', 'are', 'being', 'saved', 'and', 'jesus', 'said', 'to', 'hi', ',', 'to', 'them', ',', 'strive', 'to', 'enter', 'by', 'the', 'narrow', 'door', 'for', 'many', 'i', 'tell', 'you', 'will', 'seek', 'to', 'enter', 'and', 'will', 'not', 'be', 'able', ',', 'once', 'the', 'head', 'of', 'the', 'house', 'gets', 'up', 'and', 'shut', '##s', 'the', 'door', 'and', 'you', 'begin', 'to', 'stand', 'outside', 'and', 'knock', 'on', 'the', 'door', 'saying', 'lord', 'open', 'to', 'us', ',', 'then', 'he', 'will', 'answer', 'and', 'say', 'to', 'you', 'i', 'do', 'not', 'know', 'where', 'you', 'are', 'from', ',', 'then', 'you', 'will', 'begin', 'to', 'say', 'we', 'ate', 'and', 'drank', 'in', 'your', 'presence', 'and', 'you', 'taught', 'in', 'our', 'streets', ',', 'and', 'he', 'will', 'say', 'i', 'tell', 'you', 'i', 'do', 'not', 'know', 'where', 'you', 'are', 'from', ',', 'depart', 'from', 'me', 'all', 'you', 'evil', 'doe', '##rs', ',', 'there', 'will', 'be', 'weeping', 'and', 'g', '##nas', '##hing', 'of', 'teeth', 'there', ',', 'when', 'you', 'see', 'abraham', 'and', 'isaac', 'and', 'jacob', 'and', 'all', 'the', 'profits', 'in', 'the', 'kingdom', 'of', 'god', ',', 'but', 'yourselves', 'being', 'cast', 'out', 'and', 'they', 'will', 'come', 'from', 'east', 'and', 'west', 'and', 'from', 'north', 'and', 'south', 'and', 'will', 'rec', '##line', 'at', 'the', 'table', 'in', 'the', 'kingdom', 'of', 'god', ',', 'and', 'behold', 'some', 'ar', 'some', 'are', 'last', 'who', 'will', 'be', 'first', 'and', 'some', 'are', 'first', 'who', 'will', 'be', 'last', ',', 'so', 'it', \"'\", 's', 'just', 'that', 'question', 'then', ',', 'let', \"'\", 's', 'remind', 'ourselves', 'that', 'is', 'put', 'to', 'jesus', 'lord', 'are', 'there', 'just', 'a', 'few', 'who', 'are', 'being', 'saved', '[SEP]']\n",
      "processed 700 words\n",
      "calculating clusters for woodland\n",
      "processed 800 words\n",
      "calculating clusters for know\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1577 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'as', 'i', 'mentioned', 'early', 'the', ',', 'the', 'city', 'of', 'ser', '##mar', '##ia', 'it', 'was', 'under', 'siege', 'and', 'the', 'army', 'of', 'ser', '##ia', 'was', 'en', '##camp', '##ed', 'all', 'around', 'it', ',', 'ben', 'had', '##ad', 'was', 'a', 'great', 'warrior', ',', 'he', 'would', 'of', 'been', 'the', ',', 'the', 'alexander', 'or', 'the', 'napoleon', 'of', 'his', 'day', 'and', 'he', 'had', 'set', 'up', 'this', 'en', '##camp', '##ment', 'around', 'the', 'city', 'of', 'ser', '##mar', '##ia', ',', 'nobody', 'could', 'get', 'in', ',', 'nobody', 'could', 'get', 'out', 'and', 'very', 'quickly', 'the', 'stocks', 'of', 'food', 'and', 'water', 'er', 'were', 'used', 'up', ',', 'ratio', '##ning', 'would', 'of', 'been', 'introduced', 'but', 'it', 'only', 'lasted', 'for', 'a', 'certain', 'period', ',', 'they', \"'\", 'd', 'got', 'to', 'the', 'stage', 'it', 'tells', 'us', 'in', 'the', 'previous', 'chapter', 'that', 'er', ',', 'that', 'a', 'donkey', '##s', 'head', 'was', 'sold', 'for', 'eighty', 'she', '##kel', \"'\", 's', 'of', 'silver', 'and', 'some', 'folk', 'had', 'even', 'got', 'to', 'the', ',', 'had', 'sunk', 'to', 'the', 'level', 'of', 'can', '##ni', '##bal', '##ism', ',', 'of', 'eating', 'their', 'own', 'children', 'and', 'the', 'city', 'was', ',', 'when', 'they', 'heard', 'about', 'this', 'they', 'were', 'in', 'an', 'up', '##ro', '##ar', 'and', 'they', 'started', 'blaming', 'god', 'and', 'in', 'between', 'the', 'city', 'of', 'ser', '##mar', '##ia', 'of', 'all', 'its', 'suffering', 'and', 'hopeless', '##ness', 'and', 'helpless', '##ness', 'and', 'the', 'army', 'en', '##camp', '##ed', 'about', 'with', 'all', 'of', 'their', 'supplies', ',', 'there', 'was', 'this', 'area', 'of', 'no', 'mans', 'land', 'in', 'which', 'they', 'were', 'caught', 'up', 'four', 'men', 'who', 'were', 'leap', '##ers', 'and', 'they', 'were', 'trapped', 'there', ',', 'they', 'did', 'n', \"'\", 't', 'want', 'to', 'go', 'over', 'to', 'the', 'ser', '##ians', 'because', 'they', \"'\", 'd', 'be', 'killed', ',', 'they', 'did', 'n', \"'\", 't', 'want', 'to', 'go', 'back', 'into', 'the', 'city', 'because', 'they', 'were', 'n', \"'\", 't', 'allowed', 'there', 'and', 'any', 'way', 'what', 'was', 'the', 'point', ',', 'they', \"'\", 'd', 'only', 'die', 'of', 'starvation', 'in', 'there', 'and', 'so', 'these', 'four', 'men', 'are', 'caught', 'up', 'in', 'no', 'man', \"'\", 's', 'land', 'and', 'yet', 'their', 'no', 'better', 'off', 'than', 'people', 'in', 'the', 'city', ',', 'now', 'god', 'had', 'promised', 'deliver', '##ance', ',', 'through', 'his', 'serve', 'and', 'eli', '##ger', 'he', 'had', 'promised', 'deliver', '##ance', ',', 'eli', '##ger', 'said', 'tomorrow', 'about', 'this', 'time', 'a', 'measure', 'of', 'fine', 'flour', 'shall', 'be', 'sold', 'for', 'a', 'se', 'she', '##kel', 'and', 'two', 'measures', 'of', 'barley', 'for', 'a', 'she', '##kel', 'in', 'the', 'gate', 'of', 'se', '##mar', '##ia', ',', 'he', 'said', 'the', 'gates', 'are', 'go', '##n', 'na', 'be', 'open', ',', 'there', \"'\", 's', 'go', '##n', 'na', 'be', 'food', 'and', 'its', 'go', '##n', 'na', 'be', 'a', 'reasonable', 'price', 'and', 'it', 'says', 'the', 'royal', 'officer', 'who', \"'\", 's', 'hand', 'the', 'king', 'was', 'leaning', 'on', 'said', 'the', 'man', 'of', 'god', 'said', 'behold', ',', 'if', 'the', 'lord', 'shall', 'make', 'windows', 'in', 'heaven', 'could', 'such', 'a', 'thing', 'be', ',', 'he', 'said', 'do', 'n', \"'\", 't', 'talk', 'stupid', 'man', ',', 'how', 'can', 'such', 'a', 'thing', 'happen', 'for', 'us', '?', ',', 'he', 'did', 'n', \"'\", 't', 'believe', 'what', 'god', 'servant', 'said', 'and', 'eli', '##ger', 'brings', 'out', 'to', 'him', 'a', 'terrible', 'judgment', ',', 'he', 'says', 'because', 'of', 'your', 'un', '##bel', '##ie', '##f', 'you', 'will', 'see', 'it', ',', 'but', 'your', 'not', 'participate', 'in', 'it', 'but', 'lets', 'look', 'at', 'these', 'four', 'men', 'for', 'a', 'moment', ',', 'co', '##s', 'that', \"'\", 's', 'where', 'our', 'real', 'interest', 'lies', 'this', 'morning', ',', 'i', 'just', 'wanted', 'to', 'say', 'three', 'things', 'in', 'their', 'experience', ',', 'the', 'first', 'things', 'is', 'that', 'they', 'were', 'amazed', 'that', ',', 'at', 'what', 'they', 'found', ',', 'because', 'after', 'they', 'come', 'together', 'and', 'they', 'talk', 'about', 'it', 'and', 'they', 'said', 'well', 'what', 'shall', 'we', 'do', 'and', 'they', 'weighed', 'the', 'pro', \"'\", 's', 'and', 'the', 'con', '##s', 'and', 'se', '##mar', '##ia', 'does', 'n', \"'\", 't', 'look', 'very', 'attractive', 'with', 'its', 'can', '##ni', '##bal', '##ism', ',', 'they', 'said', 'well', 'the', 'least', 'if', 'we', 'stay', 'here', 'were', 'go', '##n', 'na', 'die', ',', 'if', 'we', 'go', 'into', 'se', '##mar', '##ia', 'we', \"'\", 'll', 'die', ',', 'lets', 'go', 'down', 'to', 'the', 'ser', '##ein', 'camp', ',', 'the', 'worse', 'they', 'can', 'do', 'to', 'us', 'is', 'put', 'us', 'to', 'death', 'and', 'were', 'dying', 'men', 'any', 'way', ',', 'but', 'they', 'may', 'just', 'take', 'pity', 'on', 'us', ',', 'we', 'maybe', 'allowed', 'to', 'gr', '##ope', 'around', 'in', 'their', 'dust', '##bin', '##s', 'and', 'get', 'some', 'scrap', '##s', 'of', 'food', ',', 'they', 'may', 'at', 'least', 'allow', 'us', 'that', ',', 'and', 'so', 'they', 'make', 'their', 'way', 'down', 'just', 'as', 'evening', 'is', 'falling', ',', 'they', 'make', 'their', 'way', 'down', 'to', 'the', 'ser', '##ein', 'lines', 'and', 'when', 'they', 'get', 'there', ',', 'they', 'are', 'amazed', 'at', 'what', 'they', 'find', ',', 'you', 'see', 'their', 'condition', 'was', 'helpless', 'and', 'hopeless', ',', 'they', 'were', 'dying', 'men', 'any', 'way', ',', 'they', 'were', 'le', '##pers', ',', 'but', 'they', 'were', 'dying', 'of', 'starvation', ',', 'that', 'was', 'far', 'more', 'imminent', 'than', 'their', 'le', '##pro', '##sy', ',', 'their', 'problems', 'and', 'their', 'needs', 'were', 'greater', 'than', 'themselves', ',', 'they', 'could', 'not', 'meet', 'their', 'own', 'needs', ',', 'their', 'problems', 'and', 'their', 'needs', 'were', 'greater', 'than', 'their', 'government', ',', 'the', 'king', 'in', 'se', '##mar', '##ia', 'and', 'all', 'of', 'his', 'court', 'could', 'not', 'meet', 'the', 'needs', 'of', 'his', 'people', 'and', 'then', 'in', 'verse', 'five', ',', 'we', 'read', 'something', 'there', ',', 'they', 'arose', 'at', 'twilight', 'to', 'go', 'to', 'the', 'camp', 'of', 'ara', '##mian', '##s', 'or', 'the', 'ser', '##ein', \"'\", 's', 'and', 'when', 'they', 'came', 'to', 'the', 'outskirts', 'of', 'the', 'camp', 'of', 'the', 'ser', '##ein', \"'\", 's', 'behold', 'there', 'was', 'no', 'one', 'there', ',', 'they', 'expected', 'to', 'at', 'least', 'meet', 'a', 'guard', ',', 'there', 'would', 'surely', 'be', 'somebody', 'on', 'sent', '##ry', 'duty', 'even', 'if', 'the', 'rest', 'of', 'the', 'soldiers', 'had', 'gone', 'in', 'to', 'their', 'tents', 'and', 'were', 'perhaps', 'getting', 'ready', 'for', 'their', ',', 'for', 'the', 'evening', ',', 'going', 'to', 'bed', 'or', 'whatever', 'they', 'were', 'go', '##n', 'na', 'be', 'doing', ',', 'having', 'their', 'evening', 'meal', ',', 'there', 'would', 'at', 'least', 'be', 'somebody', 'on', 'guard', 'duty', ',', 'but', 'when', 'they', 'got', 'there', ',', 'there', 'was', 'no', 'one', 'there', ',', 'god', 'had', 'stepped', 'in', ',', 'god', 'had', 'intervened', 'and', 'the', 'good', 'news', 'of', 'the', 'christian', 'gospel', 'is', 'that', 'god', 'has', 'intervened', 'in', 'our', ',', 'in', 'the', 'midst', 'of', 'our', 'helpless', '##ness', ',', 'in', 'the', 'midst', 'of', 'our', 'hopeless', '##ness', ',', 'god', 'has', 'intervened', ',', 'he', 'had', 'stepped', 'in', 'to', 'history', ',', 'so', 'often', 'you', \"'\", 'll', 'hear', 'folks', 'say', ',', 'well', 'why', 'does', 'n', \"'\", 't', 'god', 'do', 'something', ',', 'why', 'does', 'god', 'allow', 'this', 'to', 'happen', ',', 'why', 'does', 'god', 'allow', 'that', 'one', ',', 'why', 'does', 'n', \"'\", 't', 'he', 'do', 'something', 'all', 'they', 'really', 'show', 'by', 'that', 'comment', 'is', 'their', 'own', 'ignorance', ',', 'because', 'god', 'has', 'done', 'something', ',', 'god', 'has', 'intervened', ',', 'listen', 'to', 'what', 'it', 'says', 'in', 'john', 'three', 'sixteen', ',', 'for', 'god', 'so', 'loved', 'the', 'world', 'that', 'he', 'gave', ',', 'he', \"'\", 's', 'only', 'son', 'and', 'the', 'er', ',', 'the', 'er', 'apostle', 'paul', 'and', 'he', \"'\", 's', 'writing', 'to', 'the', 'gall', '##ations', ',', 'in', 'chapter', 'four', 'and', 'in', 'verses', 'four', 'and', 'five', 'hear', 'what', 'he', 'says', 'there', ',', 'but', 'when', 'the', 'time', 'had', 'fully', 'come', 'god', 'sent', 'his', 'son', ',', 'born', 'of', 'a', 'woman', ',', 'born', 'under', 'law', 'to', 'red', '##eem', 'those', 'under', 'law', 'that', 'we', 'might', 'receive', 'the', 'full', 'rights', 'of', 'son', ',', 'er', 'of', 'sons', ',', 'god', 'has', 'done', 'something', ',', 'he', \"'\", 's', 'sent', 'his', 'son', 'jesus', 'christ', 'into', 'this', 'world', 'in', 'fact', 'his', 'done', 'the', 'greatest', 'thing', 'he', 'could', 'do', ',', 'he', 'has', 'done', 'the', 'very', 'ultimate', 'thing', ',', 'he', 'has', 'sent', 'his', 'son', 'into', 'the', 'world', 'that', \"'\", 's', 'the', 'greatest', 'intervention', 'god', 'could', 'ever', 'have', 'made', ',', 'it', 'was', 'far', 'greater', 'than', ',', 'than', 'just', 'intervening', 'in', 'sm', ',', 'in', 'some', 'small', 'local', 'event', ',', 'were', 'you', 'see', 'some', 'catastrophe', 'happening', 'and', 'you', 'say', 'well', 'why', 'does', 'n', \"'\", 't', 'god', 'do', 'something', 'there', ',', 'or', 'there', \"'\", 's', 'a', 'war', 'situation', 'going', 'on', 'in', 'some', 'other', 'part', 'of', 'the', 'world', ',', 'well', 'why', 'does', 'n', \"'\", 't', 'god', 'step', 'in', 'and', 'stop', 'it', ',', 'god', 'has', 'stepped', 'in', ',', 'not', 'in', 'a', 'local', 'situation', ',', 'not', 'in', 'some', 'er', 'passing', 'problem', 'or', 'need', 'but', 'he', \"'\", 's', 'stepped', 'into', 'the', 'greatest', 'way', 'possible', 'by', 'sending', 'his', 'son', 'jesus', 'christ', 'into', 'the', 'world', 'to', 'dye', 'for', 'men', 'and', 'woman', ',', 'to', 'take', 'away', 'sin', ',', 'to', 'pay', 'the', 'price', 'that', 'god', \"'\", 's', 'righteous', '##ness', 'demands', 'for', 'sin', 'so', 'god', 'has', 'intervened', 'and', 'his', 'intervention', 'has', 'changed', 'the', 'whole', 'situation', ',', 'its', 'brought', 'a', 'whole', 'new', 'complexion', 'on', 'things', ',', 'its', 'changed', 'the', 'colour', 'completely', ',', 'no', 'longer', 'is', 'the', 'world', 'now', 'under', 'darkness', 'and', 'in', ',', 'and', 'in', 'pending', 'judgment', 'in', 'doom', ',', 'because', 'jesus', 'christ', 'came', 'and', 'he', 'took', 'that', 'judgment', 'and', 'that', ',', 'that', 'condemnation', 'upon', 'himself', ',', 'he', 'said', 'i', \"'\", 've', 'not', 'come', 'to', 'condemn', 'the', 'world', 'he', 'said', 'its', 'already', 'condemned', ',', 'its', 'already', 'under', 'judgement', ',', 'the', 'sword', 'of', 'dam', '##oc', '##les', 'is', 'already', 'hanging', 'over', 'the', 'world', 'and', 'jesus', 'christ', 'came', 'in', 'and', 'to', 'take', 'that', 'judgment', 'and', 'that', 'condemnation', 'on', 'himself', 'and', 'when', 'he', 'died', 'there', 'on', 'the', 'cross', 'and', 'rose', 'again', ',', 'there', 'came', 'that', 'burst', 'of', 'light', 'in', 'a', 'world', 'that', 'had', 'been', 'sh', '##roud', '##ed', 'in', 'blackness', 'and', 'darkness', ',', 'a', 'world', 'that', 'had', 'been', 'sh', '##roud', '##ed', 'in', 'sin', 'suddenly', 'for', 'the', 'first', 'time', 'sees', 'the', 'light', ',', 'god', 'has', 'paid', 'for', 'himself', 'the', 'price', 'of', 'sin', ',', 'god', 'has', 'intervened', 'and', 'changed', 'the', 'whole', 'situation', 'and', 'the', 'message', 'of', 'the', 'gospel', 'is', 'that', 'if', 'you', 'and', 'i', 'allow', 'that', 'intervention', 'to', 'effect', 'us', 'personally', ',', 'then', 'like', 'those', 'four', 'men', 'surely', 'we', 'too', 'are', 'amazed', 'at', 'what', 'we', \"'\", 've', 'found', '.', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (774 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'jesus', 'had', 'had', 'many', 'interviews', 'with', 'people', ',', 'we', \"'\", 've', 'looked', 'at', 'some', 'of', 'them', 'over', 'these', 'past', 'few', 'weeks', ',', 'the', 'time', 'when', 'he', 'met', 'with', 'nic', '##ade', '##mus', ',', 'the', 'religious', 'leader', ',', 'the', 'time', 'he', 'went', 'out', 'of', 'his', 'way', 'to', 'meet', 'with', 'a', 'woman', 'of', 'se', '##mar', '##ia', 'in', 'her', 'dyer', 'need', ',', 'the', 'other', 'occasion', 'that', 'we', 'looked', 'at', 'er', 'a', 'week', 'or', 'so', 'back', 'when', 'he', 'called', 'an', '##zaki', '##as', 'from', 'that', 'tree', 'of', 'which', 'he', 'was', 'hiding', ',', 'last', 'week', 'his', 'judge', ',', 'pilot', ',', 'but', 'of', 'all', 'those', 'interviews', 'and', 'as', 'many', 'others', 'that', 'we', 'have', 'n', \"'\", 't', 'looked', 'at', 'this', 'surely', 'must', 'be', 'one', 'of', 'the', 'strange', '##st', 'as', 'jesus', 'himself', 'is', 'in', 'the', 'process', 'of', 'dying', 'and', 'as', 'he', 'is', 'dying', 'he', 'is', 'confronted', 'with', 'another', 'person', 'who', 'has', 'a', 'need', ',', 'but', 'jesus', 'your', 'need', 'is', 'as', 'greatest', 'as', 'any', 'body', 'else', '##s', ',', 'your', 'pain', ',', 'your', 'suffering', ',', 'your', 'physical', 'suffering', 'was', 'every', 'bit', 'of', 'great', 'as', 'those', 'around', 'you', ',', 'why', 'be', 'bothered', 'with', 'others', 'is', 'n', \"'\", 't', 'that', 'so', 'often', 'our', 'story', ',', 'when', 'we', 'are', 'in', 'need', 'we', 'can', 'forget', 'all', 'about', 'other', 'people', ',', 'it', 'does', 'n', \"'\", 't', 'matter', 'there', 'need', ',', 'its', 'poor', 'me', ',', 'what', 'about', 'me', ',', 'what', 'about', 'my', 'need', ',', 'what', 'about', 'my', 'requirements', ',', 'what', 'about', 'my', 'suffering', ',', 'but', 'we', 'see', 'here', 'how', 'jesus', 'apart', 'from', 'any', 'thing', 'else', 'deals', 'with', 'his', 'own', 'suffering', ',', 'he', 'deals', 'with', 'it', 'by', 'minister', '##ing', 'to', 'the', 'needs', 'of', 'other', 'people', ',', 'and', 'this', 'surely', 'then', 'must', 'be', 'one', 'of', 'the', 'most', 'strange', 'and', 'one', 'of', 'the', 'most', 'interviews', 'that', 'our', 'lord', 'ever', 'had', 'when', 'he', 'was', 'here', 'on', 'earth', ',', 'with', 'this', 'dying', 'thief', ',', 'but', 'he', 'was', 'more', 'than', 'a', 'thief', 'he', 'was', 'a', 'er', ',', 'he', 'was', 'a', 're', 'a', 'rebel', ',', 'he', 'was', 'a', 'terrorist', 'or', 'a', 'freedom', 'fighter', 'depending', 'on', 'which', 'way', 'you', 'wanted', 'to', 'look', 'at', 'it', 'and', 'he', 'was', 'dying', 'for', 'his', 'crimes', 'and', 'he', 'was', 'n', \"'\", 't', 'alone', 'because', 'there', 'there', 'was', 'this', 'man', 'we', \"'\", 've', 'been', 'talking', 'about', ',', 'there', 'was', 'jesus', 'and', 'there', 'was', 'another', 'one', ',', 'another', 'criminal', 'on', 'the', 'other', 'side', 'and', 'we', 'find', 'that', 'this', 'is', 'all', 'in', 'keeping', 'with', 'what', 'god', 'had', 'promised', ',', 'all', 'there', 'in', ',', 'in', 'line', 'with', 'his', 'prophecy', 'way', 'back', 'in', 'i', '##zia', '##h', 'chapter', 'fifty', 'three', ',', 'it', 'tells', 'us', 'that', 'he', 'was', 'numbered', 'with', 'the', 'trans', '##gre', '##sso', '##rs', ',', 'that', 'he', 'died', 'with', 'sin', '##ful', 'men', 'with', ',', 'with', 'law', 'breakers', 'and', 'here', 'it', 'is', 'its', 'happening', 'right', 'in', 'front', 'of', 'the', ',', 'the', 'very', 'eyes', 'of', 'the', 'jewish', 'leaders', 'and', 'the', 'jewish', 'authorities', 'our', 'lords', 'intention', 'in', 'coming', 'into', 'the', 'world', 'was', 'to', 'save', 'men', 'and', 'women', ',', 'to', 'seek', 'out', 'and', 'to', 'save', 'sinn', '##ers', ',', 'remember', 'thirty', 'odd', 'years', 'previous', 'to', 'this', 'event', 'the', 'word', 'had', 'come', ',', 'for', 'mary', 'his', 'mother', ',', 'to', 'joseph', ',', 'we', 'will', 'call', 'his', 'name', 'jesus', 'because', 'he', 'will', 'save', 'his', 'people', 'from', 'their', 'sins', 'and', 'later', 'on', 'writing', 'to', 'timothy', 'the', 'apostle', 'paul', 'in', 'the', 'first', 'chapter', 'of', 'the', 'first', 'book', 'in', 'verse', 'fifteen', 'he', 'says', 'it', 'is', 'a', 'trust', 'worthy', 'statement', 'des', '##erving', 'full', 'acceptance', 'that', 'christ', 'jesus', 'came', 'into', 'the', 'world', 'to', 'save', 'sinn', '##ers', ',', 'this', 'was', 'his', 'purpose', ',', 'this', 'was', 'his', 'reason', 'for', 'coming', 'into', 'the', 'world', ',', 'not', 'to', 'be', 'a', 'good', 'man', ',', 'not', 'to', 'be', 'a', ',', 'a', 'great', 'leader', ',', 'not', 'to', 'give', 'us', 'some', 'model', 'that', 'we', 'can', ',', 'you', 'know', ',', 'that', 'we', 'can', 'plan', 'our', 'life', 'out', 'and', 'try', 'and', 'live', 'up', 'to', 'his', 'standards', ',', 'he', 'says', 'i', \"'\", 've', 'come', 'to', 'give', 'my', 'life', 'as', 'a', 'ransom', ',', 'i', 'have', 'come', 'to', 'save', 'and', 'to', 'seek', 'that', 'which', 'was', 'lost', 'and', 'here', 'in', 'this', 'incident', 'as', 'he', 'himself', 'is', 'dying', 'and', 'is', 'in', 'physical', 'pain', 'and', 'torment', 'he', 'is', 'carrying', 'out', 'this', 'very', 'work', ',', 'of', 'seeking', 'out', 'and', 'saving', 'of', 'those', 'who', 'will', 'turn', 'to', 'him', ',', 'those', 'who', 'will', 'put', 'their', 'trust', 'in', 'him', ',', 'he', 'is', 'saving', 'the', 'lost', ',', 'and', 'we', 'see', 'in', 'a', 'wonderful', 'how', 'great', 'the', 'compassion', 'of', 'jesus', 'was', 'and', 'is', ',', 'in', 'reaching', 'out', 'and', 'rescuing', 'those', 'who', 'are', 'lost', ',', 'here', 'we', 'see', 'our', 'lord', 'suffering', 'the', 'most', 'terrible', 'agony', 'and', 'yet', 'in', 'the', 'midst', 'of', 'his', 'own', 'sorrow', 'and', 'pain', 'and', ',', 'and', 'torment', 'he', 'thinks', 'of', 'this', 'dying', 'thief', 'and', 'extends', 'his', 'grace', 'and', 'mercy', 'to', 'him', '.', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1577 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'as', 'i', 'mentioned', 'early', 'the', ',', 'the', 'city', 'of', 'ser', '##mar', '##ia', 'it', 'was', 'under', 'siege', 'and', 'the', 'army', 'of', 'ser', '##ia', 'was', 'en', '##camp', '##ed', 'all', 'around', 'it', ',', 'ben', 'had', '##ad', 'was', 'a', 'great', 'warrior', ',', 'he', 'would', 'of', 'been', 'the', ',', 'the', 'alexander', 'or', 'the', 'napoleon', 'of', 'his', 'day', 'and', 'he', 'had', 'set', 'up', 'this', 'en', '##camp', '##ment', 'around', 'the', 'city', 'of', 'ser', '##mar', '##ia', ',', 'nobody', 'could', 'get', 'in', ',', 'nobody', 'could', 'get', 'out', 'and', 'very', 'quickly', 'the', 'stocks', 'of', 'food', 'and', 'water', 'er', 'were', 'used', 'up', ',', 'ratio', '##ning', 'would', 'of', 'been', 'introduced', 'but', 'it', 'only', 'lasted', 'for', 'a', 'certain', 'period', ',', 'they', \"'\", 'd', 'got', 'to', 'the', 'stage', 'it', 'tells', 'us', 'in', 'the', 'previous', 'chapter', 'that', 'er', ',', 'that', 'a', 'donkey', '##s', 'head', 'was', 'sold', 'for', 'eighty', 'she', '##kel', \"'\", 's', 'of', 'silver', 'and', 'some', 'folk', 'had', 'even', 'got', 'to', 'the', ',', 'had', 'sunk', 'to', 'the', 'level', 'of', 'can', '##ni', '##bal', '##ism', ',', 'of', 'eating', 'their', 'own', 'children', 'and', 'the', 'city', 'was', ',', 'when', 'they', 'heard', 'about', 'this', 'they', 'were', 'in', 'an', 'up', '##ro', '##ar', 'and', 'they', 'started', 'blaming', 'god', 'and', 'in', 'between', 'the', 'city', 'of', 'ser', '##mar', '##ia', 'of', 'all', 'its', 'suffering', 'and', 'hopeless', '##ness', 'and', 'helpless', '##ness', 'and', 'the', 'army', 'en', '##camp', '##ed', 'about', 'with', 'all', 'of', 'their', 'supplies', ',', 'there', 'was', 'this', 'area', 'of', 'no', 'mans', 'land', 'in', 'which', 'they', 'were', 'caught', 'up', 'four', 'men', 'who', 'were', 'leap', '##ers', 'and', 'they', 'were', 'trapped', 'there', ',', 'they', 'did', 'n', \"'\", 't', 'want', 'to', 'go', 'over', 'to', 'the', 'ser', '##ians', 'because', 'they', \"'\", 'd', 'be', 'killed', ',', 'they', 'did', 'n', \"'\", 't', 'want', 'to', 'go', 'back', 'into', 'the', 'city', 'because', 'they', 'were', 'n', \"'\", 't', 'allowed', 'there', 'and', 'any', 'way', 'what', 'was', 'the', 'point', ',', 'they', \"'\", 'd', 'only', 'die', 'of', 'starvation', 'in', 'there', 'and', 'so', 'these', 'four', 'men', 'are', 'caught', 'up', 'in', 'no', 'man', \"'\", 's', 'land', 'and', 'yet', 'their', 'no', 'better', 'off', 'than', 'people', 'in', 'the', 'city', ',', 'now', 'god', 'had', 'promised', 'deliver', '##ance', ',', 'through', 'his', 'serve', 'and', 'eli', '##ger', 'he', 'had', 'promised', 'deliver', '##ance', ',', 'eli', '##ger', 'said', 'tomorrow', 'about', 'this', 'time', 'a', 'measure', 'of', 'fine', 'flour', 'shall', 'be', 'sold', 'for', 'a', 'se', 'she', '##kel', 'and', 'two', 'measures', 'of', 'barley', 'for', 'a', 'she', '##kel', 'in', 'the', 'gate', 'of', 'se', '##mar', '##ia', ',', 'he', 'said', 'the', 'gates', 'are', 'go', '##n', 'na', 'be', 'open', ',', 'there', \"'\", 's', 'go', '##n', 'na', 'be', 'food', 'and', 'its', 'go', '##n', 'na', 'be', 'a', 'reasonable', 'price', 'and', 'it', 'says', 'the', 'royal', 'officer', 'who', \"'\", 's', 'hand', 'the', 'king', 'was', 'leaning', 'on', 'said', 'the', 'man', 'of', 'god', 'said', 'behold', ',', 'if', 'the', 'lord', 'shall', 'make', 'windows', 'in', 'heaven', 'could', 'such', 'a', 'thing', 'be', ',', 'he', 'said', 'do', 'n', \"'\", 't', 'talk', 'stupid', 'man', ',', 'how', 'can', 'such', 'a', 'thing', 'happen', 'for', 'us', '?', ',', 'he', 'did', 'n', \"'\", 't', 'believe', 'what', 'god', 'servant', 'said', 'and', 'eli', '##ger', 'brings', 'out', 'to', 'him', 'a', 'terrible', 'judgment', ',', 'he', 'says', 'because', 'of', 'your', 'un', '##bel', '##ie', '##f', 'you', 'will', 'see', 'it', ',', 'but', 'your', 'not', 'participate', 'in', 'it', 'but', 'lets', 'look', 'at', 'these', 'four', 'men', 'for', 'a', 'moment', ',', 'co', '##s', 'that', \"'\", 's', 'where', 'our', 'real', 'interest', 'lies', 'this', 'morning', ',', 'i', 'just', 'wanted', 'to', 'say', 'three', 'things', 'in', 'their', 'experience', ',', 'the', 'first', 'things', 'is', 'that', 'they', 'were', 'amazed', 'that', ',', 'at', 'what', 'they', 'found', ',', 'because', 'after', 'they', 'come', 'together', 'and', 'they', 'talk', 'about', 'it', 'and', 'they', 'said', 'well', 'what', 'shall', 'we', 'do', 'and', 'they', 'weighed', 'the', 'pro', \"'\", 's', 'and', 'the', 'con', '##s', 'and', 'se', '##mar', '##ia', 'does', 'n', \"'\", 't', 'look', 'very', 'attractive', 'with', 'its', 'can', '##ni', '##bal', '##ism', ',', 'they', 'said', 'well', 'the', 'least', 'if', 'we', 'stay', 'here', 'were', 'go', '##n', 'na', 'die', ',', 'if', 'we', 'go', 'into', 'se', '##mar', '##ia', 'we', \"'\", 'll', 'die', ',', 'lets', 'go', 'down', 'to', 'the', 'ser', '##ein', 'camp', ',', 'the', 'worse', 'they', 'can', 'do', 'to', 'us', 'is', 'put', 'us', 'to', 'death', 'and', 'were', 'dying', 'men', 'any', 'way', ',', 'but', 'they', 'may', 'just', 'take', 'pity', 'on', 'us', ',', 'we', 'maybe', 'allowed', 'to', 'gr', '##ope', 'around', 'in', 'their', 'dust', '##bin', '##s', 'and', 'get', 'some', 'scrap', '##s', 'of', 'food', ',', 'they', 'may', 'at', 'least', 'allow', 'us', 'that', ',', 'and', 'so', 'they', 'make', 'their', 'way', 'down', 'just', 'as', 'evening', 'is', 'falling', ',', 'they', 'make', 'their', 'way', 'down', 'to', 'the', 'ser', '##ein', 'lines', 'and', 'when', 'they', 'get', 'there', ',', 'they', 'are', 'amazed', 'at', 'what', 'they', 'find', ',', 'you', 'see', 'their', 'condition', 'was', 'helpless', 'and', 'hopeless', ',', 'they', 'were', 'dying', 'men', 'any', 'way', ',', 'they', 'were', 'le', '##pers', ',', 'but', 'they', 'were', 'dying', 'of', 'starvation', ',', 'that', 'was', 'far', 'more', 'imminent', 'than', 'their', 'le', '##pro', '##sy', ',', 'their', 'problems', 'and', 'their', 'needs', 'were', 'greater', 'than', 'themselves', ',', 'they', 'could', 'not', 'meet', 'their', 'own', 'needs', ',', 'their', 'problems', 'and', 'their', 'needs', 'were', 'greater', 'than', 'their', 'government', ',', 'the', 'king', 'in', 'se', '##mar', '##ia', 'and', 'all', 'of', 'his', 'court', 'could', 'not', 'meet', 'the', 'needs', 'of', 'his', 'people', 'and', 'then', 'in', 'verse', 'five', ',', 'we', 'read', 'something', 'there', ',', 'they', 'arose', 'at', 'twilight', 'to', 'go', 'to', 'the', 'camp', 'of', 'ara', '##mian', '##s', 'or', 'the', 'ser', '##ein', \"'\", 's', 'and', 'when', 'they', 'came', 'to', 'the', 'outskirts', 'of', 'the', 'camp', 'of', 'the', 'ser', '##ein', \"'\", 's', 'behold', 'there', 'was', 'no', 'one', 'there', ',', 'they', 'expected', 'to', 'at', 'least', 'meet', 'a', 'guard', ',', 'there', 'would', 'surely', 'be', 'somebody', 'on', 'sent', '##ry', 'duty', 'even', 'if', 'the', 'rest', 'of', 'the', 'soldiers', 'had', 'gone', 'in', 'to', 'their', 'tents', 'and', 'were', 'perhaps', 'getting', 'ready', 'for', 'their', ',', 'for', 'the', 'evening', ',', 'going', 'to', 'bed', 'or', 'whatever', 'they', 'were', 'go', '##n', 'na', 'be', 'doing', ',', 'having', 'their', 'evening', 'meal', ',', 'there', 'would', 'at', 'least', 'be', 'somebody', 'on', 'guard', 'duty', ',', 'but', 'when', 'they', 'got', 'there', ',', 'there', 'was', 'no', 'one', 'there', ',', 'god', 'had', 'stepped', 'in', ',', 'god', 'had', 'intervened', 'and', 'the', 'good', 'news', 'of', 'the', 'christian', 'gospel', 'is', 'that', 'god', 'has', 'intervened', 'in', 'our', ',', 'in', 'the', 'midst', 'of', 'our', 'helpless', '##ness', ',', 'in', 'the', 'midst', 'of', 'our', 'hopeless', '##ness', ',', 'god', 'has', 'intervened', ',', 'he', 'had', 'stepped', 'in', 'to', 'history', ',', 'so', 'often', 'you', \"'\", 'll', 'hear', 'folks', 'say', ',', 'well', 'why', 'does', 'n', \"'\", 't', 'god', 'do', 'something', ',', 'why', 'does', 'god', 'allow', 'this', 'to', 'happen', ',', 'why', 'does', 'god', 'allow', 'that', 'one', ',', 'why', 'does', 'n', \"'\", 't', 'he', 'do', 'something', 'all', 'they', 'really', 'show', 'by', 'that', 'comment', 'is', 'their', 'own', 'ignorance', ',', 'because', 'god', 'has', 'done', 'something', ',', 'god', 'has', 'intervened', ',', 'listen', 'to', 'what', 'it', 'says', 'in', 'john', 'three', 'sixteen', ',', 'for', 'god', 'so', 'loved', 'the', 'world', 'that', 'he', 'gave', ',', 'he', \"'\", 's', 'only', 'son', 'and', 'the', 'er', ',', 'the', 'er', 'apostle', 'paul', 'and', 'he', \"'\", 's', 'writing', 'to', 'the', 'gall', '##ations', ',', 'in', 'chapter', 'four', 'and', 'in', 'verses', 'four', 'and', 'five', 'hear', 'what', 'he', 'says', 'there', ',', 'but', 'when', 'the', 'time', 'had', 'fully', 'come', 'god', 'sent', 'his', 'son', ',', 'born', 'of', 'a', 'woman', ',', 'born', 'under', 'law', 'to', 'red', '##eem', 'those', 'under', 'law', 'that', 'we', 'might', 'receive', 'the', 'full', 'rights', 'of', 'son', ',', 'er', 'of', 'sons', ',', 'god', 'has', 'done', 'something', ',', 'he', \"'\", 's', 'sent', 'his', 'son', 'jesus', 'christ', 'into', 'this', 'world', 'in', 'fact', 'his', 'done', 'the', 'greatest', 'thing', 'he', 'could', 'do', ',', 'he', 'has', 'done', 'the', 'very', 'ultimate', 'thing', ',', 'he', 'has', 'sent', 'his', 'son', 'into', 'the', 'world', 'that', \"'\", 's', 'the', 'greatest', 'intervention', 'god', 'could', 'ever', 'have', 'made', ',', 'it', 'was', 'far', 'greater', 'than', ',', 'than', 'just', 'intervening', 'in', 'sm', ',', 'in', 'some', 'small', 'local', 'event', ',', 'were', 'you', 'see', 'some', 'catastrophe', 'happening', 'and', 'you', 'say', 'well', 'why', 'does', 'n', \"'\", 't', 'god', 'do', 'something', 'there', ',', 'or', 'there', \"'\", 's', 'a', 'war', 'situation', 'going', 'on', 'in', 'some', 'other', 'part', 'of', 'the', 'world', ',', 'well', 'why', 'does', 'n', \"'\", 't', 'god', 'step', 'in', 'and', 'stop', 'it', ',', 'god', 'has', 'stepped', 'in', ',', 'not', 'in', 'a', 'local', 'situation', ',', 'not', 'in', 'some', 'er', 'passing', 'problem', 'or', 'need', 'but', 'he', \"'\", 's', 'stepped', 'into', 'the', 'greatest', 'way', 'possible', 'by', 'sending', 'his', 'son', 'jesus', 'christ', 'into', 'the', 'world', 'to', 'dye', 'for', 'men', 'and', 'woman', ',', 'to', 'take', 'away', 'sin', ',', 'to', 'pay', 'the', 'price', 'that', 'god', \"'\", 's', 'righteous', '##ness', 'demands', 'for', 'sin', 'so', 'god', 'has', 'intervened', 'and', 'his', 'intervention', 'has', 'changed', 'the', 'whole', 'situation', ',', 'its', 'brought', 'a', 'whole', 'new', 'complexion', 'on', 'things', ',', 'its', 'changed', 'the', 'colour', 'completely', ',', 'no', 'longer', 'is', 'the', 'world', 'now', 'under', 'darkness', 'and', 'in', ',', 'and', 'in', 'pending', 'judgment', 'in', 'doom', ',', 'because', 'jesus', 'christ', 'came', 'and', 'he', 'took', 'that', 'judgment', 'and', 'that', ',', 'that', 'condemnation', 'upon', 'himself', ',', 'he', 'said', 'i', \"'\", 've', 'not', 'come', 'to', 'condemn', 'the', 'world', 'he', 'said', 'its', 'already', 'condemned', ',', 'its', 'already', 'under', 'judgement', ',', 'the', 'sword', 'of', 'dam', '##oc', '##les', 'is', 'already', 'hanging', 'over', 'the', 'world', 'and', 'jesus', 'christ', 'came', 'in', 'and', 'to', 'take', 'that', 'judgment', 'and', 'that', 'condemnation', 'on', 'himself', 'and', 'when', 'he', 'died', 'there', 'on', 'the', 'cross', 'and', 'rose', 'again', ',', 'there', 'came', 'that', 'burst', 'of', 'light', 'in', 'a', 'world', 'that', 'had', 'been', 'sh', '##roud', '##ed', 'in', 'blackness', 'and', 'darkness', ',', 'a', 'world', 'that', 'had', 'been', 'sh', '##roud', '##ed', 'in', 'sin', 'suddenly', 'for', 'the', 'first', 'time', 'sees', 'the', 'light', ',', 'god', 'has', 'paid', 'for', 'himself', 'the', 'price', 'of', 'sin', ',', 'god', 'has', 'intervened', 'and', 'changed', 'the', 'whole', 'situation', 'and', 'the', 'message', 'of', 'the', 'gospel', 'is', 'that', 'if', 'you', 'and', 'i', 'allow', 'that', 'intervention', 'to', 'effect', 'us', 'personally', ',', 'then', 'like', 'those', 'four', 'men', 'surely', 'we', 'too', 'are', 'amazed', 'at', 'what', 'we', \"'\", 've', 'found', '.', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1577 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'as', 'i', 'mentioned', 'early', 'the', ',', 'the', 'city', 'of', 'ser', '##mar', '##ia', 'it', 'was', 'under', 'siege', 'and', 'the', 'army', 'of', 'ser', '##ia', 'was', 'en', '##camp', '##ed', 'all', 'around', 'it', ',', 'ben', 'had', '##ad', 'was', 'a', 'great', 'warrior', ',', 'he', 'would', 'of', 'been', 'the', ',', 'the', 'alexander', 'or', 'the', 'napoleon', 'of', 'his', 'day', 'and', 'he', 'had', 'set', 'up', 'this', 'en', '##camp', '##ment', 'around', 'the', 'city', 'of', 'ser', '##mar', '##ia', ',', 'nobody', 'could', 'get', 'in', ',', 'nobody', 'could', 'get', 'out', 'and', 'very', 'quickly', 'the', 'stocks', 'of', 'food', 'and', 'water', 'er', 'were', 'used', 'up', ',', 'ratio', '##ning', 'would', 'of', 'been', 'introduced', 'but', 'it', 'only', 'lasted', 'for', 'a', 'certain', 'period', ',', 'they', \"'\", 'd', 'got', 'to', 'the', 'stage', 'it', 'tells', 'us', 'in', 'the', 'previous', 'chapter', 'that', 'er', ',', 'that', 'a', 'donkey', '##s', 'head', 'was', 'sold', 'for', 'eighty', 'she', '##kel', \"'\", 's', 'of', 'silver', 'and', 'some', 'folk', 'had', 'even', 'got', 'to', 'the', ',', 'had', 'sunk', 'to', 'the', 'level', 'of', 'can', '##ni', '##bal', '##ism', ',', 'of', 'eating', 'their', 'own', 'children', 'and', 'the', 'city', 'was', ',', 'when', 'they', 'heard', 'about', 'this', 'they', 'were', 'in', 'an', 'up', '##ro', '##ar', 'and', 'they', 'started', 'blaming', 'god', 'and', 'in', 'between', 'the', 'city', 'of', 'ser', '##mar', '##ia', 'of', 'all', 'its', 'suffering', 'and', 'hopeless', '##ness', 'and', 'helpless', '##ness', 'and', 'the', 'army', 'en', '##camp', '##ed', 'about', 'with', 'all', 'of', 'their', 'supplies', ',', 'there', 'was', 'this', 'area', 'of', 'no', 'mans', 'land', 'in', 'which', 'they', 'were', 'caught', 'up', 'four', 'men', 'who', 'were', 'leap', '##ers', 'and', 'they', 'were', 'trapped', 'there', ',', 'they', 'did', 'n', \"'\", 't', 'want', 'to', 'go', 'over', 'to', 'the', 'ser', '##ians', 'because', 'they', \"'\", 'd', 'be', 'killed', ',', 'they', 'did', 'n', \"'\", 't', 'want', 'to', 'go', 'back', 'into', 'the', 'city', 'because', 'they', 'were', 'n', \"'\", 't', 'allowed', 'there', 'and', 'any', 'way', 'what', 'was', 'the', 'point', ',', 'they', \"'\", 'd', 'only', 'die', 'of', 'starvation', 'in', 'there', 'and', 'so', 'these', 'four', 'men', 'are', 'caught', 'up', 'in', 'no', 'man', \"'\", 's', 'land', 'and', 'yet', 'their', 'no', 'better', 'off', 'than', 'people', 'in', 'the', 'city', ',', 'now', 'god', 'had', 'promised', 'deliver', '##ance', ',', 'through', 'his', 'serve', 'and', 'eli', '##ger', 'he', 'had', 'promised', 'deliver', '##ance', ',', 'eli', '##ger', 'said', 'tomorrow', 'about', 'this', 'time', 'a', 'measure', 'of', 'fine', 'flour', 'shall', 'be', 'sold', 'for', 'a', 'se', 'she', '##kel', 'and', 'two', 'measures', 'of', 'barley', 'for', 'a', 'she', '##kel', 'in', 'the', 'gate', 'of', 'se', '##mar', '##ia', ',', 'he', 'said', 'the', 'gates', 'are', 'go', '##n', 'na', 'be', 'open', ',', 'there', \"'\", 's', 'go', '##n', 'na', 'be', 'food', 'and', 'its', 'go', '##n', 'na', 'be', 'a', 'reasonable', 'price', 'and', 'it', 'says', 'the', 'royal', 'officer', 'who', \"'\", 's', 'hand', 'the', 'king', 'was', 'leaning', 'on', 'said', 'the', 'man', 'of', 'god', 'said', 'behold', ',', 'if', 'the', 'lord', 'shall', 'make', 'windows', 'in', 'heaven', 'could', 'such', 'a', 'thing', 'be', ',', 'he', 'said', 'do', 'n', \"'\", 't', 'talk', 'stupid', 'man', ',', 'how', 'can', 'such', 'a', 'thing', 'happen', 'for', 'us', '?', ',', 'he', 'did', 'n', \"'\", 't', 'believe', 'what', 'god', 'servant', 'said', 'and', 'eli', '##ger', 'brings', 'out', 'to', 'him', 'a', 'terrible', 'judgment', ',', 'he', 'says', 'because', 'of', 'your', 'un', '##bel', '##ie', '##f', 'you', 'will', 'see', 'it', ',', 'but', 'your', 'not', 'participate', 'in', 'it', 'but', 'lets', 'look', 'at', 'these', 'four', 'men', 'for', 'a', 'moment', ',', 'co', '##s', 'that', \"'\", 's', 'where', 'our', 'real', 'interest', 'lies', 'this', 'morning', ',', 'i', 'just', 'wanted', 'to', 'say', 'three', 'things', 'in', 'their', 'experience', ',', 'the', 'first', 'things', 'is', 'that', 'they', 'were', 'amazed', 'that', ',', 'at', 'what', 'they', 'found', ',', 'because', 'after', 'they', 'come', 'together', 'and', 'they', 'talk', 'about', 'it', 'and', 'they', 'said', 'well', 'what', 'shall', 'we', 'do', 'and', 'they', 'weighed', 'the', 'pro', \"'\", 's', 'and', 'the', 'con', '##s', 'and', 'se', '##mar', '##ia', 'does', 'n', \"'\", 't', 'look', 'very', 'attractive', 'with', 'its', 'can', '##ni', '##bal', '##ism', ',', 'they', 'said', 'well', 'the', 'least', 'if', 'we', 'stay', 'here', 'were', 'go', '##n', 'na', 'die', ',', 'if', 'we', 'go', 'into', 'se', '##mar', '##ia', 'we', \"'\", 'll', 'die', ',', 'lets', 'go', 'down', 'to', 'the', 'ser', '##ein', 'camp', ',', 'the', 'worse', 'they', 'can', 'do', 'to', 'us', 'is', 'put', 'us', 'to', 'death', 'and', 'were', 'dying', 'men', 'any', 'way', ',', 'but', 'they', 'may', 'just', 'take', 'pity', 'on', 'us', ',', 'we', 'maybe', 'allowed', 'to', 'gr', '##ope', 'around', 'in', 'their', 'dust', '##bin', '##s', 'and', 'get', 'some', 'scrap', '##s', 'of', 'food', ',', 'they', 'may', 'at', 'least', 'allow', 'us', 'that', ',', 'and', 'so', 'they', 'make', 'their', 'way', 'down', 'just', 'as', 'evening', 'is', 'falling', ',', 'they', 'make', 'their', 'way', 'down', 'to', 'the', 'ser', '##ein', 'lines', 'and', 'when', 'they', 'get', 'there', ',', 'they', 'are', 'amazed', 'at', 'what', 'they', 'find', ',', 'you', 'see', 'their', 'condition', 'was', 'helpless', 'and', 'hopeless', ',', 'they', 'were', 'dying', 'men', 'any', 'way', ',', 'they', 'were', 'le', '##pers', ',', 'but', 'they', 'were', 'dying', 'of', 'starvation', ',', 'that', 'was', 'far', 'more', 'imminent', 'than', 'their', 'le', '##pro', '##sy', ',', 'their', 'problems', 'and', 'their', 'needs', 'were', 'greater', 'than', 'themselves', ',', 'they', 'could', 'not', 'meet', 'their', 'own', 'needs', ',', 'their', 'problems', 'and', 'their', 'needs', 'were', 'greater', 'than', 'their', 'government', ',', 'the', 'king', 'in', 'se', '##mar', '##ia', 'and', 'all', 'of', 'his', 'court', 'could', 'not', 'meet', 'the', 'needs', 'of', 'his', 'people', 'and', 'then', 'in', 'verse', 'five', ',', 'we', 'read', 'something', 'there', ',', 'they', 'arose', 'at', 'twilight', 'to', 'go', 'to', 'the', 'camp', 'of', 'ara', '##mian', '##s', 'or', 'the', 'ser', '##ein', \"'\", 's', 'and', 'when', 'they', 'came', 'to', 'the', 'outskirts', 'of', 'the', 'camp', 'of', 'the', 'ser', '##ein', \"'\", 's', 'behold', 'there', 'was', 'no', 'one', 'there', ',', 'they', 'expected', 'to', 'at', 'least', 'meet', 'a', 'guard', ',', 'there', 'would', 'surely', 'be', 'somebody', 'on', 'sent', '##ry', 'duty', 'even', 'if', 'the', 'rest', 'of', 'the', 'soldiers', 'had', 'gone', 'in', 'to', 'their', 'tents', 'and', 'were', 'perhaps', 'getting', 'ready', 'for', 'their', ',', 'for', 'the', 'evening', ',', 'going', 'to', 'bed', 'or', 'whatever', 'they', 'were', 'go', '##n', 'na', 'be', 'doing', ',', 'having', 'their', 'evening', 'meal', ',', 'there', 'would', 'at', 'least', 'be', 'somebody', 'on', 'guard', 'duty', ',', 'but', 'when', 'they', 'got', 'there', ',', 'there', 'was', 'no', 'one', 'there', ',', 'god', 'had', 'stepped', 'in', ',', 'god', 'had', 'intervened', 'and', 'the', 'good', 'news', 'of', 'the', 'christian', 'gospel', 'is', 'that', 'god', 'has', 'intervened', 'in', 'our', ',', 'in', 'the', 'midst', 'of', 'our', 'helpless', '##ness', ',', 'in', 'the', 'midst', 'of', 'our', 'hopeless', '##ness', ',', 'god', 'has', 'intervened', ',', 'he', 'had', 'stepped', 'in', 'to', 'history', ',', 'so', 'often', 'you', \"'\", 'll', 'hear', 'folks', 'say', ',', 'well', 'why', 'does', 'n', \"'\", 't', 'god', 'do', 'something', ',', 'why', 'does', 'god', 'allow', 'this', 'to', 'happen', ',', 'why', 'does', 'god', 'allow', 'that', 'one', ',', 'why', 'does', 'n', \"'\", 't', 'he', 'do', 'something', 'all', 'they', 'really', 'show', 'by', 'that', 'comment', 'is', 'their', 'own', 'ignorance', ',', 'because', 'god', 'has', 'done', 'something', ',', 'god', 'has', 'intervened', ',', 'listen', 'to', 'what', 'it', 'says', 'in', 'john', 'three', 'sixteen', ',', 'for', 'god', 'so', 'loved', 'the', 'world', 'that', 'he', 'gave', ',', 'he', \"'\", 's', 'only', 'son', 'and', 'the', 'er', ',', 'the', 'er', 'apostle', 'paul', 'and', 'he', \"'\", 's', 'writing', 'to', 'the', 'gall', '##ations', ',', 'in', 'chapter', 'four', 'and', 'in', 'verses', 'four', 'and', 'five', 'hear', 'what', 'he', 'says', 'there', ',', 'but', 'when', 'the', 'time', 'had', 'fully', 'come', 'god', 'sent', 'his', 'son', ',', 'born', 'of', 'a', 'woman', ',', 'born', 'under', 'law', 'to', 'red', '##eem', 'those', 'under', 'law', 'that', 'we', 'might', 'receive', 'the', 'full', 'rights', 'of', 'son', ',', 'er', 'of', 'sons', ',', 'god', 'has', 'done', 'something', ',', 'he', \"'\", 's', 'sent', 'his', 'son', 'jesus', 'christ', 'into', 'this', 'world', 'in', 'fact', 'his', 'done', 'the', 'greatest', 'thing', 'he', 'could', 'do', ',', 'he', 'has', 'done', 'the', 'very', 'ultimate', 'thing', ',', 'he', 'has', 'sent', 'his', 'son', 'into', 'the', 'world', 'that', \"'\", 's', 'the', 'greatest', 'intervention', 'god', 'could', 'ever', 'have', 'made', ',', 'it', 'was', 'far', 'greater', 'than', ',', 'than', 'just', 'intervening', 'in', 'sm', ',', 'in', 'some', 'small', 'local', 'event', ',', 'were', 'you', 'see', 'some', 'catastrophe', 'happening', 'and', 'you', 'say', 'well', 'why', 'does', 'n', \"'\", 't', 'god', 'do', 'something', 'there', ',', 'or', 'there', \"'\", 's', 'a', 'war', 'situation', 'going', 'on', 'in', 'some', 'other', 'part', 'of', 'the', 'world', ',', 'well', 'why', 'does', 'n', \"'\", 't', 'god', 'step', 'in', 'and', 'stop', 'it', ',', 'god', 'has', 'stepped', 'in', ',', 'not', 'in', 'a', 'local', 'situation', ',', 'not', 'in', 'some', 'er', 'passing', 'problem', 'or', 'need', 'but', 'he', \"'\", 's', 'stepped', 'into', 'the', 'greatest', 'way', 'possible', 'by', 'sending', 'his', 'son', 'jesus', 'christ', 'into', 'the', 'world', 'to', 'dye', 'for', 'men', 'and', 'woman', ',', 'to', 'take', 'away', 'sin', ',', 'to', 'pay', 'the', 'price', 'that', 'god', \"'\", 's', 'righteous', '##ness', 'demands', 'for', 'sin', 'so', 'god', 'has', 'intervened', 'and', 'his', 'intervention', 'has', 'changed', 'the', 'whole', 'situation', ',', 'its', 'brought', 'a', 'whole', 'new', 'complexion', 'on', 'things', ',', 'its', 'changed', 'the', 'colour', 'completely', ',', 'no', 'longer', 'is', 'the', 'world', 'now', 'under', 'darkness', 'and', 'in', ',', 'and', 'in', 'pending', 'judgment', 'in', 'doom', ',', 'because', 'jesus', 'christ', 'came', 'and', 'he', 'took', 'that', 'judgment', 'and', 'that', ',', 'that', 'condemnation', 'upon', 'himself', ',', 'he', 'said', 'i', \"'\", 've', 'not', 'come', 'to', 'condemn', 'the', 'world', 'he', 'said', 'its', 'already', 'condemned', ',', 'its', 'already', 'under', 'judgement', ',', 'the', 'sword', 'of', 'dam', '##oc', '##les', 'is', 'already', 'hanging', 'over', 'the', 'world', 'and', 'jesus', 'christ', 'came', 'in', 'and', 'to', 'take', 'that', 'judgment', 'and', 'that', 'condemnation', 'on', 'himself', 'and', 'when', 'he', 'died', 'there', 'on', 'the', 'cross', 'and', 'rose', 'again', ',', 'there', 'came', 'that', 'burst', 'of', 'light', 'in', 'a', 'world', 'that', 'had', 'been', 'sh', '##roud', '##ed', 'in', 'blackness', 'and', 'darkness', ',', 'a', 'world', 'that', 'had', 'been', 'sh', '##roud', '##ed', 'in', 'sin', 'suddenly', 'for', 'the', 'first', 'time', 'sees', 'the', 'light', ',', 'god', 'has', 'paid', 'for', 'himself', 'the', 'price', 'of', 'sin', ',', 'god', 'has', 'intervened', 'and', 'changed', 'the', 'whole', 'situation', 'and', 'the', 'message', 'of', 'the', 'gospel', 'is', 'that', 'if', 'you', 'and', 'i', 'allow', 'that', 'intervention', 'to', 'effect', 'us', 'personally', ',', 'then', 'like', 'those', 'four', 'men', 'surely', 'we', 'too', 'are', 'amazed', 'at', 'what', 'we', \"'\", 've', 'found', '.', '[SEP]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 900 words\n",
      "calculating clusters for bizarre\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sequence too long\n",
      "['[CLS]', 'i', 'suppose', 'this', 'is', 'er', '##m', ',', 'a', 'car', '##ica', '##ture', ',', 'a', 'self', 'portrait', 'this', 'little', ',', 'i', ',', 'in', 'fact', 'i', 'was', ',', 'really', 'actually', 'hilarious', 'as', 'i', 'felt', 'that', ',', 'i', ',', 'i', ',', 'do', 'n', \"'\", 't', 'actually', 'know', 'an', 'awful', 'lot', 'about', 'ga', '##ug', '##in', ',', 'but', 'if', ',', 'if', 'i', 'knew', 'nothing', 'about', 'him', 'at', 'all', ',', 'i', 'would', 'of', 'thought', 'he', 'was', 'having', 'a', 'bit', 'of', 'joke', 'of', 'himself', 'with', 'this', ',', 'but', 'er', ',', 'being', 'the', 'person', 'that', 'he', 'was', 'i', 'ca', 'n', \"'\", 't', 'image', 'that', 'he', 'had', 'that', 'quality', ',', 'that', ',', 'i', 'do', 'n', \"'\", 't', 'believe', 'he', 'would', 'be', 'laughing', 'at', 'himself', ',', 'er', '##m', ',', 'er', '##m', ',', 'the', 'symbolism', 'er', '##m', 'and', 'conflict', 'of', 'this', 'painting', 'its', 'da', '##zzle', '##s', 'me', 'more', 'than', ',', 'than', 'the', 'colour', 'or', 'at', 'least', 'as', 'much', 'as', 'the', 'colours', 'in', 'it', ',', 'but', 'there', \"'\", 's', 'a', ',', 'there', \"'\", 's', 'a', 'half', 'eaten', ',', 'well', 'it', 'is', 'n', \"'\", 't', 'half', 'eaten', ',', 'but', 'there', \"'\", 's', 'half', 'an', 'apple', 'at', 'the', 'top', 'and', ',', 'and', 'that', 'was', 'the', ',', 'the', 'way', 'into', 'me', 'finally', ',', 'for', ',', 'for', 'writing', 'about', 'this', ',', 'this', 'again', 'is', 'a', 'shopping', 'list', ',', 'i', 'call', 'it', 'a', 'shopping', 'list', ',', 'this', 'is', 'just', 'visual', 'images', 'that', ',', 'that', 'will', 'be', 'opened', 'out', 'at', 'some', 'point', 'and', 'turn', 'it', 'into', 'something', ',', 'and', 'my', 'images', 'were', 'er', '##m', 'shoulders', 'of', 'the', 'mata', '##dor', 'smoking', 'snakes', ',', 'dare', 'to', 'bit', 'an', 'apple', ',', 'see', 'one', 'half', 'gone', 'and', 'still', 'i', 'wear', 'a', 'halo', 'intact', ',', 'that', 'i', \"'\", 'm', 'sure', 'i', \"'\", 've', 'completely', 'wrong', 'about', 'him', 'as', 'a', ',', 'a', 'person', ',', 'but', 'as', 'the', 'painting', 'that', \"'\", 's', 'obviously', 'something', 'else', ',', 'er', '##m', ',', 'i', 'found', 'that', 'one', 'of', 'the', 'things', 'that', 'were', 'he', \"'\", 's', ',', 'i', ',', 'i', 'think', 'probably', 'that', 'everybody', 'who', 'writes', 'is', 'that', 'you', \"'\", 'll', 'come', 'to', 'a', 'point', 'when', 'you', 'ca', 'n', \"'\", 't', 'write', ',', 'you', 'stop', 'writing', ',', 'you', 'have', 'n', \"'\", 't', 'got', 'anything', 'you', 'want', 'to', 'write', 'about', ',', 'or', 'your', 'frightened', 'of', 'writing', ',', 'and', 'i', 'devi', '##se', 'exercises', 'so', 'that', ',', 'that', 'does', 'n', \"'\", 't', 'happened', 'to', 'me', ',', 'i', 'think', 'writing', 'is', 'like', 'any', 'skill', 'you', 'have', 'to', 'keep', 'doing', 'it', 'to', 'be', 'able', 'to', 'do', 'it', ',', 'its', ',', 'you', ',', 'some', 'of', 'it', 'is', 'a', 'game', 'and', 'the', 'rest', 'of', 'it', 'is', 'hard', 'work', ',', 'and', 'one', 'of', 'the', 'exercises', 'i', ',', 'i', 'delighted', 'using', 'er', '##m', 'a', 'portrait', 'of', 'a', 'woman', 'er', '##m', ',', 'its', 'about', 'er', '##m', 'the', 'er', 'still', 'life', ',', 'its', 'the', 'back', 'one', ',', 'yes', 'this', 'one', 'here', ',', 'i', 'have', ',', 'i', ',', 'i', 'hope', 'to', 'use', 'this', 'as', 'a', 'writing', 'exercise', 'i', 'found', 'the', ',', 'the', 'math', '##s', 'in', 'this', 'and', 'the', 'colour', 'of', 'the', 'piece', 'of', 'fruit', 'in', 'the', 'background', ',', 'very', 'interesting', 'because', 'most', 'of', 'the', 'colours', 'to', 'me', 'seem', 'a', ',', 'a', 'lot', 'less', 'vibrant', 'then', 'many', 'of', 'his', 'other', 'paintings', ',', 'and', 'so', 'they', ',', 'they', 'attracted', 'me', 'and', 'have', 'a', ',', 'a', 'strong', 'sense', 'of', 'er', '##m', ',', 'er', 'a', 'hidden', 'desire', 'in', 'that', 'and', 'so', 'it', ',', 'to', 'use', 'it', 'as', 'a', 'writing', 'exercise', 'which', 'i', 'intend', 'doing', ',', 'it', 'will', 'be', 'able', 'about', 'a', 'situation', 'of', 'character', \"'\", 's', 'with', 'er', '##m', 'something', 'that', \"'\", 's', 'hidden', ',', 'some', 'desire', ',', 'i', 'do', 'n', \"'\", 't', ',', 'the', ',', 'not', 'even', 'spoke', 'about', 'to', 'themselves', 'or', ',', 'or', 'generally', ',', 'er', '##m', 'i', 'like', 'to', 'sort', 'of', 'say', 'that', 'came', 'from', 'those', 'two', 'little', ',', 'just', 'this', 'amounts', 'of', 'colour', 'which', 'seemed', 'to', 'be', 'saying', 'such', 'a', 'lot', '[SEP]']\n",
      "not enough tokens to make 5 clusters for word: orthodontist\n",
      "not enough tokens to make 7 clusters for word: orthodontist\n",
      "not enough tokens to make 5 clusters for word: orthodontist\n",
      "not enough tokens to make 7 clusters for word: orthodontist\n",
      "not enough tokens to make 5 clusters for word: orthodontist\n",
      "not enough tokens to make 7 clusters for word: orthodontist\n",
      "not enough tokens to make 5 clusters for word: orthodontist\n",
      "not enough tokens to make 7 clusters for word: orthodontist\n",
      "processed 1000 words\n",
      "calculating clusters for importance\n",
      "processed 1100 words\n",
      "calculating clusters for swamp\n",
      "processed 1200 words\n",
      "calculating clusters for bell\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "import numpy as np\n",
    "import bert_helper\n",
    "import csv\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for each word file we have, do the following:\n",
    "    for each layer we care about, calculate the token embedding at that layer for each token\n",
    "        for each number of clusters we care about, calculate the centroids of those clusters\n",
    "        \n",
    "store results in a file, one for each word+layer+cluster_number combo, resulting in a file structure like the following:\n",
    "\n",
    "word_data/\n",
    "  |-airplane/\n",
    "  | |- bnc_tokens.csv\n",
    "  | |- layer_0_k_1_clusters.csv\n",
    "  | |   ...\n",
    "  | |- layer_0_k_7_clusters.csv\n",
    "  | |   ...\n",
    "  | |- layer_11_k_7_clusters.csv\n",
    "  \n",
    "each cluster file is a csv with the following fields:\n",
    "    word\n",
    "    layer\n",
    "    cluster_size_k\n",
    "    cluster_number\n",
    "    centroid\n",
    "    token_ids\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "(model, tokenizer) = bert_helper.initialize()\n",
    "\n",
    "i = 0\n",
    "for word in unique_words:\n",
    "    i+=1\n",
    "    if i % 100 == 0:\n",
    "        print(\"processed %s words\" % i)\n",
    "        print(\"calculating clusters for %s\" % word)\n",
    "\n",
    "    # it's more efficient to collect all the vectors for all the layers at once,\n",
    "    # since we calculate the whole activation network at once for each token\n",
    "    vectors = []\n",
    "\n",
    "    \n",
    "    # create a directory to store all our clustering results in\n",
    "    data_dir = './data/word_data'\n",
    "    results_dir = os.path.join(data_dir, word, 'analysis_results')    \n",
    "    if os.path.exists(results_dir):\n",
    "        shutil.rmtree(results_dir)\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "    # read in the tokens for this word\n",
    "    pathname = os.path.join(data_dir, word, 'BNC_tokens.csv')\n",
    "    with open(pathname, mode='r') as csv_file:\n",
    "        reader = csv.DictReader(csv_file, delimiter='\\t', fieldnames=[\"word\", \"sentence\", \"tag\", \"uid\"])\n",
    "        \n",
    "        data = [row for row in reader]\n",
    "\n",
    "        # generate embeddings for each token\n",
    "        for row in data:\n",
    "            sentence = row[\"sentence\"]\n",
    "            vector = bert_helper.get_bert_vectors_for(word, sentence, model, tokenizer)\n",
    "            # if the token was too long we may not have succeeded in generating embeddings for it, in which case we will throw it out\n",
    "            if vector != None:\n",
    "                row[\"embedding\"] = vector\n",
    "            else:\n",
    "                row[\"embedding\"] = None\n",
    "        data = list(filter(lambda row: row[\"embedding\"] != None, data))\n",
    "\n",
    "        for layer in layers:\n",
    "            layer_vectors = [row[\"embedding\"][layer] for row in data]\n",
    "        \n",
    "            for k in cluster_sizes:\n",
    "                if len(data) >= k:\n",
    "                    # calculate clusters\n",
    "                    kmeans_obj = KMeans(n_clusters=k)\n",
    "                    kmeans_obj.fit(layer_vectors)\n",
    "                    label_list = kmeans_obj.labels_\n",
    "                    cluster_centroids = kmeans_obj.cluster_centers_\n",
    "\n",
    "\n",
    "                    # store clusternumber with data\n",
    "                    for index,datapoint in enumerate(data):\n",
    "                        datapoint['cluster_number'] = label_list[index]\n",
    "\n",
    "                    # generate outfile name\n",
    "                    filename = \"layer_\" + str(layer) + \"_clusters_k_equals_\" + str(k) + \".csv\"\n",
    "                    outpath = os.path.join(results_dir, filename)\n",
    "\n",
    "\n",
    "                    with open(outpath, mode='w') as disk:\n",
    "                        writer = csv.DictWriter(disk, delimiter='\\t', fieldnames=['word', 'clusternumber', 'centroid', 'sentence_uids'])\n",
    "\n",
    "\n",
    "                        # retrieve centroid for each cluster and uids of sentences in cluster:\n",
    "                        for clusternumber in range(k):\n",
    "                            sentence_uids = []\n",
    "                            for index, datapoint in enumerate(data):\n",
    "                                if datapoint['cluster_number'] == clusternumber:\n",
    "                                    sentence_uids.append(datapoint['uid'])\n",
    "                            out_data = {'word': word,\n",
    "                                        'clusternumber': clusternumber,\n",
    "                                        'centroid': cluster_centroids[clusternumber],\n",
    "                                        'sentence_uids': sentence_uids}\n",
    "\n",
    "                            # store in file\n",
    "                            # write dta for this cluster\n",
    "                            writer.writerow(out_data)\n",
    "\n",
    "                else:\n",
    "                    print(\"not enough tokens to make %s clusters for word: %s\" % (k, word))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will evaluate each combination of layer and num_clusters against similarity gold standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In the end we want two data structures that looks like this:\n",
    "\n",
    "layer k_clusters ws353_pearson p ws353_spearman p ws353_n simlex_pearson p  simlex_spearman p  simlex_n\n",
    "0     1          .77             .73              200     .54              .49                 988\n",
    "....  ....       ....\n",
    "0     7          .88             .80              180     .54              .65                 950\n",
    "1     1          ....\n",
    "...   ....       ....\n",
    "11    7          ....\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "\n",
    "results_file = './data/bnc_cluster_analysis_ws353_similarity_results.csv'\n",
    "fieldnames = ['layer', 'k_clusters', 'pearson', 'pearson_P', 'spearman', 'spearman_P', 'N']\n",
    "with open(results_file, mode='w') as disk:\n",
    "    writer = csv.DictWriter(disk, delimiter='\\t', fieldnames=fieldnames)\n",
    "    \n",
    "    \n",
    "    for layer_number in layers:\n",
    "        for k in cluster_sizes:\n",
    "            \n",
    "            # calc sim for all the word pairs\n",
    "            data = ws353\n",
    "            expected_similarities = []\n",
    "            for row in data:\n",
    "                word1 = row['word1']\n",
    "                word2 = row['word2']\n",
    "                observed_similarity = row['similarity']\n",
    "\n",
    "                # get centroid data for these words at this layer and this k size\n",
    "                pairwise_centroids = {}\n",
    "                for word in [word1, word2]:\n",
    "                    cluster_filename = \"layer_\" + str(layer_number) + \"_clusters_k_equals_\" + str(k) + \".csv\"\n",
    "                    cluster_path = os.path.join('./data/word_data/', word, 'analysis_results', cluster_filename)\n",
    "                    with open(cluster_path, mode='r') as csv_file:\n",
    "                        fieldnames = ['word', 'clusternumber', 'centroid', 'sentence_uids']\n",
    "                        reader = csv.DictReader(csv_file, delimiter='\\t', fieldnames=fieldnames)\n",
    "\n",
    "                        word_centroids = []\n",
    "                        for line in reader:\n",
    "                            centroid = np.fromstring(line['centroid'][2:-2], dtype=np.float, sep=' ')\n",
    "                            word_centroids.append(centroid)\n",
    "                        pairwise_centroids[word] = word_centroids\n",
    "\n",
    "\n",
    "\n",
    "                # calculate maxsim\n",
    "                # calculate predicted similarity from of each pair of cluster centroids of both words\n",
    "                predicted_similarities = []\n",
    "                for centroid1 in pairwise_centroids[word1]:\n",
    "                    for centroid2 in pairwise_centroids[word2]:\n",
    "                        predicted_similarity = 1 - cosine(centroid1, centroid2)\n",
    "                        predicted_similarities.append(predicted_similarity)\n",
    "                # find the max of the pairwise similarities\n",
    "                max_sim = max(predicted_similarities)\n",
    "\n",
    "                row['predicted_similarity'] = max_sim\n",
    "\n",
    "            # create data frame \n",
    "            df = pd.DataFrame.from_records(data)\n",
    "            X = df['predicted_similarity']\n",
    "            y = df['similarity']\n",
    "\n",
    "            # run pearson expected vs observed\n",
    "            pearson_value = pearsonr(X,y)\n",
    "\n",
    "            # run spearman expected vs observed\n",
    "            spearman_value = spearmanr(X,y)\n",
    "\n",
    "\n",
    "            # save results to file\n",
    "            output = {'layer': layer_number,\n",
    "                      'k_clusters': k,\n",
    "                      'pearson': pearson_value[0],\n",
    "                      'pearson_P': pearson_value[1],\n",
    "                      'spearman': spearman_value[0],\n",
    "                      'spearman_P': spearman_value[1],\n",
    "                      'N': len(df)\n",
    "                     }\n",
    "            writer.writerow(output)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the same for simlex999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n"
     ]
    }
   ],
   "source": [
    "results_file = './data/bnc_cluster_analysis_simlex999_similarity_results.csv'\n",
    "fieldnames = ['layer', 'k_clusters', 'pearson', 'pearson_P', 'spearman', 'spearman_P', 'N']\n",
    "with open(results_file, mode='w') as disk:\n",
    "    writer = csv.DictWriter(disk, delimiter='\\t', fieldnames=fieldnames)\n",
    "    \n",
    "    \n",
    "    for layer_number in layers:\n",
    "        for k in cluster_sizes:\n",
    "            \n",
    "            # calc sim for all the word pairs\n",
    "            data = simlex999\n",
    "            expected_similarities = []\n",
    "            for row in data:\n",
    "                word1 = row['word1']\n",
    "                word2 = row['word2']\n",
    "                observed_similarity = row['SimLex999']\n",
    "\n",
    "                # get centroid data for these words at this layer and this k size\n",
    "                pairwise_centroids = {}\n",
    "                for word in [word1, word2]:\n",
    "                    cluster_filename = \"layer_\" + str(layer_number) + \"_clusters_k_equals_\" + str(k) + \".csv\"\n",
    "                    cluster_path = os.path.join('./data/word_data/', word, 'analysis_results', cluster_filename)\n",
    "                    \n",
    "                    try:\n",
    "                        with open(cluster_path, mode='r') as csv_file:\n",
    "                            fieldnames = ['word', 'clusternumber', 'centroid', 'sentence_uids']\n",
    "                            reader = csv.DictReader(csv_file, delimiter='\\t', fieldnames=fieldnames)\n",
    "\n",
    "                            word_centroids = []\n",
    "                            for line in reader:\n",
    "                                centroid = np.fromstring(line['centroid'][2:-2], dtype=np.float, sep=' ')\n",
    "                                word_centroids.append(centroid)\n",
    "                            pairwise_centroids[word] = word_centroids\n",
    "                    except:\n",
    "                        print(\"can't calculate predicted similarity for pair %s, %s\" %(word1, word2))\n",
    "                        print(\"   no tokens collected for %s\" % word)\n",
    "\n",
    "\n",
    "\n",
    "                # calculate maxsim\n",
    "                # calculate predicted similarity from of each pair of cluster centroids of both words\n",
    "                # only if we have centroids for both words\n",
    "                if (word1 in pairwise_centroids) and (word2 in pairwise_centroids):\n",
    "                    predicted_similarities = []\n",
    "                    for centroid1 in pairwise_centroids[word1]:\n",
    "                        for centroid2 in pairwise_centroids[word2]:\n",
    "                            predicted_similarity = 1 - cosine(centroid1, centroid2)\n",
    "                            predicted_similarities.append(predicted_similarity)\n",
    "                    # find the max of the pairwise similarities\n",
    "                    max_sim = max(predicted_similarities)\n",
    "\n",
    "                    row['predicted_similarity'] = max_sim\n",
    "                else:\n",
    "                    row['predicted_similarity'] = None\n",
    "            \n",
    "            # remove pairs from consideration for which we have no predicted similarity to compare\n",
    "            data = list(filter(lambda row: row['predicted_similarity'] != None, data))\n",
    "\n",
    "\n",
    "            # create data frame \n",
    "            df = pd.DataFrame.from_records(data)\n",
    "            X = df['predicted_similarity']\n",
    "            y = df['SimLex999']\n",
    "\n",
    "            # run pearson expected vs observed\n",
    "            pearson_value = pearsonr(X,y)\n",
    "\n",
    "            # run spearman expected vs observed\n",
    "            spearman_value = spearmanr(X,y)\n",
    "\n",
    "\n",
    "            # save results to file\n",
    "            output = {'layer': layer_number,\n",
    "                      'k_clusters': k,\n",
    "                      'pearson': pearson_value[0],\n",
    "                      'pearson_P': pearson_value[1],\n",
    "                      'spearman': spearman_value[0],\n",
    "                      'spearman_P': spearman_value[1],\n",
    "                      'N': len(df)\n",
    "                     }\n",
    "            writer.writerow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try avgsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In the end we want two data structures that looks like this:\n",
    "\n",
    "layer k_clusters ws353_pearson p ws353_spearman p ws353_n simlex_pearson p  simlex_spearman p  simlex_n\n",
    "0     1          .77             .73              200     .54              .49                 988\n",
    "....  ....       ....\n",
    "0     7          .88             .80              180     .54              .65                 950\n",
    "1     1          ....\n",
    "...   ....       ....\n",
    "11    7          ....\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "\n",
    "results_file = './data/bnc_cluster_analysis_ws353_avgsim_similarity_results.csv'\n",
    "fieldnames = ['layer', 'k_clusters', 'pearson', 'pearson_P', 'spearman', 'spearman_P', 'N']\n",
    "with open(results_file, mode='w') as disk:\n",
    "    writer = csv.DictWriter(disk, delimiter='\\t', fieldnames=fieldnames)\n",
    "    \n",
    "    \n",
    "    for layer_number in layers:\n",
    "        for k in cluster_sizes:\n",
    "            \n",
    "            # calc sim for all the word pairs\n",
    "            data = ws353\n",
    "            expected_similarities = []\n",
    "            for row in data:\n",
    "                word1 = row['word1']\n",
    "                word2 = row['word2']\n",
    "                observed_similarity = row['similarity']\n",
    "\n",
    "                # get centroid data for these words at this layer and this k size\n",
    "                pairwise_centroids = {}\n",
    "                for word in [word1, word2]:\n",
    "                    cluster_filename = \"layer_\" + str(layer_number) + \"_clusters_k_equals_\" + str(k) + \".csv\"\n",
    "                    cluster_path = os.path.join('./data/word_data/', word, 'analysis_results', cluster_filename)\n",
    "                    with open(cluster_path, mode='r') as csv_file:\n",
    "                        fieldnames = ['word', 'clusternumber', 'centroid', 'sentence_uids']\n",
    "                        reader = csv.DictReader(csv_file, delimiter='\\t', fieldnames=fieldnames)\n",
    "\n",
    "                        word_centroids = []\n",
    "                        for line in reader:\n",
    "                            centroid = np.fromstring(line['centroid'][2:-2], dtype=np.float, sep=' ')\n",
    "                            word_centroids.append(centroid)\n",
    "                        pairwise_centroids[word] = word_centroids\n",
    "\n",
    "\n",
    "\n",
    "                # calculate maxsim\n",
    "                # calculate predicted similarity from of each pair of cluster centroids of both words\n",
    "                predicted_similarities = []\n",
    "                for centroid1 in pairwise_centroids[word1]:\n",
    "                    for centroid2 in pairwise_centroids[word2]:\n",
    "                        predicted_similarity = 1 - cosine(centroid1, centroid2)\n",
    "                        predicted_similarities.append(predicted_similarity)\n",
    "                # find the max of the pairwise similarities\n",
    "                avg_sim = np.sum(predicted_similarities) / k*k\n",
    "\n",
    "                row['predicted_similarity'] = avg_sim\n",
    "\n",
    "            # create data frame \n",
    "            df = pd.DataFrame.from_records(data)\n",
    "            X = df['predicted_similarity']\n",
    "            y = df['similarity']\n",
    "\n",
    "            # run pearson expected vs observed\n",
    "            pearson_value = pearsonr(X,y)\n",
    "\n",
    "            # run spearman expected vs observed\n",
    "            spearman_value = spearmanr(X,y)\n",
    "\n",
    "\n",
    "            # save results to file\n",
    "            output = {'layer': layer_number,\n",
    "                      'k_clusters': k,\n",
    "                      'pearson': pearson_value[0],\n",
    "                      'pearson_P': pearson_value[1],\n",
    "                      'spearman': spearman_value[0],\n",
    "                      'spearman_P': spearman_value[1],\n",
    "                      'N': len(df)\n",
    "                     }\n",
    "            writer.writerow(output)\n",
    "            \n",
    "            \n",
    "results_file = './data/bnc_cluster_analysis_simlex999_avgsim_similarity_results.csv'\n",
    "fieldnames = ['layer', 'k_clusters', 'pearson', 'pearson_P', 'spearman', 'spearman_P', 'N']\n",
    "with open(results_file, mode='w') as disk:\n",
    "    writer = csv.DictWriter(disk, delimiter='\\t', fieldnames=fieldnames)\n",
    "    \n",
    "    \n",
    "    for layer_number in layers:\n",
    "        for k in cluster_sizes:\n",
    "            \n",
    "            # calc sim for all the word pairs\n",
    "            data = simlex999\n",
    "            expected_similarities = []\n",
    "            for row in data:\n",
    "                word1 = row['word1']\n",
    "                word2 = row['word2']\n",
    "                observed_similarity = row['SimLex999']\n",
    "\n",
    "                # get centroid data for these words at this layer and this k size\n",
    "                pairwise_centroids = {}\n",
    "                for word in [word1, word2]:\n",
    "                    cluster_filename = \"layer_\" + str(layer_number) + \"_clusters_k_equals_\" + str(k) + \".csv\"\n",
    "                    cluster_path = os.path.join('./data/word_data/', word, 'analysis_results', cluster_filename)\n",
    "                    \n",
    "                    try:\n",
    "                        with open(cluster_path, mode='r') as csv_file:\n",
    "                            fieldnames = ['word', 'clusternumber', 'centroid', 'sentence_uids']\n",
    "                            reader = csv.DictReader(csv_file, delimiter='\\t', fieldnames=fieldnames)\n",
    "\n",
    "                            word_centroids = []\n",
    "                            for line in reader:\n",
    "                                centroid = np.fromstring(line['centroid'][2:-2], dtype=np.float, sep=' ')\n",
    "                                word_centroids.append(centroid)\n",
    "                            pairwise_centroids[word] = word_centroids\n",
    "                    except:\n",
    "                        print(\"can't calculate predicted similarity for pair %s, %s\" %(word1, word2))\n",
    "                        print(\"   no tokens collected for %s\" % word)\n",
    "\n",
    "\n",
    "\n",
    "                # calculate maxsim\n",
    "                # calculate predicted similarity from of each pair of cluster centroids of both words\n",
    "                # only if we have centroids for both words\n",
    "                if (word1 in pairwise_centroids) and (word2 in pairwise_centroids):\n",
    "                    predicted_similarities = []\n",
    "                    for centroid1 in pairwise_centroids[word1]:\n",
    "                        for centroid2 in pairwise_centroids[word2]:\n",
    "                            predicted_similarity = 1 - cosine(centroid1, centroid2)\n",
    "                            predicted_similarities.append(predicted_similarity)\n",
    "                    # find the max of the pairwise similarities\n",
    "                    avg_sim = np.sum(predicted_similarities) / k*k\n",
    "\n",
    "                    row['predicted_similarity'] = avg_sim\n",
    "                else:\n",
    "                    row['predicted_similarity'] = None\n",
    "            \n",
    "            # remove pairs from consideration for which we have no predicted similarity to compare\n",
    "            data = list(filter(lambda row: row['predicted_similarity'] != None, data))\n",
    "\n",
    "\n",
    "            # create data frame \n",
    "            df = pd.DataFrame.from_records(data)\n",
    "            X = df['predicted_similarity']\n",
    "            y = df['SimLex999']\n",
    "\n",
    "            # run pearson expected vs observed\n",
    "            pearson_value = pearsonr(X,y)\n",
    "\n",
    "            # run spearman expected vs observed\n",
    "            spearman_value = spearmanr(X,y)\n",
    "\n",
    "\n",
    "            # save results to file\n",
    "            output = {'layer': layer_number,\n",
    "                      'k_clusters': k,\n",
    "                      'pearson': pearson_value[0],\n",
    "                      'pearson_P': pearson_value[1],\n",
    "                      'spearman': spearman_value[0],\n",
    "                      'spearman_P': spearman_value[1],\n",
    "                      'N': len(df)\n",
    "                     }\n",
    "            writer.writerow(output)            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n",
      "can't calculate predicted similarity for pair orthodontist, dentist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair doctor, orthodontist\n",
      "   no tokens collected for orthodontist\n",
      "can't calculate predicted similarity for pair disorganize, organize\n",
      "   no tokens collected for disorganize\n"
     ]
    }
   ],
   "source": [
    "#UMMM have you been doing this wrong with cosine distance when you should have been using similarity?\n",
    "\n",
    "results_file = './data/bnc_cluster_analysis_simlex999_similarity_results_cossim.csv'\n",
    "fieldnames = ['layer', 'k_clusters', 'pearson', 'pearson_P', 'spearman', 'spearman_P', 'N']\n",
    "with open(results_file, mode='w') as disk:\n",
    "    writer = csv.DictWriter(disk, delimiter='\\t', fieldnames=fieldnames)\n",
    "    \n",
    "    \n",
    "    for layer_number in layers:\n",
    "        for k in cluster_sizes:\n",
    "            \n",
    "            # calc sim for all the word pairs\n",
    "            data = simlex999\n",
    "            expected_similarities = []\n",
    "            for row in data:\n",
    "                word1 = row['word1']\n",
    "                word2 = row['word2']\n",
    "                observed_similarity = row['SimLex999']\n",
    "\n",
    "                # get centroid data for these words at this layer and this k size\n",
    "                pairwise_centroids = {}\n",
    "                for word in [word1, word2]:\n",
    "                    cluster_filename = \"layer_\" + str(layer_number) + \"_clusters_k_equals_\" + str(k) + \".csv\"\n",
    "                    cluster_path = os.path.join('./data/word_data/', word, 'analysis_results', cluster_filename)\n",
    "                    \n",
    "                    try:\n",
    "                        with open(cluster_path, mode='r') as csv_file:\n",
    "                            fieldnames = ['word', 'clusternumber', 'centroid', 'sentence_uids']\n",
    "                            reader = csv.DictReader(csv_file, delimiter='\\t', fieldnames=fieldnames)\n",
    "\n",
    "                            word_centroids = []\n",
    "                            for line in reader:\n",
    "                                centroid = np.fromstring(line['centroid'][2:-2], dtype=np.float, sep=' ')\n",
    "                                word_centroids.append(centroid)\n",
    "                            pairwise_centroids[word] = word_centroids\n",
    "                    except:\n",
    "                        print(\"can't calculate predicted similarity for pair %s, %s\" %(word1, word2))\n",
    "                        print(\"   no tokens collected for %s\" % word)\n",
    "\n",
    "\n",
    "\n",
    "                # calculate maxsim\n",
    "                # calculate predicted similarity from of each pair of cluster centroids of both words\n",
    "                # only if we have centroids for both words\n",
    "                if (word1 in pairwise_centroids) and (word2 in pairwise_centroids):\n",
    "                    predicted_similarities = []\n",
    "                    for centroid1 in pairwise_centroids[word1]:\n",
    "                        for centroid2 in pairwise_centroids[word2]:\n",
    "                            #predicted_similarity = 1 - cosine(centroid1, centroid2)\n",
    "                            predicted_similarity = cosine(centroid1, centroid2)\n",
    "                            predicted_similarities.append(predicted_similarity)\n",
    "                    # find the max of the pairwise similarities\n",
    "                    max_sim = max(predicted_similarities)\n",
    "\n",
    "                    row['predicted_similarity'] = max_sim\n",
    "                else:\n",
    "                    row['predicted_similarity'] = None\n",
    "            \n",
    "            # remove pairs from consideration for which we have no predicted similarity to compare\n",
    "            data = list(filter(lambda row: row['predicted_similarity'] != None, data))\n",
    "\n",
    "\n",
    "            # create data frame \n",
    "            df = pd.DataFrame.from_records(data)\n",
    "            X = df['predicted_similarity']\n",
    "            y = df['SimLex999']\n",
    "\n",
    "            # run pearson expected vs observed\n",
    "            pearson_value = pearsonr(X,y)\n",
    "\n",
    "            # run spearman expected vs observed\n",
    "            spearman_value = spearmanr(X,y)\n",
    "\n",
    "\n",
    "            # save results to file\n",
    "            output = {'layer': layer_number,\n",
    "                      'k_clusters': k,\n",
    "                      'pearson': pearson_value[0],\n",
    "                      'pearson_P': pearson_value[1],\n",
    "                      'spearman': spearman_value[0],\n",
    "                      'spearman_P': spearman_value[1],\n",
    "                      'N': len(df)\n",
    "                     }\n",
    "            writer.writerow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
