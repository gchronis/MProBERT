{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('FACTSHEET', 'SUBST'),\n",
       "  ('WHAT', 'PRON'),\n",
       "  ('IS', 'VERB'),\n",
       "  ('AIDS', 'SUBST'),\n",
       "  ('?', 'PUN')],\n",
       " [('AIDS', 'SUBST'),\n",
       "  ('(', 'PUL'),\n",
       "  ('Acquired', 'VERB'),\n",
       "  ('Immune', 'ADJ'),\n",
       "  ('Deficiency', 'SUBST'),\n",
       "  ('Syndrome', 'SUBST'),\n",
       "  (')', 'PUR'),\n",
       "  ('is', 'VERB'),\n",
       "  ('a', 'ART'),\n",
       "  ('condition', 'SUBST'),\n",
       "  ('caused', 'VERB'),\n",
       "  ('by', 'PREP'),\n",
       "  ('a', 'ART'),\n",
       "  ('virus', 'SUBST'),\n",
       "  ('called', 'VERB'),\n",
       "  ('HIV', 'SUBST'),\n",
       "  ('(', 'PUL'),\n",
       "  ('Human', 'ADJ'),\n",
       "  ('Immuno', 'SUBST'),\n",
       "  ('Deficiency', 'SUBST'),\n",
       "  ('Virus', 'SUBST'),\n",
       "  (')', 'PUR'),\n",
       "  ('.', 'PUN')]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "b = datasets.get_bnc()\n",
    "b.tagged_sents()[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load WS353"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 203 word pairs from WordSim similarity dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('word1', 'monk'), ('word2', 'oracle'), ('similarity', 5.0)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws353 = datasets.get_ws353()\n",
    "\n",
    "ws353[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Simlex-999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 999 word pairs from simlex999 dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('word1', 'bold'),\n",
       "             ('word2', 'proud'),\n",
       "             ('POS', 'A'),\n",
       "             ('SimLex999', 3.97),\n",
       "             ('conc_w1', '2.43'),\n",
       "             ('conc_w2', '2.07'),\n",
       "             ('concQ', '1'),\n",
       "             ('assoc_USF', '0.12'),\n",
       "             ('sim_assoc333', '0'),\n",
       "             ('SD_simlex', '1.36')])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simlex999 = datasets.get_simlex999()\n",
    "\n",
    "simlex999[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num words in datasets: 2404\n",
      "Number of unique words in simlex + wordsim: 1224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'legion',\n",
       " 'modern',\n",
       " 'fence',\n",
       " 'foot',\n",
       " 'evening',\n",
       " 'attempt',\n",
       " 'decide',\n",
       " 'might',\n",
       " 'eye',\n",
       " 'flour',\n",
       " 'big',\n",
       " 'intuition',\n",
       " 'impression',\n",
       " 'denial',\n",
       " 'breakfast',\n",
       " 'minute',\n",
       " 'key',\n",
       " 'student',\n",
       " 'army',\n",
       " 'jaguar',\n",
       " 'mustard',\n",
       " 'bold',\n",
       " 'swamp',\n",
       " 'intelligent',\n",
       " 'profession',\n",
       " 'succeed',\n",
       " 'cheap',\n",
       " 'navy',\n",
       " 'bathroom',\n",
       " 'new',\n",
       " 'creator',\n",
       " 'island',\n",
       " 'monster',\n",
       " 'stream',\n",
       " 'live',\n",
       " 'large',\n",
       " 'bar',\n",
       " 'image',\n",
       " 'disc',\n",
       " 'lad',\n",
       " 'cloth',\n",
       " 'wheat',\n",
       " 'bishop',\n",
       " 'drizzle',\n",
       " 'ceremony',\n",
       " 'pursue',\n",
       " 'rail',\n",
       " 'boundary',\n",
       " 'reef',\n",
       " 'course',\n",
       " 'please',\n",
       " 'sentry',\n",
       " 'gain',\n",
       " 'ox',\n",
       " 'collection',\n",
       " 'basketball',\n",
       " 'storm',\n",
       " 'purse',\n",
       " 'sign',\n",
       " 'fabric',\n",
       " 'serial',\n",
       " 'egg',\n",
       " 'trick',\n",
       " 'dad',\n",
       " 'food',\n",
       " 'style',\n",
       " 'game',\n",
       " 'summer',\n",
       " 'vessel',\n",
       " 'honey',\n",
       " 'chance',\n",
       " 'assume',\n",
       " 'week',\n",
       " 'wonderful',\n",
       " 'odd',\n",
       " 'delay',\n",
       " 'create',\n",
       " 'forest',\n",
       " 'kind',\n",
       " 'buck',\n",
       " 'fashion',\n",
       " 'sunshine',\n",
       " 'reality',\n",
       " 'violin',\n",
       " 'emergency',\n",
       " 'possess',\n",
       " 'scheme',\n",
       " 'contemplate',\n",
       " 'whiskey',\n",
       " 'dawn',\n",
       " 'sunset',\n",
       " 'comprehend',\n",
       " 'proximity',\n",
       " 'deserve',\n",
       " 'kidney',\n",
       " 'plead',\n",
       " 'adversary',\n",
       " 'theme',\n",
       " 'fun',\n",
       " 'rhythm',\n",
       " 'conclude',\n",
       " 'effort',\n",
       " 'marathon',\n",
       " 'floor',\n",
       " 'dull',\n",
       " 'ear',\n",
       " 'quick',\n",
       " 'horse',\n",
       " 'composer',\n",
       " 'shrink',\n",
       " 'seven',\n",
       " 'spend',\n",
       " 'tooth',\n",
       " 'kilometer',\n",
       " 'jazz',\n",
       " 'hang',\n",
       " 'satisfy',\n",
       " 'story',\n",
       " 'reflection',\n",
       " 'value',\n",
       " 'absence',\n",
       " 'find',\n",
       " 'inn',\n",
       " 'physician',\n",
       " 'cottage',\n",
       " 'perception',\n",
       " 'cannon',\n",
       " 'box',\n",
       " 'theft',\n",
       " 'finger',\n",
       " 'tragedy',\n",
       " 'surgery',\n",
       " 'index',\n",
       " 'engagement',\n",
       " 'behave',\n",
       " 'gut',\n",
       " 'declare',\n",
       " 'recess',\n",
       " 'honest',\n",
       " 'bible',\n",
       " 'trial',\n",
       " 'law',\n",
       " 'medal',\n",
       " 'approach',\n",
       " 'definition',\n",
       " 'chapel',\n",
       " 'short',\n",
       " 'god',\n",
       " 'explore',\n",
       " 'motto',\n",
       " 'coffee',\n",
       " 'prestige',\n",
       " 'unhappy',\n",
       " 'yale',\n",
       " 'date',\n",
       " 'bone',\n",
       " 'stud',\n",
       " 'candy',\n",
       " 'aggression',\n",
       " 'violation',\n",
       " 'party',\n",
       " 'air',\n",
       " 'restless',\n",
       " 'scarce',\n",
       " 'orchestra',\n",
       " 'cemetery',\n",
       " 'harm',\n",
       " 'lady',\n",
       " 'jackson',\n",
       " 'sweater',\n",
       " 'reject',\n",
       " 'mob',\n",
       " 'announce',\n",
       " 'taxi',\n",
       " 'report',\n",
       " 'ignorance',\n",
       " 'exotic',\n",
       " 'participate',\n",
       " 'bath',\n",
       " 'jet',\n",
       " 'cabbage',\n",
       " 'board',\n",
       " 'cow',\n",
       " 'disease',\n",
       " 'mouth',\n",
       " 'menu',\n",
       " 'rod',\n",
       " 'hip',\n",
       " 'theory',\n",
       " 'defend',\n",
       " 'paragraph',\n",
       " 'inform',\n",
       " 'zone',\n",
       " 'bicycle',\n",
       " 'cone',\n",
       " 'fame',\n",
       " 'task',\n",
       " 'proof',\n",
       " 'differ',\n",
       " 'movement',\n",
       " 'bread',\n",
       " 'conquest',\n",
       " 'child',\n",
       " 'cage',\n",
       " 'sauce',\n",
       " 'barn',\n",
       " 'center',\n",
       " 'spoon',\n",
       " 'enjoy',\n",
       " 'nephew',\n",
       " 'withdraw',\n",
       " 'vein',\n",
       " 'coat',\n",
       " 'dominate',\n",
       " 'childish',\n",
       " 'noon',\n",
       " 'example',\n",
       " 'terrible',\n",
       " 'appear',\n",
       " 'avenue',\n",
       " 'oracle',\n",
       " 'chest',\n",
       " 'antecedent',\n",
       " 'direction',\n",
       " 'recommendation',\n",
       " 'attitude',\n",
       " 'crystal',\n",
       " 'bus',\n",
       " 'cliff',\n",
       " 'hill',\n",
       " 'noise',\n",
       " 'elbow',\n",
       " 'plan',\n",
       " 'implement',\n",
       " 'hymn',\n",
       " 'wizard',\n",
       " 'north',\n",
       " 'music',\n",
       " 'arrange',\n",
       " 'street',\n",
       " 'want',\n",
       " 'foolish',\n",
       " 'rat',\n",
       " 'dense',\n",
       " 'monk',\n",
       " 'chaos',\n",
       " 'gather',\n",
       " 'tongue',\n",
       " 'fantasy',\n",
       " 'fact',\n",
       " 'machine',\n",
       " 'moon',\n",
       " 'noticeable',\n",
       " 'plate',\n",
       " 'anxious',\n",
       " 'mayor',\n",
       " 'computation',\n",
       " 'originate',\n",
       " 'river',\n",
       " 'bacon',\n",
       " 'money',\n",
       " 'insight',\n",
       " 'decoration',\n",
       " 'song',\n",
       " 'archbishop',\n",
       " 'soup',\n",
       " 'code',\n",
       " 'dumb',\n",
       " 'dollar',\n",
       " 'exit',\n",
       " 'build',\n",
       " 'harsh',\n",
       " 'valley',\n",
       " 'vinegar',\n",
       " 'narrow',\n",
       " 'take',\n",
       " 'vitamin',\n",
       " 'company',\n",
       " 'wisdom',\n",
       " 'temper',\n",
       " 'rapid',\n",
       " 'uncle',\n",
       " 'inexpensive',\n",
       " 'possibility',\n",
       " 'american',\n",
       " 'cd',\n",
       " 'belief',\n",
       " 'children',\n",
       " 'booth',\n",
       " 'capital',\n",
       " 'class',\n",
       " 'despair',\n",
       " 'beef',\n",
       " 'stove',\n",
       " 'politician',\n",
       " 'investigate',\n",
       " 'five',\n",
       " 'side',\n",
       " 'oak',\n",
       " 'nation',\n",
       " 'unnecessary',\n",
       " 'hen',\n",
       " 'seashore',\n",
       " 'bench',\n",
       " 'sinner',\n",
       " 'wealth',\n",
       " 'fur',\n",
       " 'cotton',\n",
       " 'airport',\n",
       " 'book',\n",
       " 'put',\n",
       " 'prey',\n",
       " 'pride',\n",
       " 'church',\n",
       " 'triumph',\n",
       " 'pair',\n",
       " 'presence',\n",
       " 'argue',\n",
       " 'person',\n",
       " 'employer',\n",
       " 'helper',\n",
       " 'planet',\n",
       " 'yen',\n",
       " 'herb',\n",
       " 'word',\n",
       " 'wire',\n",
       " 'wide',\n",
       " 'strong',\n",
       " 'quest',\n",
       " 'morning',\n",
       " 'driver',\n",
       " 'straw',\n",
       " 'tennis',\n",
       " 'hear',\n",
       " 'shower',\n",
       " 'death',\n",
       " 'smile',\n",
       " 'payment',\n",
       " 'mexico',\n",
       " 'young',\n",
       " 'owe',\n",
       " 'leave',\n",
       " 'sorrow',\n",
       " 'august',\n",
       " 'salary',\n",
       " 'acknowledge',\n",
       " 'thumb',\n",
       " 'bag',\n",
       " 'polyester',\n",
       " 'normal',\n",
       " 'misery',\n",
       " 'borrow',\n",
       " 'brain',\n",
       " 'marry',\n",
       " 'coal',\n",
       " 'justify',\n",
       " 'aluminum',\n",
       " 'sun',\n",
       " 'group',\n",
       " 'potato',\n",
       " 'brandy',\n",
       " 'ball',\n",
       " 'engage',\n",
       " 'topic',\n",
       " 'management',\n",
       " 'verdict',\n",
       " 'cup',\n",
       " 'turkey',\n",
       " 'warrior',\n",
       " 'wander',\n",
       " 'hut',\n",
       " 'information',\n",
       " 'circumstance',\n",
       " 'knife',\n",
       " 'asylum',\n",
       " 'pact',\n",
       " 'skill',\n",
       " 'happiness',\n",
       " 'reason',\n",
       " 'sea',\n",
       " 'orthodontist',\n",
       " 'tough',\n",
       " 'mood',\n",
       " 'shore',\n",
       " 'dinner',\n",
       " 'day',\n",
       " 'observation',\n",
       " 'crowd',\n",
       " 'liquor',\n",
       " 'explain',\n",
       " 'clarify',\n",
       " 'rabbit',\n",
       " 'palm',\n",
       " 'illegal',\n",
       " 'target',\n",
       " 'cash',\n",
       " 'project',\n",
       " 'baseball',\n",
       " 'sickness',\n",
       " 'recent',\n",
       " 'lion',\n",
       " 'accomplish',\n",
       " 'destruction',\n",
       " 'overcome',\n",
       " 'brazil',\n",
       " 'examine',\n",
       " 'dreary',\n",
       " 'tin',\n",
       " 'singer',\n",
       " 'steeple',\n",
       " 'calendar',\n",
       " 'sheep',\n",
       " 'steak',\n",
       " 'news',\n",
       " 'dirty',\n",
       " 'grass',\n",
       " 'replace',\n",
       " 'arrive',\n",
       " 'different',\n",
       " 'problem',\n",
       " 'pain',\n",
       " 'coast',\n",
       " 'physics',\n",
       " 'beverage',\n",
       " 'wife',\n",
       " 'seafood',\n",
       " 'bottle',\n",
       " 'age',\n",
       " 'school',\n",
       " 'baby',\n",
       " 'paper',\n",
       " 'strength',\n",
       " 'architecture',\n",
       " 'atom',\n",
       " 'go',\n",
       " 'cigarette',\n",
       " 'salad',\n",
       " 'chief',\n",
       " 'timber',\n",
       " 'require',\n",
       " 'room',\n",
       " 'multiply',\n",
       " 'happen',\n",
       " 'bad',\n",
       " 'father',\n",
       " 'guitar',\n",
       " 'keep',\n",
       " 'elegance',\n",
       " 'reduce',\n",
       " 'magician',\n",
       " 'instructor',\n",
       " 'balloon',\n",
       " 'movie',\n",
       " 'laden',\n",
       " 'button',\n",
       " 'meal',\n",
       " 'save',\n",
       " 'persuade',\n",
       " 'buddy',\n",
       " 'beer',\n",
       " 'plane',\n",
       " 'aviation',\n",
       " 'plenty',\n",
       " 'block',\n",
       " 'violent',\n",
       " 'encourage',\n",
       " 'waist',\n",
       " 'hike',\n",
       " 'cognition',\n",
       " 'benchmark',\n",
       " 'weird',\n",
       " 'smart',\n",
       " 'hard',\n",
       " 'stock',\n",
       " 'earn',\n",
       " 'breathe',\n",
       " 'lamb',\n",
       " 'sex',\n",
       " 'culture',\n",
       " 'assignment',\n",
       " 'month',\n",
       " 'window',\n",
       " 'milk',\n",
       " 'attach',\n",
       " 'alley',\n",
       " 'brow',\n",
       " 'holy',\n",
       " 'ashamed',\n",
       " 'teeth',\n",
       " 'divide',\n",
       " 'anatomy',\n",
       " 'psychology',\n",
       " 'cell',\n",
       " 'experience',\n",
       " 'melody',\n",
       " 'maid',\n",
       " 'jaw',\n",
       " 'charcoal',\n",
       " 'term',\n",
       " 'credibility',\n",
       " 'newspaper',\n",
       " 'agree',\n",
       " 'ocean',\n",
       " 'minority',\n",
       " 'construction',\n",
       " 'beautiful',\n",
       " 'loop',\n",
       " 'guardian',\n",
       " 'harvard',\n",
       " 'cocktail',\n",
       " 'decade',\n",
       " 'calf',\n",
       " 'champagne',\n",
       " 'chair',\n",
       " 'cheek',\n",
       " 'forget',\n",
       " 'dog',\n",
       " 'universe',\n",
       " 'luck',\n",
       " 'insect',\n",
       " 'performance',\n",
       " 'awful',\n",
       " 'life',\n",
       " 'glass',\n",
       " 'computer',\n",
       " 'landscape',\n",
       " 'infrastructure',\n",
       " 'formal',\n",
       " 'friend',\n",
       " 'cab',\n",
       " 'organize',\n",
       " 'crucial',\n",
       " 'bean',\n",
       " 'border',\n",
       " 'deployment',\n",
       " 'hurricane',\n",
       " 'squad',\n",
       " 'neck',\n",
       " 'conversation',\n",
       " 'long',\n",
       " 'difficulty',\n",
       " 'cop',\n",
       " 'come',\n",
       " 'seed',\n",
       " 'elect',\n",
       " 'heroine',\n",
       " 'vanish',\n",
       " 'toe',\n",
       " 'college',\n",
       " 'cent',\n",
       " 'disorganize',\n",
       " 'cancer',\n",
       " 'avoid',\n",
       " 'vodka',\n",
       " 'crib',\n",
       " 'threat',\n",
       " 'string',\n",
       " 'funny',\n",
       " 'meat',\n",
       " 'mammal',\n",
       " 'captain',\n",
       " 'cheerful',\n",
       " 'polite',\n",
       " 'bed',\n",
       " 'apartment',\n",
       " 'cabin',\n",
       " 'battle',\n",
       " 'meet',\n",
       " 'mad',\n",
       " 'obvious',\n",
       " 'allow',\n",
       " 'sure',\n",
       " 'blood',\n",
       " 'forgive',\n",
       " 'doorway',\n",
       " 'hall',\n",
       " 'buy',\n",
       " 'society',\n",
       " 'tobacco',\n",
       " 'liquid',\n",
       " 'adult',\n",
       " 'west',\n",
       " 'catastrophe',\n",
       " 'banker',\n",
       " 'obey',\n",
       " 'morality',\n",
       " 'issue',\n",
       " 'prove',\n",
       " 'century',\n",
       " 'fox',\n",
       " 'fauna',\n",
       " 'disappear',\n",
       " 'iron',\n",
       " 'choose',\n",
       " 'isolation',\n",
       " 'give',\n",
       " 'fiction',\n",
       " 'start',\n",
       " 'hose',\n",
       " 'arm',\n",
       " 'gossip',\n",
       " 'chocolate',\n",
       " 'locate',\n",
       " 'deliver',\n",
       " 'lose',\n",
       " 'belt',\n",
       " 'peace',\n",
       " 'inspect',\n",
       " 'hypertension',\n",
       " 'season',\n",
       " 'steal',\n",
       " 'communicate',\n",
       " 'order',\n",
       " 'match',\n",
       " 'second',\n",
       " 'media',\n",
       " 'racism',\n",
       " 'roof',\n",
       " 'recognition',\n",
       " 'hotel',\n",
       " 'mink',\n",
       " 'ceiling',\n",
       " 'remind',\n",
       " 'abundance',\n",
       " 'thunderstorm',\n",
       " 'hat',\n",
       " 'oil',\n",
       " 'jacket',\n",
       " 'safety',\n",
       " 'film',\n",
       " 'hallway',\n",
       " 'crime',\n",
       " 'atmosphere',\n",
       " 'jail',\n",
       " 'engine',\n",
       " 'joy',\n",
       " 'water',\n",
       " 'get',\n",
       " 'fuck',\n",
       " 'endurance',\n",
       " 'mars',\n",
       " 'flower',\n",
       " 'door',\n",
       " 'entity',\n",
       " 'sheriff',\n",
       " 'surface',\n",
       " 'friendly',\n",
       " 'rice',\n",
       " 'institution',\n",
       " 'receive',\n",
       " 'protect',\n",
       " 'hospital',\n",
       " 'closet',\n",
       " 'phone',\n",
       " 'carpet',\n",
       " 'lawyer',\n",
       " 'calculation',\n",
       " 'comfort',\n",
       " 'happy',\n",
       " 'fury',\n",
       " 'lobster',\n",
       " 'evaluate',\n",
       " 'biology',\n",
       " 'certain',\n",
       " 'cord',\n",
       " 'drawer',\n",
       " 'actress',\n",
       " 'clever',\n",
       " 'ask',\n",
       " 'beauty',\n",
       " 'curve',\n",
       " 'tiger',\n",
       " 'practice',\n",
       " 'cat',\n",
       " 'essay',\n",
       " 'son',\n",
       " 'development',\n",
       " 'star',\n",
       " 'motel',\n",
       " 'ability',\n",
       " 'read',\n",
       " 'mare',\n",
       " 'metal',\n",
       " 'region',\n",
       " 'operation',\n",
       " 'spine',\n",
       " 'destroy',\n",
       " 'complain',\n",
       " 'eat',\n",
       " 'portray',\n",
       " 'rock',\n",
       " 'proclaim',\n",
       " 'vehicle',\n",
       " 'championship',\n",
       " 'accident',\n",
       " 'telephone',\n",
       " 'agreement',\n",
       " 'analyze',\n",
       " 'type',\n",
       " 'communication',\n",
       " 'page',\n",
       " 'try',\n",
       " 'series',\n",
       " 'trading',\n",
       " 'cucumber',\n",
       " 'midday',\n",
       " 'drink',\n",
       " 'aisle',\n",
       " 'pipe',\n",
       " 'proud',\n",
       " 'population',\n",
       " 'murder',\n",
       " 'governor',\n",
       " 'danger',\n",
       " 'highway',\n",
       " 'opponent',\n",
       " 'confusion',\n",
       " 'carnivore',\n",
       " 'soccer',\n",
       " 'brother',\n",
       " 'lend',\n",
       " 'construct',\n",
       " 'deck',\n",
       " 'weather',\n",
       " 'anchor',\n",
       " 'jewel',\n",
       " 'organism',\n",
       " 'men',\n",
       " 'accept',\n",
       " 'curtain',\n",
       " 'helium',\n",
       " 'marriage',\n",
       " 'gun',\n",
       " 'important',\n",
       " 'confidence',\n",
       " 'top',\n",
       " 'bell',\n",
       " 'corner',\n",
       " 'simple',\n",
       " 'haze',\n",
       " 'choke',\n",
       " 'fog',\n",
       " 'boy',\n",
       " 'hawk',\n",
       " 'head',\n",
       " 'rationalize',\n",
       " 'kill',\n",
       " 'loss',\n",
       " 'rough',\n",
       " 'wagon',\n",
       " 'cousin',\n",
       " 'belly',\n",
       " 'activity',\n",
       " 'impatient',\n",
       " 'author',\n",
       " 'gin',\n",
       " 'goat',\n",
       " 'lens',\n",
       " 'doctor',\n",
       " 'think',\n",
       " 'expand',\n",
       " 'manager',\n",
       " 'pet',\n",
       " 'sharp',\n",
       " 'listing',\n",
       " 'emotion',\n",
       " 'mechanic',\n",
       " 'colonel',\n",
       " 'object',\n",
       " 'museum',\n",
       " 'jar',\n",
       " 'cooperate',\n",
       " 'choir',\n",
       " 'rooster',\n",
       " 'interview',\n",
       " 'letter',\n",
       " 'viewer',\n",
       " 'costume',\n",
       " 'cigar',\n",
       " 'sick',\n",
       " 'insane',\n",
       " 'speak',\n",
       " 'pilot',\n",
       " 'leader',\n",
       " 'classroom',\n",
       " 'nose',\n",
       " 'sofa',\n",
       " 'understand',\n",
       " 'pearl',\n",
       " 'realize',\n",
       " 'muscle',\n",
       " 'add',\n",
       " 'cardboard',\n",
       " 'weekend',\n",
       " 'psychiatry',\n",
       " 'energy',\n",
       " 'area',\n",
       " 'mile',\n",
       " 'absorb',\n",
       " 'clothes',\n",
       " 'house',\n",
       " 'size',\n",
       " 'frigid',\n",
       " 'greet',\n",
       " 'equipment',\n",
       " 'pupil',\n",
       " 'bird',\n",
       " 'ray',\n",
       " 'kid',\n",
       " 'flexible',\n",
       " 'cathedral',\n",
       " 'ankle',\n",
       " 'logic',\n",
       " 'situation',\n",
       " 'feline',\n",
       " 'nerve',\n",
       " 'capability',\n",
       " 'imitate',\n",
       " 'way',\n",
       " 'inch',\n",
       " 'dirt',\n",
       " 'snow',\n",
       " 'secretary',\n",
       " 'investigation',\n",
       " 'nurse',\n",
       " 'strange',\n",
       " 'idea',\n",
       " 'maple',\n",
       " 'saint',\n",
       " 'appliance',\n",
       " 'flight',\n",
       " 'opinion',\n",
       " 'princess',\n",
       " 'winter',\n",
       " 'rain',\n",
       " 'king',\n",
       " 'log',\n",
       " 'dentist',\n",
       " 'factory',\n",
       " 'fee',\n",
       " 'pray',\n",
       " 'alcohol',\n",
       " 'volunteer',\n",
       " 'difficult',\n",
       " 'fresh',\n",
       " 'ice',\n",
       " 'soldier',\n",
       " 'abdomen',\n",
       " 'deny',\n",
       " 'fever',\n",
       " 'remain',\n",
       " 'text',\n",
       " 'pie',\n",
       " 'huge',\n",
       " 'personnel',\n",
       " 'gauge',\n",
       " 'condition',\n",
       " 'sunrise',\n",
       " 'apparent',\n",
       " 'cattle',\n",
       " 'garden',\n",
       " 'currency',\n",
       " 'sense',\n",
       " 'car',\n",
       " 'skin',\n",
       " 'occur',\n",
       " 'suds',\n",
       " 'moss',\n",
       " 'butter',\n",
       " 'sly',\n",
       " 'ballad',\n",
       " 'equation',\n",
       " 'molecule',\n",
       " 'lemon',\n",
       " 'automobile',\n",
       " 'minister',\n",
       " 'insurance',\n",
       " 'fast',\n",
       " 'diamond',\n",
       " 'sell',\n",
       " 'wrist',\n",
       " 'parade',\n",
       " 'cruel',\n",
       " 'precedent',\n",
       " 'crazy',\n",
       " 'association',\n",
       " 'ignore',\n",
       " 'sailor',\n",
       " 'sugar',\n",
       " 'know',\n",
       " 'beg',\n",
       " 'yard',\n",
       " 'appointment',\n",
       " 'beast',\n",
       " 'era',\n",
       " 'tiny',\n",
       " 'broad',\n",
       " 'spirit',\n",
       " 'juice',\n",
       " 'demon',\n",
       " 'right',\n",
       " 'hound',\n",
       " 'formula',\n",
       " 'rattle',\n",
       " 'victor',\n",
       " 'criticism',\n",
       " 'afraid',\n",
       " 'grief',\n",
       " 'do',\n",
       " 'football',\n",
       " 'compare',\n",
       " 'discuss',\n",
       " 'voyage',\n",
       " 'heart',\n",
       " 'manslaughter',\n",
       " 'modest',\n",
       " 'line',\n",
       " 'acquire',\n",
       " 'proper',\n",
       " 'fraud',\n",
       " 'bias',\n",
       " 'ancient',\n",
       " 'gentleman',\n",
       " 'fragile',\n",
       " 'camp',\n",
       " 'beach',\n",
       " 'bunch',\n",
       " 'canyon',\n",
       " 'bowl',\n",
       " 'theater',\n",
       " 'old',\n",
       " 'drum',\n",
       " 'fail',\n",
       " 'kitchen',\n",
       " 'pollution',\n",
       " 'arrow',\n",
       " 'hysteria',\n",
       " 'scientist',\n",
       " 'industry',\n",
       " 'predict',\n",
       " 'president',\n",
       " 'home',\n",
       " 'train',\n",
       " 'mist',\n",
       " 'bush',\n",
       " 'cereal',\n",
       " 'discover',\n",
       " 'soul',\n",
       " 'couple',\n",
       " 'win',\n",
       " 'reader',\n",
       " 'literature',\n",
       " 'tear',\n",
       " 'stomach',\n",
       " 'tower',\n",
       " 'wall',\n",
       " 'root',\n",
       " 'article',\n",
       " 'clue',\n",
       " 'make',\n",
       " 'supper',\n",
       " 'frustration',\n",
       " 'racket',\n",
       " 'attorney',\n",
       " 'throat',\n",
       " 'generous',\n",
       " 'angle',\n",
       " 'state',\n",
       " 'hole',\n",
       " 'partner',\n",
       " 'goal',\n",
       " 'advise',\n",
       " 'wednesday',\n",
       " 'body',\n",
       " 'champion',\n",
       " 'hand',\n",
       " 'conclusion',\n",
       " 'lake',\n",
       " 'cock',\n",
       " 'anarchy',\n",
       " 'collect',\n",
       " 'interest',\n",
       " 'devil',\n",
       " 'stupid',\n",
       " 'tea',\n",
       " 'lung',\n",
       " 'mountain',\n",
       " 'business',\n",
       " 'depth',\n",
       " 'globe',\n",
       " 'profit',\n",
       " 'sprint',\n",
       " 'journal',\n",
       " 'attend',\n",
       " 'elevator',\n",
       " 'anger',\n",
       " 'hero',\n",
       " 'confession',\n",
       " 'corporation',\n",
       " 'couch',\n",
       " ...}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of all the words in ws353\n",
    "first_word = [row['word1'] for row in ws353]\n",
    "second_word = [row['word2'] for row in ws353]\n",
    "ws353_wordlist = first_word + second_word\n",
    "\n",
    "ws353_wordlist \n",
    "\n",
    "# get a list of all the words in simlex999\n",
    "first_word = [row['word1'] for row in simlex999]\n",
    "second_word = [row['word2'] for row in simlex999]\n",
    "simlex999_wordlist = first_word + second_word\n",
    "\n",
    "all_words = ws353_wordlist + simlex999_wordlist\n",
    "print(\"Num words in datasets: %s\" % len(all_words))\n",
    "\n",
    "unique_words = set(all_words) \n",
    "print(\"Number of unique words in simlex + wordsim: %s\" % len(unique_words))\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get token contexts for BNC words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Sentences in BNC corpus: 6026276\n",
      "shuffling indexes\n",
      "done shuffling\n",
      "[3507019, 581929, 4058133, 3148070, 4115605, 1325049, 4213575, 5166636, 2626864, 387970, 3512371, 5673613, 3848123, 5112398, 2338855, 3423827, 2387587, 5129218, 2255525, 4459182, 3709436, 43254, 861312, 868118, 5760421, 4277481, 4776066, 5102187, 450959, 4285010, 3088179, 5261823, 2510741, 5033797, 4949842, 3459945, 3511474, 3439804, 2985856, 5928711, 4895678, 321603, 2953841, 4542068, 4664671, 1516488, 248790, 5671886, 5477120, 2531623]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4973ae3b4cb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0mbnc_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0mcollect_bnc_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-4973ae3b4cb5>\u001b[0m in \u001b[0;36mcollect_bnc_examples\u001b[0;34m(words, max_num_examples, override)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;31m# fetch the next random sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mcorpus_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomized_indexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk/collections.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;31m# Use iterate_from to extract it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index out of range'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0;31m# Get everything we can from this piece.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_tok\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_toknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoknum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_blocknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n\u001b[1;32m    308\u001b[0m                 \u001b[0;34m'block reader %s() should return list or tuple.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk/corpus/reader/xmldocs.py\u001b[0m in \u001b[0;36mread_block\u001b[0;34m(self, stream, tagspec, elt_handler)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'END_TAG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_XML_TAG_NAME\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m                     \u001b[0;31m# sanity checks:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def randomly(seq, pseudo=True):\n",
    "    import random\n",
    "    shuffled = list(seq)  \n",
    "    if pseudo:\n",
    "        seed = lambda : 0.479032895084095295148903189394529083928435389203890819038471\n",
    "        random.shuffle(shuffled, seed)\n",
    "    else:\n",
    "        print(\"shuffling indexes\")\n",
    "        random.shuffle(shuffled) \n",
    "        print(\"done shuffling\")\n",
    "    return list(shuffled)\n",
    "\n",
    "def bnc_length(pathname='./data/count_of_bnc_sentences.txt'):\n",
    "    try:\n",
    "        with open(pathname, 'r') as fh:\n",
    "            count = int(fh.read())\n",
    "            return count\n",
    "    except:\n",
    "        print(\"BNC not yet indexed. Calculating length and writing to 'data/count_of_bnc_sentences.txt'\")\n",
    "        bnc_reader = load_bnc()\n",
    "        corpus = bnc_reader.tagged_sents(strip_space=True)\n",
    "        length = len(corpus)\n",
    "        with open(pathname, 'w') as disk:\n",
    "            disk.write(str(length))\n",
    "        return length\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "TODO problem: all of the common words are going to be super similar \n",
    "    to each other because they are collected early on in the same sentences\n",
    "\"\"\"\n",
    "\n",
    "def collect_bnc_examples(words, max_num_examples=100, override=False):\n",
    "    import os.path\n",
    "    import csv\n",
    "    \n",
    "    filename = 'bnc_tokens_353_and_simlex.csv'\n",
    "    parent_dir = './data'\n",
    "    pathname = os.path.join(parent_dir, filename)  \n",
    "    \n",
    "    # do we already have the data collected?\n",
    "    if os.path.isfile(pathname) and override==False:\n",
    "        print(\"data already exist at %s\" % pathname)\n",
    "        return\n",
    "    \n",
    "    else:    \n",
    "        bnc_reader = datasets.get_bnc()\n",
    "        corpus = bnc_reader.tagged_sents(strip_space=True)\n",
    "        corpus_length = bnc_length()\n",
    "        print(\"# Sentences in BNC corpus: %s\" % corpus_length)\n",
    "\n",
    "        \n",
    "        with open(pathname, mode='w') as outfile:\n",
    "            writer = csv.writer(outfile, delimiter='\\t', quoting=csv.QUOTE_NONNUMERIC)\n",
    "            \n",
    "            # create a data structure for keeping tabs on how many tokens we have collected\n",
    "            unigrams = {}\n",
    "            for word in unique_words:\n",
    "                    unigrams[word]=max_num_examples                    \n",
    "            \n",
    "            # come up with a random order in which to traverse the BNC\n",
    "            randomized_indexes = randomly([x for x in range(corpus_length)], pseudo=False)\n",
    "            print(randomized_indexes[:50])\n",
    "            \n",
    "            \n",
    "            \"\"\"\"\n",
    "            Iterate through the corpus, looking at words one by one, and \n",
    "            keep iterating as long as we still have tokens to collect\n",
    "            \"\"\"\n",
    "            i = 0\n",
    "            \n",
    "            while (unigrams and randomized_indexes):\n",
    "                # track progress\n",
    "                i+=1\n",
    "                if i % 100000 == 0:\n",
    "                    print(\"Processed %s sentences\" % i)\n",
    "                \n",
    "                # fetch the next random sentence\n",
    "                corpus_index = randomized_indexes.pop()\n",
    "                sentence = corpus[corpus_index]\n",
    "               \n",
    "            \n",
    "                # keep track of words we've seen in this sentence, so we don't collect\n",
    "                # a word twice if it appears twice in the sentence. \n",
    "                seen_words = set()\n",
    "                \n",
    "                for word_tuple in sentence:\n",
    "                    word = word_tuple[0].lower()\n",
    "                    tag = word_tuple[1]\n",
    "\n",
    "                    token_count = unigrams.get(word) \n",
    "                    \n",
    "                    # collect this sentence as a token of the word\n",
    "                    if (token_count != None) and (word not in seen_words):\n",
    "\n",
    "                        string = ' '.join([w[0] for w in sentence])\n",
    "                        \n",
    "                        if i % 100000 == 0:\n",
    "                            print(word)\n",
    "                            print(tag)\n",
    "                            print(string)\n",
    "                            print(corpus_index)\n",
    "                        \n",
    "                        writer.writerow([word, string, tag, corpus_index])\n",
    "                        seen_words.add(word)\n",
    "                        if unigrams[word]==0:\n",
    "                            del unigrams[word]\n",
    "                        else:\n",
    "                            unigrams[word] -=1\n",
    "\n",
    "\n",
    "\n",
    "bnc_length()\n",
    "\n",
    "collect_bnc_examples(unique_words, override = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some stats on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the average number of examples for each word?\n",
    "#os.system(\"awk 'BEGIN {FS=OFS=\"\\t\"} NR > 0 {a[$1]+=1} END {for (i in a) {print i, a[i]}}' bnc_words_with_context_tokens.txt | sort -n >> bnc_counts.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib\n",
    "import seaborn as sns, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#############\n",
    "# How many do we keep if we want 20 examples minimum? 10?\n",
    "##############\n",
    "with open('./data/similarity_dataset/words_with_BNC_tokens.csv', mode=\"r\") as infile:\n",
    "    reader = csv.DictReader(infile, delimiter=\"\\t\", quoting=csv.QUOTE_NONNUMERIC, fieldnames=[\"word\", \"sentence\", \"POS\", \"id\"])\n",
    "    #token_counts = {row[0]:int(row[1]) for row in reader}\n",
    "    \n",
    "    data = [row for row in reader]\n",
    "    # print(data[:10])\n",
    "    \n",
    "    token_counts = {}\n",
    "    for row in data:\n",
    "        word = row[\"word\"]\n",
    "        if word in token_counts:\n",
    "            token_counts[word] += 1\n",
    "        else:\n",
    "            token_counts[word] = 0\n",
    "    \n",
    "    fifty = [(x, y) for (x,y) in token_counts.items() if y >=50]\n",
    "    over_twenty = {x: y for (x,y) in token_counts.items() if y >=20}\n",
    "    over_ten = [[x, y] for (x,y) in token_counts.items() if y >=10]\n",
    "    print(\"50 tokens for %s words\" %len(fifty))\n",
    "    print(\"Over 20 tokens for %s words\" %len(over_twenty))\n",
    "    print(\"Over 10 tokens for %s words\" %len(over_ten))\n",
    "\n",
    "################\n",
    "# What's the relationship betweeen abstractness and frequency in the corpus? We want this to be relatively even, I think....\n",
    "################\n",
    "\n",
    "df = pd.DataFrame.from_records(fifty, columns=[\"word\", \"token_count\"])\n",
    "print(df)\n",
    "print(df.columns)\n",
    "\n",
    "# Cut the window in 2 parts\n",
    "f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.15, .85)})\n",
    "\n",
    "# Add a graph in each part\n",
    "sns.boxplot(df[\"token_count\"], ax=ax_box)\n",
    "sns.distplot(df[\"token_count\"], ax=ax_hist)\n",
    "\n",
    " \n",
    "# Remove x axis name for the boxplot\n",
    "ax_box.set(xlabel='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained BERT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytorch-pretrained-bert\n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#% matplotlib inline\n",
    "\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save token vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# bert base uncased\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "# function to calculate token vector for a word in a context\n",
    "# in case the word parses into multiple word pieces, take the average of the context vectors for each piece\n",
    "\n",
    "# calculate token vectors for each example of each word in dataset\n",
    "\n",
    "\n",
    "with open('./data/similarity_dataset/words_with_BNC_tokens.csv', mode=\"r\") as infile:\n",
    "    fieldnames = [\"word\", \"sentence\", \"POS\", \"id\"]\n",
    "    reader = csv.DictReader(infile, delimiter=\"\\t\", quoting=csv.QUOTE_NONNUMERIC, fieldnames=fieldnames)\n",
    "\n",
    "    for row in reader:\n",
    "        word = row[\"word\"]\n",
    "        text = row[\"sentence\"]\n",
    "        pos = row[\"POS\"]\n",
    "        uid = \"BNC\" + str(int(row[\"id\"]))\n",
    "\n",
    "        # ensure directory exists for this word\n",
    "        data_dir = \"./data/similarity_dataset\"\n",
    "        filename = word\n",
    "        word_dir = os.path.join(data_dir, filename)\n",
    "        try:\n",
    "            os.mkdir(word_dir)\n",
    "            print(\"making directory for word at %s\" % word_dir)\n",
    "        except:\n",
    "            None\n",
    "\n",
    "        # open file for this word to spit vector into\n",
    "        vector_file = os.path.join(word_dir, \"BNC_tokens_with_last_layer_vectors.csv\")\n",
    "        with open(vector_file, mode=\"a\") as outfile:\n",
    "            writer = csv.writer(outfile, delimiter='\\t', quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "            \"\"\"\n",
    "            Run the token sentence through the model and calculate a word vector\n",
    "            based on the mean of the WordPiece vectors in the last layer\n",
    "            \"\"\"\n",
    "            tokenized_word = tokenizer.tokenize(word)\n",
    "\n",
    "            # Add the special tokens.\n",
    "            marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "            # Split the sentence into tokens.\n",
    "            tokenized_text = tokenizer.tokenize(marked_text)\n",
    "            segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "            # Map the token strings to their vocabulary indeces.\n",
    "            indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "            # Display the words with their indeces.\n",
    "            #for tup in zip(tokenized_text, indexed_tokens):\n",
    "                #print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n",
    "\n",
    "            # Mark each of the tokens as belonging to sentence \"1\".\n",
    "            segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "            # Convert inputs to PyTorch tensors\n",
    "            tokens_tensor = torch.tensor([indexed_tokens])\n",
    "            segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "            try:\n",
    "                # Predict hidden states features for each layer\n",
    "                with torch.no_grad():\n",
    "                    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "            except:\n",
    "                print(\"tokenized sequence too long\")\n",
    "                print(tokenized_text)\n",
    "\n",
    "            # Rearrange hidden layers to be grouped by token\n",
    "            # Concatenate the tensors for all layers. We use `stack` here to\n",
    "            # create a new dimension in the tensor.\n",
    "            token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "            token_embeddings.size()\n",
    "\n",
    "            # Remove dimension 1, the \"batches\".\n",
    "            token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "            token_embeddings.size()\n",
    "\n",
    "\n",
    "            # Swap dimensions 0 and 1.\n",
    "            token_embeddings = token_embeddings.permute(1,0,2)\n",
    "            token_embeddings.size()\n",
    "\n",
    "\n",
    "            ####\n",
    "            vectors = []\n",
    "            for word_piece in tokenized_word:\n",
    "                # TODO should be the matching slice, because this doesnt account for repeat word  pieces\n",
    "                index = tokenized_text.index(word_piece)\n",
    "                token = token_embeddings[index]\n",
    "                # `token` is a [12 x 768] tensor\n",
    "\n",
    "                # Sum the vectors from the last four layers.\n",
    "                #sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "                # Use the vectors from the last layer\n",
    "                vec = token[-1:]\n",
    "\n",
    "                vectors.append(vec.numpy())\n",
    "\n",
    "            #vectors2 = torch.tensor(vectors)\n",
    "\n",
    "            # use the mean of all of the word_pieces. \n",
    "            word_vector = np.average(vectors, axis=0)\n",
    "            \n",
    "            \n",
    "            # finally, write all of the info with the vector to disk\n",
    "            writer.writerow([word, text, pos, uid, word_vector])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate centroid for each wor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we didnt collect tokens for uppercase words by mistake. so, dont try to calculate for uppercase words\n",
    "lowercase_unique_words = filter(lambda x: x[0].islower(), unique_words)\n",
    "\n",
    "\n",
    "for word in lowercase_unique_words:\n",
    "    data_dir = './data/similarity_dataset'\n",
    "    word_dir = os.path.join(data_dir, word)\n",
    "    \n",
    "\n",
    "    \n",
    "    path = os.path.join(word_dir, 'BNC_tokens_with_last_layer_vectors.csv')\n",
    "    if os.path.isfile(path):\n",
    "    \n",
    "        with open(path, mode='r') as csv_file:\n",
    "            reader = csv.DictReader(csv_file, delimiter='\\t', fieldnames=[\"word\", \"sentence\", \"tag\", \"uid\", \"vector\"])\n",
    "\n",
    "\n",
    "            # calculate centroid\n",
    "            word = word\n",
    "\n",
    "            first = next(reader)[\"vector\"][2:-2]\n",
    "            initial_vec = np.fromstring(first, dtype=np.float, sep=' ')\n",
    "            centroid = initial_vec\n",
    "\n",
    "            for row in reader:\n",
    "                # chop off the weird brackets at beginning and end of string\n",
    "                vector = np.fromstring(row[\"vector\"][2:-2], dtype=np.float, sep=' ')\n",
    "\n",
    "                centroid = ( centroid + vector ) / 2\n",
    "\n",
    "            outpath = os.path.join(word_dir, \"BNC_layer_11_centroid.csv\")\n",
    "            with open(outpath, mode='w') as disk:\n",
    "                writer = csv.DictWriter(disk, delimiter='\\t', fieldnames=[\"word\", \"centroid\"])\n",
    "                data = {\"word\": word, \"centroid\": centroid}\n",
    "                writer.writerow(data)\n",
    "            \n",
    "    else:\n",
    "        # guessss this word wasn't in the bnc for some reason?\n",
    "        print(\"no file for %s\" % word)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Centroids on 353 similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "# create a list to store predicted and observed similarities\n",
    "similarities = []\n",
    "\n",
    "# open wordsim 353\n",
    "ws353\n",
    "\n",
    "# for each word pair, calculate the cosine similarity between the centroids for the words\n",
    "for entry in ws353:\n",
    "    \n",
    "    word1 = entry['word1'].lower()\n",
    "    word2 = entry['word2'].lower()\n",
    "    observed_similarity = entry['similarity']\n",
    "\n",
    "    # retrieve centroid for both words\n",
    "    data_dir = './data/similarity_dataset'\n",
    "    word1_dir = os.path.join(data_dir, word1)\n",
    "    word2_dir = os.path.join(data_dir, word2)\n",
    "    \n",
    "    if os.path.isdir(word1_dir) and os.path.isdir(word2_dir):\n",
    "\n",
    "        word1_centroid = []\n",
    "        word2_centroid = []\n",
    "\n",
    "\n",
    "\n",
    "        path = os.path.join(word1_dir, 'BNC_layer_11_centroid.csv')\n",
    "        with open(path, mode='r') as infile:\n",
    "            reader = csv.DictReader(infile, delimiter='\\t', fieldnames=[\"word\", \"vector\"])\n",
    "            row = next(reader)\n",
    "            word1_centroid = np.fromstring(row[\"vector\"][2:-2], dtype=np.float, sep=' ')\n",
    "\n",
    "        path = os.path.join(word2_dir, 'BNC_layer_11_centroid.csv')\n",
    "        with open(path, mode='r') as infile:\n",
    "            reader = csv.DictReader(infile, delimiter='\\t', fieldnames=[\"word\", \"vector\"])\n",
    "            row = next(reader)\n",
    "            word2_centroid = np.fromstring(row[\"vector\"][2:-2], dtype=np.float, sep=' ')  \n",
    "\n",
    "        # calculate predicted similarity from centroids\n",
    "        predicted_similarity = 1- cosine(word1_centroid, word2_centroid)\n",
    "\n",
    "        # store the predicted and observed similarity in a dict of {word: predicted: observed:}\n",
    "        data = {\"word1\": word1, \"word2\": word2, \"predicted\": predicted_similarity, \"observed\": observed_similarity}\n",
    "        similarities.append(data)\n",
    "    \n",
    "    else:\n",
    "        print(\"no data collected for %s or %s\" % (word1, word2))\n",
    "\n",
    "# calculate pearsons correlation coefficient. \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_records(similarities)\n",
    "print(df)\n",
    "\n",
    "X = df[\"predicted\"]\n",
    "y = df[\"observed\"]\n",
    "\n",
    "pearson_value = pearsonr(X,y)\n",
    "\n",
    "print(\"Pearson's correlation coefficient between observed similarity and centroid cosine similarity is {}\".format(pearson_value))\n",
    "print('N = {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the same for SimLex Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "# create a list to store predicted and observed similarities\n",
    "similarities = []\n",
    "\n",
    "# open simlex 999\n",
    "simlex999\n",
    "\n",
    "# for each word pair, calculate the cosine similarity between the centroids for the words\n",
    "for entry in simlex999:\n",
    "    \n",
    "    word1 = entry['word1'].lower()\n",
    "    word2 = entry['word2'].lower()\n",
    "    observed_similarity = entry['SimLex999']\n",
    "\n",
    "    # retrieve centroid for both words\n",
    "    data_dir = './data/similarity_dataset'\n",
    "    word1_dir = os.path.join(data_dir, word1)\n",
    "    word2_dir = os.path.join(data_dir, word2)\n",
    "    \n",
    "    if os.path.isdir(word1_dir) and os.path.isdir(word2_dir):\n",
    "\n",
    "        word1_centroid = []\n",
    "        word2_centroid = []\n",
    "\n",
    "\n",
    "\n",
    "        path = os.path.join(word1_dir, 'BNC_layer_11_centroid.csv')\n",
    "        with open(path, mode='r') as infile:\n",
    "            reader = csv.DictReader(infile, delimiter='\\t', fieldnames=[\"word\", \"vector\"])\n",
    "            row = next(reader)\n",
    "            word1_centroid = np.fromstring(row[\"vector\"][2:-2], dtype=np.float, sep=' ')\n",
    "\n",
    "        path = os.path.join(word2_dir, 'BNC_layer_11_centroid.csv')\n",
    "        with open(path, mode='r') as infile:\n",
    "            reader = csv.DictReader(infile, delimiter='\\t', fieldnames=[\"word\", \"vector\"])\n",
    "            row = next(reader)\n",
    "            word2_centroid = np.fromstring(row[\"vector\"][2:-2], dtype=np.float, sep=' ')  \n",
    "\n",
    "        # calculate predicted similarity from centroids\n",
    "        predicted_similarity = 1- cosine(word1_centroid, word2_centroid)\n",
    "\n",
    "        # store the predicted and observed similarity in a dict of {word: predicted: observed:}\n",
    "        data = {\"word1\": word1, \"word2\": word2, \"predicted\": predicted_similarity, \"observed\": observed_similarity}\n",
    "        similarities.append(data)\n",
    "    \n",
    "    else:\n",
    "        print(\"no data collected for %s or %s\" % (word1, word2))\n",
    "\n",
    "# calculate pearsons correlation coefficient. \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_records(similarities)\n",
    "print(df)\n",
    "\n",
    "X = df[\"predicted\"]\n",
    "y = df[\"observed\"]\n",
    "\n",
    "pearson_value = pearsonr(X,y)\n",
    "\n",
    "print(\"Pearson's correlation coefficient between observed similarity and centroid cosine similarity is {}\".format(pearson_value))\n",
    "print('N = {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate K-means clusters for a single word, and print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "context_vectors = []\n",
    "with open('data/similarity_dataset/mountain/BNC_tokens_with_last_layer_vectors.csv', mode='r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter='\\t', fieldnames=[\"word\", \"sentence\", \"tag\", \"uid\", \"vector\"])\n",
    "\n",
    "    data = [row for row in csv_reader]\n",
    "    \n",
    "    # get just the numbers without the array brackets [[ ]] on either side\n",
    "    context_vector_strings = [row[\"vector\"][2:-2] for row in data]\n",
    "    context_vectors = [np.fromstring(x[2:-2], dtype=np.float, sep=' ') for x in context_vector_strings]\n",
    "\n",
    "\n",
    "numclusters = 4\n",
    "kmeans_obj = KMeans(n_clusters=numclusters)\n",
    "kmeans_obj.fit(context_vectors)\n",
    "label_list = kmeans_obj.labels_\n",
    "cluster_centroids = kmeans_obj.cluster_centers_\n",
    "\n",
    "print(label_list)\n",
    "print(len(label_list))\n",
    "print(cluster_centroids)\n",
    "\n",
    "# Let's print the sentences that got clustered together.\n",
    "for clusternumber in range(numclusters):\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"Sentences in cluster\", clusternumber)\n",
    "    for index, datapoint in enumerate(data):\n",
    "        if label_list[index] == clusternumber:\n",
    "            print(datapoint[\"sentence\"])\n",
    "            print(\"\\n\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate k=4 and store clusters for all the words we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "# we didnt collect tokens for uppercase words by mistake. so, dont try to calculate for uppercase words\n",
    "lowercase_unique_words = filter(lambda x: x[0].islower(), unique_words)\n",
    "\n",
    "\n",
    "for word in lowercase_unique_words:\n",
    "    data_dir = './data/similarity_dataset'\n",
    "    word_dir = os.path.join(data_dir, word)\n",
    "    \n",
    "    \n",
    "    path = os.path.join(word_dir, 'BNC_tokens_with_last_layer_vectors.csv')\n",
    "    if os.path.isfile(path):\n",
    "    \n",
    "        with open(path, mode='r') as csv_file:\n",
    "            reader = csv.DictReader(csv_file, delimiter='\\t', fieldnames=[\"word\", \"sentence\", \"tag\", \"uid\", \"vector\"])\n",
    "\n",
    "\n",
    "\n",
    "            data = [row for row in reader]\n",
    "            if len(data) >= 4:\n",
    "                \n",
    "                #context_vectors = []\n",
    "\n",
    "                \n",
    "                # fetch input vectors\n",
    "                # get just the numbers without the array brackets [[ ]] on either side\n",
    "                context_vector_strings = [row[\"vector\"][2:-2] for row in data]\n",
    "                context_vectors = [np.fromstring(x[2:-2], dtype=np.float, sep=' ') for x in context_vector_strings]\n",
    "\n",
    "\n",
    "                # calculate kmeans clusters\n",
    "                numclusters = 4\n",
    "                kmeans_obj = KMeans(n_clusters=numclusters)\n",
    "                kmeans_obj.fit(context_vectors)\n",
    "                label_list = kmeans_obj.labels_\n",
    "                cluster_centroids = kmeans_obj.cluster_centers_\n",
    "\n",
    "\n",
    "                # store clusternumber with data\n",
    "                for index,datapoint in enumerate(data):\n",
    "                    datapoint['cluster_number'] = label_list[index]\n",
    "\n",
    "\n",
    "                # save clusters to file\n",
    "                outpath = os.path.join(word_dir, \"BNC_layer_11_kmeans_4_clusters.csv\")\n",
    "                with open(outpath, mode='w') as disk:\n",
    "                    writer = csv.DictWriter(disk, delimiter='\\t', fieldnames=['word', 'clusternumber', 'centroid', 'sentence_uids'])\n",
    "\n",
    "\n",
    "                    # retrieve centroid for each cluster and uids of sentences in cluster:\n",
    "                    for clusternumber in range(numclusters):\n",
    "                        sentence_uids = []\n",
    "                        for index, datapoint in enumerate(data):\n",
    "                            if datapoint['cluster_number'] == clusternumber:\n",
    "                                sentence_uids.append(datapoint['uid'])\n",
    "                        out_data = {'word': word,\n",
    "                                    'clusternumber': clusternumber,\n",
    "                                    'centroid': cluster_centroids[clusternumber],\n",
    "                                    'sentence_uids': sentence_uids}\n",
    "\n",
    "                        # write dta for this cluster\n",
    "                        writer.writerow(out_data)\n",
    "            else:\n",
    "                print(\"not enough tokens to make four clusters for word: %s\" % word)\n",
    "\n",
    "    else:\n",
    "        # guessss this word wasn't in the bnc for some reason?\n",
    "        print(\"no file for %s\" % word)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ws353 similarity score for sense clusters using maxsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# load ws353 word pairs\n",
    "ws353 = load_ws353()\n",
    "\n",
    "# create a list to store predicted and observed similarities\n",
    "similarities = []\n",
    "\n",
    "\n",
    "# for each word pair, calculate the cosine similarity between the centroids for the words\n",
    "for entry in ws353:\n",
    "    \n",
    "    word1 = entry['word1'].lower()\n",
    "    word2 = entry['word2'].lower()\n",
    "    observed_similarity = entry['similarity']\n",
    "\n",
    "    # retrieve centroids for both words\n",
    "    data_dir = './data/similarity_dataset'\n",
    "    word1_path = os.path.join(data_dir, word1, 'BNC_layer_11_kmeans_4_clusters.csv')\n",
    "    word2_path = os.path.join(data_dir, word2, 'BNC_layer_11_kmeans_4_clusters.csv')\n",
    "    \n",
    "    if os.path.isfile(word1_path) and os.path.isfile(word2_path):\n",
    "\n",
    "        word1_centroids = []\n",
    "        word2_centroids = []\n",
    "\n",
    "\n",
    "\n",
    "        with open(word1_path, mode='r') as infile:\n",
    "            reader = csv.DictReader(infile, delimiter='\\t', fieldnames=['word', 'clusternumber', 'centroid', 'sentence_uids'])\n",
    "            data = [row for row in reader]\n",
    "            word1_centroids = []\n",
    "            for row in data:\n",
    "                centroid = np.fromstring(row['centroid'][2:-2], dtype=np.float, sep=' ')\n",
    "                word1_centroids.append(centroid)\n",
    "\n",
    "\n",
    "        with open(word2_path, mode='r') as infile:\n",
    "            reader = csv.DictReader(infile, delimiter='\\t', fieldnames=['word', 'clusternumber', 'centroid', 'sentence_uids'])\n",
    "            data = [row for row in reader]\n",
    "            word2_centroids = []\n",
    "            for row in data:\n",
    "                centroid = np.fromstring(row['centroid'][2:-2], dtype=np.float, sep=' ')\n",
    "                word2_centroids.append(centroid)\n",
    "                \n",
    "\n",
    "        # calculate predicted similarity from of each pair of cluster centroids of both words\n",
    "        predicted_similarities = []\n",
    "        for centroid1 in word1_centroids:\n",
    "            for centroid2 in word2_centroids:\n",
    "                predicted_similarity = 1 - cosine(centroid1, centroid2)\n",
    "                predicted_similarities.append(predicted_similarity)\n",
    "            \n",
    "            \n",
    "        # find the max of the pairwise similarities\n",
    "        max_sim = max(predicted_similarities)\n",
    "\n",
    "        # store the predicted and observed similarity in a dict of {word: predicted: observed:}\n",
    "        data = {\"word1\": word1, \"word2\": word2, \"predicted\": max_sim, \"observed\": observed_similarity}\n",
    "        similarities.append(data)\n",
    "    \n",
    "    else:\n",
    "        print(\"no data collected for %s or %s\" % (word1, word2))\n",
    "\n",
    "# calculate pearsons correlation coefficient. \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_records(similarities)\n",
    "print(df)\n",
    "\n",
    "X = df[\"predicted\"]\n",
    "y = df[\"observed\"]\n",
    "\n",
    "pearson_value = pearsonr(X,y)\n",
    "\n",
    "print(\"Pearson's correlation coefficient between observed similarity and k=4 maxsim sense cluster similarity is {}\".format(pearson_value))\n",
    "print('N = {}'.format(len(df)))\n",
    "\n",
    "\n",
    "spearman_value = spearmanr(X,y)\n",
    "\n",
    "print(\"Spearmans's correlation coefficient between observed similarity and k=4 maxsim sense cluster similarity is {}\".format(spearman_value))\n",
    "print('N = {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate SimLex999 similarity score for sense clusters using maxsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# load ws353 word pairs\n",
    "simlex999 = load_simlex999()\n",
    "\n",
    "# create a list to store predicted and observed similarities\n",
    "similarities = []\n",
    "\n",
    "\n",
    "# for each word pair, calculate the cosine similarity between the centroids for the words\n",
    "for entry in simlex999:\n",
    "    \n",
    "    word1 = entry['word1'].lower()\n",
    "    word2 = entry['word2'].lower()\n",
    "    observed_similarity = entry['SimLex999']\n",
    "\n",
    "    # retrieve centroids for both words\n",
    "    data_dir = './data/similarity_dataset'\n",
    "    word1_path = os.path.join(data_dir, word1, 'BNC_layer_11_kmeans_4_clusters.csv')\n",
    "    word2_path = os.path.join(data_dir, word2, 'BNC_layer_11_kmeans_4_clusters.csv')\n",
    "    \n",
    "    if os.path.isfile(word1_path) and os.path.isfile(word2_path):\n",
    "\n",
    "        word1_centroids = []\n",
    "        word2_centroids = []\n",
    "\n",
    "\n",
    "        with open(word1_path, mode='r') as infile:\n",
    "            reader = csv.DictReader(infile, delimiter='\\t', fieldnames=['word', 'clusternumber', 'centroid', 'sentence_uids'])\n",
    "            data = [row for row in reader]\n",
    "            word1_centroids = []\n",
    "            for row in data:\n",
    "                centroid = np.fromstring(row['centroid'][2:-2], dtype=np.float, sep=' ')\n",
    "                word1_centroids.append(centroid)\n",
    "\n",
    "\n",
    "        with open(word2_path, mode='r') as infile:\n",
    "            reader = csv.DictReader(infile, delimiter='\\t', fieldnames=['word', 'clusternumber', 'centroid', 'sentence_uids'])\n",
    "            data = [row for row in reader]\n",
    "            word2_centroids = []\n",
    "            for row in data:\n",
    "                centroid = np.fromstring(row['centroid'][2:-2], dtype=np.float, sep=' ')\n",
    "                word2_centroids.append(centroid)\n",
    "                \n",
    "\n",
    "        # calculate predicted similarity from of each pair of cluster centroids of both words\n",
    "        predicted_similarities = []\n",
    "        for centroid1 in word1_centroids:\n",
    "            for centroid2 in word2_centroids:\n",
    "                predicted_similarity = 1 - cosine(centroid1, centroid2)\n",
    "                predicted_similarities.append(predicted_similarity)\n",
    "            \n",
    "            \n",
    "        # find the max of the pairwise similarities\n",
    "        max_sim = max(predicted_similarities)\n",
    "\n",
    "        # store the predicted and observed similarity in a dict of {word: predicted: observed:}\n",
    "        data = {\"word1\": word1, \"word2\": word2, \"predicted\": max_sim, \"observed\": observed_similarity}\n",
    "        similarities.append(data)\n",
    "    \n",
    "    else:\n",
    "        print(\"no data collected for %s or %s\" % (word1, word2))\n",
    "\n",
    "# calculate pearsons correlation coefficient. \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_records(similarities)\n",
    "print(df)\n",
    "\n",
    "X = df[\"predicted\"]\n",
    "y = df[\"observed\"]\n",
    "\n",
    "pearson_value = pearsonr(X,y)\n",
    "\n",
    "print(\"Pearson's correlation coefficient between observed similarity and k=4 maxsim sense cluster similarity is {}\".format(pearson_value))\n",
    "print('N = {}'.format(len(df)))\n",
    "\n",
    "spearman_value = spearmanr(X,y)\n",
    "\n",
    "print(\"Spearmans's correlation coefficient between observed similarity and k=4 maxsim sense cluster similarity is {}\".format(spearman_value))\n",
    "print('N = {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
